{"2024-03-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18804v1","updated":"2024-03-27T17:50:00Z","published":"2024-03-27T17:50:00Z","title":"Is Modularity Transferable? A Case Study through the Lens of Knowledge\n  Distillation","summary":"  The rise of Modular Deep Learning showcases its potential in various Natural\nLanguage Processing applications. Parameter-efficient fine-tuning (PEFT)\nmodularity has been shown to work for various use cases, from domain adaptation\nto multilingual setups. However, all this work covers the case where the\nmodular components are trained and deployed within one single Pre-trained\nLanguage Model (PLM). This model-specific setup is a substantial limitation on\nthe very modularity that modular architectures are trying to achieve. We ask\nwhether current modular approaches are transferable between models and whether\nwe can transfer the modules from more robust and larger PLMs to smaller ones.\nIn this work, we aim to fill this gap via a lens of Knowledge Distillation,\ncommonly used for model compression, and present an extremely straightforward\napproach to transferring pre-trained, task-specific PEFT modules between\nsame-family PLMs. Moreover, we propose a method that allows the transfer of\nmodules between incompatible PLMs without any change in the inference\ncomplexity. The experiments on Named Entity Recognition, Natural Language\nInference, and Paraphrase Identification tasks over multiple languages and PEFT\nmethods showcase the initial potential of transferable modularity.\n","authors":["Mateusz Klimaszewski","Piotr Andruszkiewicz","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2403.18804v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18803v1","updated":"2024-03-27T17:49:31Z","published":"2024-03-27T17:49:31Z","title":"Projective Methods for Mitigating Gender Bias in Pre-trained Language\n  Models","summary":"  Mitigation of gender bias in NLP has a long history tied to debiasing static\nword embeddings. More recently, attention has shifted to debiasing pre-trained\nlanguage models. We study to what extent the simplest projective debiasing\nmethods, developed for word embeddings, can help when applied to BERT's\ninternal representations. Projective methods are fast to implement, use a small\nnumber of saved parameters, and make no updates to the existing model\nparameters. We evaluate the efficacy of the methods in reducing both intrinsic\nbias, as measured by BERT's next sentence prediction task, and in mitigating\nobserved bias in a downstream setting when fine-tuned. To this end, we also\nprovide a critical analysis of a popular gender-bias assessment test for\nquantifying intrinsic bias, resulting in an enhanced test set and new bias\nmeasures. We find that projective methods can be effective at both intrinsic\nbias and downstream bias mitigation, but that the two outcomes are not\nnecessarily correlated. This finding serves as a warning that intrinsic bias\ntest sets, based either on language modeling tasks or next sentence prediction,\nshould not be the only benchmark in developing a debiased language model.\n","authors":["Hillary Dawkins","Isar Nejadgholi","Daniel Gillis","Judi McCuaig"],"pdf_url":"https://arxiv.org/pdf/2403.18803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17574v2","updated":"2024-03-27T17:34:57Z","published":"2024-02-27T15:09:20Z","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and\n  Optimization","summary":"  Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.\n","authors":["Wenqi Zhang","Ke Tang","Hai Wu","Mengna Wang","Yongliang Shen","Guiyang Hou","Zeqi Tan","Peng Li","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17574v2.pdf","comment":"LLM-based Agent"},{"id":"http://arxiv.org/abs/2403.18783v1","updated":"2024-03-27T17:31:39Z","published":"2024-03-27T17:31:39Z","title":"Towards a World-English Language Model for On-Device Virtual Assistants","summary":"  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are\ngenerally language-, region-, and in some cases, device-dependent, which\nincreases the effort to scale and maintain them. Combining NNLMs for one or\nmore of the categories is one way to improve scalability. In this work, we\ncombine regional variants of English to build a ``World English'' NNLM for\non-device VAs. In particular, we investigate the application of adapter\nbottlenecks to model dialect-specific characteristics in our existing\nproduction NNLMs {and enhance the multi-dialect baselines}. We find that\nadapter modules are more effective in modeling dialects than specializing\nentire sub-networks. Based on this insight and leveraging the design of our\nproduction models, we introduce a new architecture for World English NNLM that\nmeets the accuracy, latency, and memory constraints of our single-dialect\nmodels.\n","authors":["Rricha Jalota","Lyan Verwimp","Markus Nussbaum-Thom","Amr Mousa","Arturo Argueta","Youssef Oualil"],"pdf_url":"https://arxiv.org/pdf/2403.18783v1.pdf","comment":"Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02009v2","updated":"2024-03-27T17:24:47Z","published":"2024-01-04T00:32:33Z","title":"Self-Contrast: Better Reflection Through Inconsistent Solving\n  Perspectives","summary":"  The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.\n","authors":["Wenqi Zhang","Yongliang Shen","Linjuan Wu","Qiuying Peng","Jun Wang","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18771v1","updated":"2024-03-27T17:20:39Z","published":"2024-03-27T17:20:39Z","title":"CheckEval: Robust Evaluation Framework using Large Language Model via\n  Checklist","summary":"  We introduce CheckEval, a novel evaluation framework using Large Language\nModels, addressing the challenges of ambiguity and inconsistency in current\nevaluation methods. CheckEval addresses these challenges by dividing evaluation\ncriteria into detailed sub-aspects and constructing a checklist of Boolean\nquestions for each, simplifying the evaluation. This approach not only renders\nthe process more interpretable but also significantly enhances the robustness\nand reliability of results by focusing on specific evaluation dimensions.\nValidated through a focused case study using the SummEval benchmark, CheckEval\nindicates a strong correlation with human judgments. Furthermore, it\ndemonstrates a highly consistent Inter-Annotator Agreement. These findings\nhighlight the effectiveness of CheckEval for objective, flexible, and precise\nevaluations. By offering a customizable and interactive framework, CheckEval\nsets a new standard for the use of LLMs in evaluation, responding to the\nevolving needs of the field and establishing a clear method for future\nLLM-based evaluation.\n","authors":["Yukyung Lee","Joonghoon Kim","Jaehee Kim","Hyowon Cho","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2403.18771v1.pdf","comment":"HEAL at CHI 2024"},{"id":"http://arxiv.org/abs/2403.18769v1","updated":"2024-03-27T17:13:38Z","published":"2024-03-27T17:13:38Z","title":"Improved Neural Protoform Reconstruction via Reflex Prediction","summary":"  Protolanguage reconstruction is central to historical linguistics. The\ncomparative method, one of the most influential theoretical and methodological\nframeworks in the history of the language sciences, allows linguists to infer\nprotoforms (reconstructed ancestral words) from their reflexes (related modern\nwords) based on the assumption of regular sound change. Not surprisingly,\nnumerous computational linguists have attempted to operationalize comparative\nreconstruction through various computational models, the most successful of\nwhich have been supervised encoder-decoder models, which treat the problem of\npredicting protoforms given sets of reflexes as a sequence-to-sequence problem.\nWe argue that this framework ignores one of the most important aspects of the\ncomparative method: not only should protoforms be inferable from cognate sets\n(sets of related reflexes) but the reflexes should also be inferable from the\nprotoforms. Leveraging another line of research -- reflex prediction -- we\npropose a system in which candidate protoforms from a reconstruction model are\nreranked by a reflex prediction model. We show that this more complete\nimplementation of the comparative method allows us to surpass state-of-the-art\nprotoform reconstruction methods on three of four Chinese and Romance datasets.\n","authors":["Liang Lu","Jingzhi Wang","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2403.18769v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18746v1","updated":"2024-03-27T16:45:02Z","published":"2024-03-27T16:45:02Z","title":"CYCLE: Learning to Self-Refine the Code Generation","summary":"  Pre-trained code language models have achieved promising performance in code\ngeneration and improved the programming efficiency of human developers.\nHowever, their self-refinement capability is typically overlooked by the\nexisting evaluations of code LMs, which focus only on the accuracy of the\none-time prediction. For the cases when code LMs fail to implement the correct\nprogram, developers actually find it hard to debug and fix the faulty\nprediction since it is not written by the developers themselves. Unfortunately,\nour study reveals that code LMs cannot efficiently self-refine their faulty\ngenerations as well.\n  In this paper, we propose CYCLE framework, learning to self-refine the faulty\ngeneration according to the available feedback, such as the execution results\nreported by the test suites. We evaluate CYCLE on three popular code generation\nbenchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE\nsuccessfully maintains, sometimes improves, the quality of one-time code\ngeneration, while significantly improving the self-refinement capability of\ncode LMs. We implement four variants of CYCLE with varied numbers of parameters\nacross 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently\nboosts the code generation performance, by up to 63.5%, across benchmarks and\nvaried model sizes. We also notice that CYCLE outperforms code LMs that have\n3$\\times$ more parameters in self-refinement.\n","authors":["Yangruibo Ding","Marcus J. Min","Gail Kaiser","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2403.18746v1.pdf","comment":"Camera-ready for OOPSLA'24"},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18697v1","updated":"2024-03-27T15:46:25Z","published":"2024-03-27T15:46:25Z","title":"The Invalsi Benchmark: measuring Language Models Mathematical and\n  Language understanding in Italian","summary":"  While Italian is by all metrics a high resource language, currently, there\nare isn't a Language Model pre-trained exclusively in this language. This\nresults in a lower number of available benchmarks to evaluate the performance\nof language models in Italian.\n  This work presents two new benchmarks to evaluate the models performance on\nmathematical understanding and language understanding in Italian. These\nbenchmarks are based on real tests that are undertaken by students of age\nbetween 11 and 18 within the Italian school system and have therefore been\nvalidated by several experts in didactics and pedagogy.\n  To validate this dataset we evaluate the performance of 9 language models\nthat are the best performing when writing in Italian, including our own\nfine-tuned models. We show that this is a challenging benchmark where current\nlanguage models are bound by 60\\% accuracy.\n  We believe that the release of this dataset paves the way for improving\nfuture models mathematical and language understanding in Italian.\n","authors":["Andrea Esuli","Giovanni Puccetti"],"pdf_url":"https://arxiv.org/pdf/2403.18697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18684v1","updated":"2024-03-27T15:27:36Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.11128v2","updated":"2024-03-27T15:22:53Z","published":"2024-03-17T07:34:12Z","title":"Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'\n  API Invocation Capabilities","summary":"  With the rise of Large Language Models (LLMs), AI assistants' ability to\nutilize tools, especially through API calls, has advanced notably. This\nprogress has necessitated more accurate evaluation methods. Many existing\nstudies adopt static evaluation, where they assess AI assistants' API call\nbased on pre-defined dialogue histories. However, such evaluation method can be\nmisleading, as an AI assistant might fail in generating API calls from\npreceding human interaction in real cases. Instead of the resource-intensive\nmethod of direct human-machine interactions, we propose Automated Dynamic\nEvaluation (AutoDE) to assess an assistant's API call capability without human\ninvolvement. In our framework, we endeavor to closely mirror genuine human\nconversation patterns in human-machine interactions, using a LLM-based user\nagent, equipped with a user script to ensure human alignment. Experimental\nresults highlight that AutoDE uncovers errors overlooked by static evaluations,\naligning more closely with human assessment. Testing four AI assistants using\nour crafted benchmark, our method further mirrored human evaluation compared to\nconventional static evaluations.\n","authors":["Honglin Mu","Yang Xu","Yunlong Feng","Xiaofeng Han","Yitong Li","Yutai Hou","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2403.11128v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18667v1","updated":"2024-03-27T15:11:00Z","published":"2024-03-27T15:11:00Z","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users","summary":"  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n","authors":["Yejin Kim","Scott Rome","Kevin Foley","Mayur Nankani","Rimon Melamed","Javier Morales","Abhay Yadav","Maria Peifer","Sardar Hamidian","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18667v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.13320v2","updated":"2024-03-27T14:57:29Z","published":"2023-09-23T09:35:55Z","title":"GlotScript: A Resource and Tool for Low Resource Writing System\n  Identification","summary":"  We present GlotScript, an open resource and tool for low resource writing\nsystem identification. GlotScript-R is a resource that provides the attested\nwriting systems for more than 7,000 languages. It is compiled by aggregating\ninformation from existing writing system resources. GlotScript-T is a writing\nsystem identification tool that covers all 161 Unicode 15.0 scripts. For an\ninput text, it returns its script distribution where scripts are identified by\nISO 15924 codes. We also present two use cases for GlotScript. First, we\ndemonstrate that GlotScript can help cleaning multilingual corpora such as mC4\nand OSCAR. Second, we analyze the tokenization of a number of language models\nsuch as GPT-4 using GlotScript and provide insights on the coverage of low\nresource scripts and languages by each language model. We hope that GlotScript\nwill become a useful resource for work on low resource languages in the NLP\ncommunity. GlotScript-R and GlotScript-T are available at\nhttps://github.com/cisnlp/GlotScript.\n","authors":["Amir Hossein Kargaran","François Yvon","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2309.13320v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18647v1","updated":"2024-03-27T14:54:27Z","published":"2024-03-27T14:54:27Z","title":"SDSAT: Accelerating LLM Inference through Speculative Decoding with\n  Semantic Adaptive Tokens","summary":"  We propose an acceleration scheme for large language models (LLMs) through\nSpeculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary\nobjective of this design is to enhance the LLM model's ability to generate\ndraft tokens more accurately without compromising the model's accuracy. The\ncore strategies involve: 1) Fine-tune the model by incorporating semantic\nadaptive tokens that possess flexible decoding capabilities without changing\nits structure, allowing them to generate high-quality draft tokens. 2) By\nemploying a training method that does not affect the standard tokens, the model\ncan acquire parallel decoding abilities atop its original framework with\nminimal training overhead. 3) We have designed the \"two-step-draft-then-verify\"\ngeneration strategies using both greedy search and nucleus sampling.\nExperiments conducted on the CodeLlama-13B and 7B models have yielded speed\nincreases of over 3.5X and 3.0X, respectively. Please refer to\nhttps://github.com/hasuoshenyun/SDSAT.\n","authors":["Chengbo Liu","Yong Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18647v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.04507v2","updated":"2024-03-27T14:50:56Z","published":"2024-03-07T14:07:00Z","title":"NLPre: a revised approach towards language-centric benchmarking of\n  Natural Language Preprocessing systems","summary":"  With the advancements of transformer-based architectures, we observe the rise\nof natural language preprocessing (NLPre) tools capable of solving preliminary\nNLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or\nmorphological analysis) without any external linguistic guidance. It is arduous\nto compare novel solutions to well-entrenched preprocessing toolkits, relying\non rule-based morphological analysers or dictionaries. Aware of the\nshortcomings of existing NLPre evaluation approaches, we investigate a novel\nmethod of reliable and fair evaluation and performance reporting. Inspired by\nthe GLUE benchmark, the proposed language-centric benchmarking system enables\ncomprehensive ongoing evaluation of multiple NLPre tools, while credibly\ntracking their performance. The prototype application is configured for Polish\nand integrated with the thoroughly assembled NLPre-PL benchmark. Based on this\nbenchmark, we conduct an extensive evaluation of a variety of Polish NLPre\nsystems. To facilitate the construction of benchmarking environments for other\nlanguages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full\ncustomization of the publicly released source code of the benchmarking system.\nThe links to all the resources (deployed platforms, source code, trained\nmodels, datasets etc.) can be found on the project website:\nhttps://sites.google.com/view/nlpre-benchmark.\n","authors":["Martyna Wiącek","Piotr Rybak","Łukasz Pszenny","Alina Wróblewska"],"pdf_url":"https://arxiv.org/pdf/2403.04507v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18624v1","updated":"2024-03-27T14:34:29Z","published":"2024-03-27T14:34:29Z","title":"Vulnerability Detection with Code Language Models: How Far Are We?","summary":"  In the context of the rising interest in code language models (code LMs) and\nvulnerability detection, we study the effectiveness of code LMs for detecting\nvulnerabilities. Our analysis reveals significant shortcomings in existing\nvulnerability datasets, including poor data quality, low label accuracy, and\nhigh duplication rates, leading to unreliable model performance in realistic\nvulnerability detection scenarios. Additionally, the evaluation methods used\nwith these datasets are not representative of real-world vulnerability\ndetection.\n  To address these challenges, we introduce PrimeVul, a new dataset for\ntraining and evaluating code LMs for vulnerability detection. PrimeVul\nincorporates a novel set of data labeling techniques that achieve comparable\nlabel accuracy to human-verified benchmarks while significantly expanding the\ndataset. It also implements a rigorous data de-duplication and chronological\ndata splitting strategy to mitigate data leakage issues, alongside introducing\nmore realistic evaluation metrics and settings. This comprehensive approach\naims to provide a more accurate assessment of code LMs' performance in\nreal-world conditions.\n  Evaluating code LMs on PrimeVul reveals that existing benchmarks\nsignificantly overestimate the performance of these models. For instance, a\nstate-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on\nPrimeVul. Attempts to improve performance through advanced training techniques\nand larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin\nto random guessing in the most stringent settings. These findings underscore\nthe considerable gap between current capabilities and the practical\nrequirements for deploying code LMs in security roles, highlighting the need\nfor more innovative research in this domain.\n","authors":["Yangruibo Ding","Yanjun Fu","Omniyyah Ibrahim","Chawin Sitawarin","Xinyun Chen","Basel Alomair","David Wagner","Baishakhi Ray","Yizheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13284v2","updated":"2024-03-27T14:30:44Z","published":"2024-02-19T09:07:59Z","title":"Structure Guided Large Language Model for SQL Generation","summary":"  Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.\n","authors":["Qinggang Zhang","Junnan Dong","Hao Chen","Wentao Li","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.13284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18609v1","updated":"2024-03-27T14:26:41Z","published":"2024-03-27T14:26:41Z","title":"A survey on learning models of spiking neural membrane systems and\n  spiking neural networks","summary":"  Spiking neural networks (SNN) are a biologically inspired model of neural\nnetworks with certain brain-like properties. In the past few decades, this\nmodel has received increasing attention in computer science community, owing\nalso to the successful phenomenon of deep learning. In SNN, communication\nbetween neurons takes place through the spikes and spike trains. This\ndifferentiates these models from the ``standard'' artificial neural networks\n(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking\nneural P systems (SNPS) can be considered a branch of SNN based more on the\nprinciples of formal automata, with many variants developed within the\nframework of the membrane computing theory. In this paper, we first briefly\ncompare structure and function, advantages and drawbacks of SNN and SNPS. A key\npart of the article is a survey of recent results and applications of machine\nlearning and deep learning models of both SNN and SNPS formalisms.\n","authors":["Prithwineel Paul","Petr Sosik","Lucie Ciencialova"],"pdf_url":"https://arxiv.org/pdf/2403.18609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v3","updated":"2024-03-27T13:59:57Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2308.12531v2","updated":"2024-03-27T13:46:37Z","published":"2023-08-24T03:40:54Z","title":"CARE: Co-Attention Network for Joint Entity and Relation Extraction","summary":"  Joint entity and relation extraction is the fundamental task of information\nextraction, consisting of two subtasks: named entity recognition and relation\nextraction. However, most existing joint extraction methods suffer from issues\nof feature confusion or inadequate interaction between the two subtasks.\nAddressing these challenges, in this work, we propose a Co-Attention network\nfor joint entity and Relation Extraction (CARE). Our approach includes adopting\na parallel encoding strategy to learn separate representations for each\nsubtask, aiming to avoid feature overlap or confusion. At the core of our\napproach is the co-attention module that captures two-way interaction between\nthe two subtasks, allowing the model to leverage entity information for\nrelation prediction and vice versa, thus promoting mutual enhancement. Through\nextensive experiments on three benchmark datasets for joint entity and relation\nextraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model\noutperforms existing baseline models. Our code will be available at\nhttps://github.com/kwj0x7f/CARE.\n","authors":["Wenjun Kong","Yamei Xia"],"pdf_url":"https://arxiv.org/pdf/2308.12531v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18555v1","updated":"2024-03-27T13:34:59Z","published":"2024-03-27T13:34:59Z","title":"Debiasing Sentence Embedders through Contrastive Word Pairs","summary":"  Over the last years, various sentence embedders have been an integral part in\nthe success of current machine learning approaches to Natural Language\nProcessing (NLP). Unfortunately, multiple sources have shown that the bias,\ninherent in the datasets upon which these embedding methods are trained, is\nlearned by them. A variety of different approaches to remove biases in\nembeddings exists in the literature. Most of these approaches are applicable to\nword embeddings and in fewer cases to sentence embeddings. It is problematic\nthat most debiasing approaches are directly transferred from word embeddings,\ntherefore these approaches fail to take into account the nonlinear nature of\nsentence embedders and the embeddings they produce. It has been shown in\nliterature that bias information is still present if sentence embeddings are\ndebiased using such methods. In this contribution, we explore an approach to\nremove linear and nonlinear bias information for NLP solutions, without\nimpacting downstream performance. We compare our approach to common debiasing\nmethods on classical bias metrics and on bias metrics which take nonlinear\ninformation into account.\n","authors":["Philip Kenneweg","Sarah Schröder","Alexander Schulz","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08268v3","updated":"2024-03-27T13:29:31Z","published":"2023-11-14T16:02:16Z","title":"A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily","summary":"  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially\nharmful content. Exploring jailbreak prompts can help to better reveal the\nweaknesses of LLMs and further steer us to secure them. Unfortunately, existing\njailbreak methods either suffer from intricate manual design or require\noptimization on other white-box models, which compromises either generalization\nor efficiency. In this paper, we generalize jailbreak prompt attacks into two\naspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we\npropose ReNeLLM, an automatic framework that leverages LLMs themselves to\ngenerate effective jailbreak prompts. Extensive experiments demonstrate that\nReNeLLM significantly improves the attack success rate while greatly reducing\nthe time cost compared to existing baselines. Our study also reveals the\ninadequacy of current defense methods in safeguarding LLMs. Finally, we analyze\nthe failure of LLMs defense from the perspective of prompt execution priority,\nand propose corresponding defense strategies. We hope that our research can\ncatalyze both the academic community and LLMs developers towards the provision\nof safer and more regulated LLMs. The code is available at\nhttps://github.com/NJUNLP/ReNeLLM.\n","authors":["Peng Ding","Jun Kuang","Dan Ma","Xuezhi Cao","Yunsen Xian","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08268v3.pdf","comment":"Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18537v1","updated":"2024-03-27T13:12:57Z","published":"2024-03-27T13:12:57Z","title":"A Path Towards Legal Autonomy: An interoperable and explainable approach\n  to extracting, transforming, loading and computing legal information using\n  large language models, expert systems and Bayesian networks","summary":"  Legal autonomy - the lawful activity of artificial intelligence agents - can\nbe achieved in one of two ways. It can be achieved either by imposing\nconstraints on AI actors such as developers, deployers and users, and on AI\nresources such as data, or by imposing constraints on the range and scope of\nthe impact that AI agents can have on the environment. The latter approach\ninvolves encoding extant rules concerning AI driven devices into the software\nof AI agents controlling those devices (e.g., encoding rules about limitations\non zones of operations into the agent software of an autonomous drone device).\nThis is a challenge since the effectivity of such an approach requires a method\nof extracting, loading, transforming and computing legal information that would\nbe both explainable and legally interoperable, and that would enable AI agents\nto reason about the law. In this paper, we sketch a proof of principle for such\na method using large language models (LLMs), expert legal systems known as\nlegal decision paths, and Bayesian networks. We then show how the proposed\nmethod could be applied to extant regulation in matters of autonomous cars,\nsuch as the California Vehicle Code.\n","authors":["Axel Constant","Hannes Westermann","Bryan Wilson","Alex Kiefer","Ines Hipolito","Sylvain Pronovost","Steven Swanson","Mahault Albarracin","Maxwell J. D. Ramstead"],"pdf_url":"https://arxiv.org/pdf/2403.18537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2403.18504v1","updated":"2024-03-27T12:33:42Z","published":"2024-03-27T12:33:42Z","title":"AcTED: Automatic Acquisition of Typical Event Duration for\n  Semi-supervised Temporal Commonsense QA","summary":"  We propose a voting-driven semi-supervised approach to automatically acquire\nthe typical duration of an event and use it as pseudo-labeled data. The human\nevaluation demonstrates that our pseudo labels exhibit surprisingly high\naccuracy and balanced coverage. In the temporal commonsense QA task,\nexperimental results show that using only pseudo examples of 400 events, we\nachieve performance comparable to the existing BERT-based weakly supervised\napproaches that require a significant amount of training examples. When\ncompared to the RoBERTa baselines, our best approach establishes\nstate-of-the-art performance with a 7% improvement in Exact Match.\n","authors":["Felix Virgo","Fei Cheng","Lis Kanashiro Pereira","Masayuki Asahara","Ichiro Kobayashi","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2403.18504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16516v2","updated":"2024-03-27T12:32:31Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v2.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16432v2","updated":"2024-03-27T11:37:58Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n$\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v2.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2311.07838v3","updated":"2024-03-27T11:36:46Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v3.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2304.03544v2","updated":"2024-03-27T10:53:42Z","published":"2023-04-07T08:49:43Z","title":"InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual\n  Topic Modeling","summary":"  Cross-lingual topic models have been prevalent for cross-lingual text\nanalysis by revealing aligned latent topics. However, most existing methods\nsuffer from producing repetitive topics that hinder further analysis and\nperformance decline caused by low-coverage dictionaries. In this paper, we\npropose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).\nInstead of the direct alignment in previous work, we propose a topic alignment\nwith mutual information method. This works as a regularization to properly\nalign topics and prevent degenerate topic representations of words, which\nmitigates the repetitive topic issue. To address the low-coverage dictionary\nissue, we further propose a cross-lingual vocabulary linking method that finds\nmore linked cross-lingual words for topic alignment beyond the translations of\na given dictionary. Extensive experiments on English, Chinese, and Japanese\ndatasets demonstrate that our method outperforms state-of-the-art baselines,\nproducing more coherent, diverse, and well-aligned topics and showing better\ntransferability for cross-lingual classification tasks.\n","authors":["Xiaobao Wu","Xinshuai Dong","Thong Nguyen","Chaoqun Liu","Liangming Pan","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2304.03544v2.pdf","comment":"Accepted to AAAI2023 conference. Code is available at\n  https://github.com/BobXWu/InfoCTM"},{"id":"http://arxiv.org/abs/2309.13322v2","updated":"2024-03-27T10:50:24Z","published":"2023-09-23T09:51:37Z","title":"From Text to Source: Results in Detecting Large Language Model-Generated\n  Content","summary":"  The widespread use of Large Language Models (LLMs), celebrated for their\nability to generate human-like text, has raised concerns about misinformation\nand ethical implications. Addressing these concerns necessitates the\ndevelopment of robust methods to detect and attribute text generated by LLMs.\nThis paper investigates \"Cross-Model Detection,\" by evaluating whether a\nclassifier trained to distinguish between source LLM-generated and\nhuman-written text can also detect text from a target LLM without further\ntraining. The study comprehensively explores various LLM sizes and families,\nand assesses the impact of conversational fine-tuning techniques, quantization,\nand watermarking on classifier generalization. The research also explores Model\nAttribution, encompassing source model identification, model family, and model\nsize classification, in addition to quantization and watermarking detection.\nOur results reveal several key findings: a clear inverse relationship between\nclassifier effectiveness and model size, with larger LLMs being more\nchallenging to detect, especially when the classifier is trained on data from\nsmaller models. Training on data from similarly sized LLMs can improve\ndetection performance from larger models but may lead to decreased performance\nwhen dealing with smaller models. Additionally, model attribution experiments\nshow promising results in identifying source models and model families,\nhighlighting detectable signatures in LLM-generated text, with particularly\nremarkable outcomes in watermarking detection, while no detectable signatures\nof quantization were observed. Overall, our study contributes valuable insights\ninto the interplay of model size, family, and training data in LLM detection\nand attribution.\n","authors":["Wissam Antoun","Benoît Sagot","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2309.13322v2.pdf","comment":"Accepted to COLING-LREC 2024"},{"id":"http://arxiv.org/abs/2403.18435v1","updated":"2024-03-27T10:40:14Z","published":"2024-03-27T10:40:14Z","title":"DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via\n  Structural Word Alignment","summary":"  Recent research demonstrates the effectiveness of using pre-trained language\nmodels for legal case retrieval. Most of the existing works focus on improving\nthe representation ability for the contextualized embedding of the [CLS] token\nand calculate relevance using textual semantic similarity. However, in the\nlegal domain, textual semantic similarity does not always imply that the cases\nare relevant enough. Instead, relevance in legal cases primarily depends on the\nsimilarity of key facts that impact the final judgment. Without proper\ntreatments, the discriminative ability of learned representations could be\nlimited since legal cases are lengthy and contain numerous non-key facts. To\nthis end, we introduce DELTA, a discriminative model designed for legal case\nretrieval. The basic idea involves pinpointing key facts in legal cases and\npulling the contextualized embedding of the [CLS] token closer to the key facts\nwhile pushing away from the non-key facts, which can warm up the case embedding\nspace in an unsupervised manner. To be specific, this study brings the word\nalignment mechanism to the contextual masked auto-encoder. First, we leverage\nshallow decoders to create information bottlenecks, aiming to enhance the\nrepresentation ability. Second, we employ the deep decoder to enable\ntranslation between different structures, with the goal of pinpointing key\nfacts to enhance discriminative ability. Comprehensive experiments conducted on\npublicly available legal benchmarks show that our approach can outperform\nexisting state-of-the-art methods in legal case retrieval. It provides a new\nperspective on the in-depth understanding and processing of legal case\ndocuments.\n","authors":["Haitao Li","Qingyao Ai","Xinyan Han","Jia Chen","Qian Dong","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18435v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.18430v1","updated":"2024-03-27T10:36:17Z","published":"2024-03-27T10:36:17Z","title":"Exploring language relations through syntactic distances and geographic\n  proximity","summary":"  Languages are grouped into families that share common linguistic traits.\nWhile this approach has been successful in understanding genetic relations\nbetween diverse languages, more analyses are needed to accurately quantify\ntheir relatedness, especially in less studied linguistic levels such as syntax.\nHere, we explore linguistic distances using series of parts of speech (POS)\nextracted from the Universal Dependencies dataset. Within an\ninformation-theoretic framework, we show that employing POS trigrams maximizes\nthe possibility of capturing syntactic variations while being at the same time\ncompatible with the amount of available data. Linguistic connections are then\nestablished by assessing pairwise distances based on the POS distributions.\nIntriguingly, our analysis reveals definite clusters that correspond to well\nknown language families and groups, with exceptions explained by distinct\nmorphological typologies. Furthermore, we obtain a significant correlation\nbetween language similarity and geographic distance, which underscores the\ninfluence of spatial proximity on language kinships.\n","authors":["Juan De Gregorio","Raúl Toral","David Sánchez"],"pdf_url":"https://arxiv.org/pdf/2403.18430v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2403.18426v1","updated":"2024-03-27T10:27:28Z","published":"2024-03-27T10:27:28Z","title":"TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions","summary":"  Nowadays, individuals tend to engage in dialogues with Large Language Models,\nseeking answers to their questions. In times when such answers are readily\naccessible to anyone, the stimulation and preservation of human's cognitive\nabilities, as well as the assurance of maintaining good reasoning skills by\nhumans becomes crucial. This study addresses such needs by proposing hints\n(instead of final answers or before giving answers) as a viable solution. We\nintroduce a framework for the automatic hint generation for factoid questions,\nemploying it to construct TriviaHG, a novel large-scale dataset featuring\n160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.\nAdditionally, we present an automatic evaluation method that measures the\nConvergence and Familiarity quality attributes of hints. To evaluate the\nTriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals\nto annotate 2,791 hints and tasked 6 humans with answering questions using the\nprovided hints. The effectiveness of hints varied, with success rates of 96%,\n78%, and 36% for questions with easy, medium, and hard answers, respectively.\nMoreover, the proposed automatic evaluation methods showed a robust correlation\nwith annotators' results. Conclusively, the findings highlight three key\ninsights: the facilitative role of hints in resolving unknown questions, the\ndependence of hint quality on answer difficulty, and the feasibility of\nemploying automatic evaluation methods for hint assessment.\n","authors":["Jamshid Mozafari","Anubhav Jangra","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.18426v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18421v1","updated":"2024-03-27T10:18:21Z","published":"2024-03-27T10:18:21Z","title":"BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text","summary":"  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.\n","authors":["Elliot Bolton","Abhinav Venigalla","Michihiro Yasunaga","David Hall","Betty Xiong","Tony Lee","Roxana Daneshjou","Jonathan Frankle","Percy Liang","Michael Carbin","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2403.18421v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2403.17647v2","updated":"2024-03-27T10:07:59Z","published":"2024-03-26T12:29:18Z","title":"Intrinsic Subgraph Generation for Interpretable Graph based Visual\n  Question Answering","summary":"  The large success of deep learning based methods in Visual Question Answering\n(VQA) has concurrently increased the demand for explainable methods. Most\nmethods in Explainable Artificial Intelligence (XAI) focus on generating\npost-hoc explanations rather than taking an intrinsic approach, the latter\ncharacterizing an interpretable model. In this work, we introduce an\ninterpretable approach for graph-based VQA and demonstrate competitive\nperformance on the GQA dataset. This approach bridges the gap between\ninterpretability and performance. Our model is designed to intrinsically\nproduce a subgraph during the question-answering process as its explanation,\nproviding insight into the decision making. To evaluate the quality of these\ngenerated subgraphs, we compare them against established post-hoc\nexplainability methods for graph neural networks, and perform a human\nevaluation. Moreover, we present quantitative metrics that correlate with the\nevaluations of human assessors, acting as automatic metrics for the generated\nexplanatory subgraphs. Our implementation is available at\nhttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.\n","authors":["Pascal Tilli","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17647v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18381v1","updated":"2024-03-27T09:19:13Z","published":"2024-03-27T09:19:13Z","title":"Improving Attributed Text Generation of Large Language Models via\n  Preference Learning","summary":"  Large language models have been widely adopted in natural language\nprocessing, yet they face the challenge of generating unreliable content.\nRecent works aim to reduce misinformation and hallucinations by resorting to\nattribution as a means to provide evidence (i.e., citations). However, current\nattribution methods usually focus on the retrieval stage and automatic\nevaluation that neglect mirroring the citation mechanisms in human scholarly\nwriting to bolster credibility. In this paper, we address these challenges by\nmodelling the attribution task as preference learning and introducing an\nAutomatic Preference Optimization (APO) framework. First, we create a curated\ncollection for post-training with 6,330 examples by collecting and filtering\nfrom existing datasets. Second, considering the high cost of labelling\npreference data, we further propose an automatic method to synthesize\nattribution preference data resulting in 95,263 pairs. Moreover, inspired by\nthe human citation process, we further propose a progressive preference\noptimization method by leveraging fine-grained information. Extensive\nexperiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate\nthat APO achieves state-of-the-art citation F1 with higher answer quality.\n","authors":["Dongfang Li","Zetian Sun","Baotian Hu","Zhenyu Liu","Xinshuo Hu","Xuebo Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18381v1.pdf","comment":"23 pages, 15 tables, 2 figures"},{"id":"http://arxiv.org/abs/2312.10997v5","updated":"2024-03-27T09:16:57Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v5.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2403.18365v1","updated":"2024-03-27T08:57:21Z","published":"2024-03-27T08:57:21Z","title":"BLADE: Enhancing Black-box Large Language Models with Small\n  Domain-Specific Models","summary":"  Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable\nof addressing a diverse range of tasks. However, general LLMs, which are\ndeveloped on open-domain data, may lack the domain-specific knowledge essential\nfor tasks in vertical domains, such as legal, medical, etc. To address this\nissue, previous approaches either conduct continuous pre-training with\ndomain-specific data or employ retrieval augmentation to support general LLMs.\nUnfortunately, these strategies are either cost-intensive or unreliable in\npractical applications. To this end, we present a novel framework named BLADE,\nwhich enhances Black-box LArge language models with small Domain-spEcific\nmodels. BLADE consists of a black-box LLM and a small domain-specific LM. The\nsmall LM preserves domain-specific knowledge and offers specialized insights,\nwhile the general LLM contributes robust language comprehension and reasoning\ncapabilities. Specifically, our method involves three steps: 1) pre-training\nthe small LM with domain-specific data, 2) fine-tuning this model using\nknowledge instruction data, and 3) joint Bayesian optimization of the general\nLLM and the small LM. Extensive experiments conducted on public legal and\nmedical benchmarks reveal that BLADE significantly outperforms existing\napproaches. This shows the potential of BLADE as an effective and\ncost-efficient solution in adapting general LLMs for vertical domains.\n","authors":["Haitao Li","Qingyao Ai","Jia Chen","Qian Dong","Zhijing Wu","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18365v1.pdf","comment":"11pages"},{"id":"http://arxiv.org/abs/2307.16071v2","updated":"2024-03-27T08:56:01Z","published":"2023-07-29T20:42:50Z","title":"ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus","summary":"  We introduce \\`{I}r\\`{o}y\\`{i}nSpeech, a new corpus influenced by the desire\nto increase the amount of high quality, contemporary Yor\\`{u}b\\'{a} speech\ndata, which can be used for both Text-to-Speech (TTS) and Automatic Speech\nRecognition (ASR) tasks. We curated about 23000 text sentences from news and\ncreative writing domains with the open license CC-BY-4.0. To encourage a\nparticipatory approach to data creation, we provide 5000 curated sentences to\nthe Mozilla Common Voice platform to crowd-source the recording and validation\nof Yor\\`{u}b\\'{a} speech data. In total, we created about 42 hours of speech\ndata recorded by 80 volunteers in-house, and 6 hours of validated recordings on\nMozilla Common Voice platform. Our TTS evaluation suggests that a\nhigh-fidelity, general domain, single-speaker Yor\\`{u}b\\'{a} voice is possible\nwith as little as 5 hours of speech. Similarly, for ASR we obtained a baseline\nword error rate (WER) of 23.8.\n","authors":["Tolulope Ogunremi","Kola Tubosun","Anuoluwapo Aremu","Iroro Orife","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2307.16071v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02151v2","updated":"2024-03-27T08:43:28Z","published":"2023-05-03T14:33:23Z","title":"Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space","summary":"  Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n","authors":["Fred Philippy","Siwen Guo","Shohreh Haddadan"],"pdf_url":"https://arxiv.org/pdf/2305.02151v2.pdf","comment":"SIGTYP Workshop 2023 (co-located with EACL 2023)"},{"id":"http://arxiv.org/abs/2403.18350v1","updated":"2024-03-27T08:42:31Z","published":"2024-03-27T08:42:31Z","title":"Evaluation of Semantic Search and its Role in\n  Retrieved-Augmented-Generation (RAG) for Arabic Language","summary":"  The latest advancements in machine learning and deep learning have brought\nforth the concept of semantic similarity, which has proven immensely beneficial\nin multiple applications and has largely replaced keyword search. However,\nevaluating semantic similarity and conducting searches for a specific query\nacross various documents continue to be a complicated task. This complexity is\ndue to the multifaceted nature of the task, the lack of standard benchmarks,\nwhereas these challenges are further amplified for Arabic language. This paper\nendeavors to establish a straightforward yet potent benchmark for semantic\nsearch in Arabic. Moreover, to precisely evaluate the effectiveness of these\nmetrics and the dataset, we conduct our assessment of semantic search within\nthe framework of retrieval augmented generation (RAG).\n","authors":["Ali Mahboub","Muhy Eddin Za'ter","Bashar Alfrou","Yazan Estaitia","Adnan Jaljuli","Asma Hakouz"],"pdf_url":"https://arxiv.org/pdf/2403.18350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18349v1","updated":"2024-03-27T08:39:56Z","published":"2024-03-27T08:39:56Z","title":"Rejection Improves Reliability: Training LLMs to Refuse Unknown\n  Questions Using RL from Knowledge Feedback","summary":"  Large Language Models (LLMs) often generate erroneous outputs, known as\nhallucinations, due to their limitations in discerning questions beyond their\nknowledge scope. While addressing hallucination has been a focal point in\nresearch, previous efforts primarily concentrate on enhancing correctness\nwithout giving due consideration to the significance of rejection mechanisms.\nIn this paper, we conduct a comprehensive examination of the role of rejection,\nintroducing the notion of model reliability along with corresponding metrics.\nThese metrics measure the model's ability to provide accurate responses while\nadeptly rejecting questions exceeding its knowledge boundaries, thereby\nminimizing hallucinations. To improve the inherent reliability of LLMs, we\npresent a novel alignment framework called Reinforcement Learning from\nKnowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically\ndetermine the model's knowledge boundary and trains a reliable reward model to\nencourage the refusal of out-of-knowledge questions. Experimental results on\nmathematical questions affirm the substantial efficacy of RLKF in significantly\nenhancing LLM reliability.\n","authors":["Hongshen Xu","Zichen Zhu","Da Ma","Situo Zhang","Shuai Fan","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18346v1","updated":"2024-03-27T08:38:49Z","published":"2024-03-27T08:38:49Z","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective","summary":"  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research.\n","authors":["Meiqi Chen","Yixin Cao","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18341v1","updated":"2024-03-27T08:32:19Z","published":"2024-03-27T08:32:19Z","title":"IterAlign: Iterative Constitutional Alignment of Large Language Models","summary":"  With the rapid development of large language models (LLMs), aligning LLMs\nwith human values and societal norms to ensure their reliability and safety has\nbecome crucial. Reinforcement learning with human feedback (RLHF) and\nConstitutional AI (CAI) have been proposed for LLM alignment. However, these\nmethods require either heavy human annotations or explicitly pre-defined\nconstitutions, which are labor-intensive and resource-consuming. To overcome\nthese drawbacks, we study constitution-based LLM alignment and propose a\ndata-driven constitution discovery and self-alignment framework called\nIterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM\nand automatically discovers new constitutions using a stronger LLM. These\nconstitutions are then used to guide self-correction of the base LLM. Such a\nconstitution discovery pipeline can be run iteratively and automatically to\ndiscover new constitutions that specifically target the alignment gaps in the\ncurrent LLM. Empirical results on several safety benchmark datasets and\nmultiple base LLMs show that IterAlign successfully improves truthfulness,\nhelpfulness, harmlessness and honesty, improving the LLM alignment by up to\n$13.5\\%$ in harmlessness.\n","authors":["Xiusi Chen","Hongzhi Wen","Sreyashi Nag","Chen Luo","Qingyu Yin","Ruirui Li","Zheng Li","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18341v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18336v1","updated":"2024-03-27T08:21:01Z","published":"2024-03-27T08:21:01Z","title":"A Dataset for Pharmacovigilance in German, French, and Japanese:\n  Annotating Adverse Drug Reactions across Languages","summary":"  User-generated data sources have gained significance in uncovering Adverse\nDrug Reactions (ADRs), with an increasing number of discussions occurring in\nthe digital world. However, the existing clinical corpora predominantly revolve\naround scientific articles in English. This work presents a multilingual corpus\nof texts concerning ADRs gathered from diverse sources, including patient fora,\nsocial media, and clinical reports in German, French, and Japanese. Our corpus\ncontains annotations covering 12 entity types, four attribute types, and 13\nrelation types. It contributes to the development of real-world multilingual\nlanguage models for healthcare. We provide statistics to highlight certain\nchallenges associated with the corpus and conduct preliminary experiments\nresulting in strong baselines for extracting entities and relations between\nthese entities, both within and across languages.\n","authors":["Lisa Raithel","Hui-Syuan Yeh","Shuntaro Yada","Cyril Grouin","Thomas Lavergne","Aurélie Névéol","Patrick Paroubek","Philippe Thomas","Tomohiro Nishiyama","Sebastian Möller","Eiji Aramaki","Yuji Matsumoto","Roland Roller","Pierre Zweigenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.18336v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18327v1","updated":"2024-03-27T08:08:00Z","published":"2024-03-27T08:08:00Z","title":"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating\n  and Interpreting Formal Specifications","summary":"  Stakeholders often describe system requirements using natural language which\nare then converted to formal syntax by a domain-expert leading to increased\ndesign costs. This paper assesses the capabilities of Large Language Models\n(LLMs) in converting between natural language descriptions and formal\nspecifications. Existing work has evaluated the capabilities of LLMs in\ngenerating formal syntax such as source code but such experiments are typically\nhand-crafted and use problems that are likely to be in the training set of\nLLMs, and often require human-annotated datasets. We propose an approach that\ncan use two copies of an LLM in conjunction with an off-the-shelf verifier to\nautomatically evaluate its translation abilities without any additional human\ninput. Our approach generates formal syntax using language grammars to\nautomatically generate a dataset. We conduct an empirical evaluation to measure\nthe accuracy of this translation task and show that SOTA LLMs cannot adequately\nsolve this task, limiting their current utility in the design of complex\nsystems.\n","authors":["Rushang Karia","Daksh Dobhal","Daniel Bramblett","Pulkit Verma","Siddharth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2403.18327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18314v1","updated":"2024-03-27T07:34:44Z","published":"2024-03-27T07:34:44Z","title":"Chinese Offensive Language Detection:Current Status and Future\n  Directions","summary":"  Despite the considerable efforts being made to monitor and regulate\nuser-generated content on social media platforms, the pervasiveness of\noffensive language, such as hate speech or cyberbullying, in the digital space\nremains a significant challenge. Given the importance of maintaining a\ncivilized and respectful online environment, there is an urgent and growing\nneed for automatic systems capable of detecting offensive speech in real time.\nHowever, developing effective systems for processing languages such as Chinese\npresents a significant challenge, owing to the language's complex and nuanced\nnature, which makes it difficult to process automatically. This paper provides\na comprehensive overview of offensive language detection in Chinese, examining\ncurrent benchmarks and approaches and highlighting specific models and tools\nfor addressing the unique challenges of detecting offensive language in this\ncomplex language. The primary objective of this survey is to explore the\nexisting techniques and identify potential avenues for further research that\ncan address the cultural and linguistic complexities of Chinese.\n","authors":["Yunze Xiao","Houda Bouamor","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2403.18314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11399v2","updated":"2024-03-27T07:05:22Z","published":"2024-03-18T01:14:47Z","title":"X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment","summary":"  The impressive development of large language models (LLMs) is expanding into\nthe realm of large multimodal models (LMMs), which incorporate multiple types\nof data beyond text. However, the nature of multimodal models leads to\nsignificant expenses in the creation of training data. Furthermore,\nconstructing multilingual data for LMMs presents its own set of challenges due\nto language diversity and complexity. Therefore, in this study, we propose two\ncost-effective methods to solve this problem: (1) vocabulary expansion and\npretraining of multilingual LLM for specific languages, and (2) automatic and\nelaborate construction of multimodal datasets using GPT4-V. Based on015 these\nmethods, we constructed a 91K English-Korean-Chinese multilingual, multimodal\ntraining dataset. Additionally, we developed a bilingual multimodal model that\nexhibits excellent performance in both Korean and English, surpassing existing\napproaches.\n","authors":["Dongjae Shin","Hyunseok Lim","Inho Won","Changsu Choi","Minjun Kim","Seungwoo Song","Hangyeol Yoo","Sangmin Kim","Kyungtae Lim"],"pdf_url":"https://arxiv.org/pdf/2403.11399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12468v3","updated":"2024-03-27T06:46:56Z","published":"2023-02-24T05:48:53Z","title":"Adapting Knowledge for Few-shot Table-to-Text Generation","summary":"  Pretrained language models (PLMs) have made remarkable progress in\ntable-to-text generation tasks. However, the lack of domain-specific knowledge\nmakes it challenging to bridge the topological gap between tabular data and\ntext, especially in real-world applications with limited resources. To mitigate\nthe limitation of insufficient labeled data, we propose a novel framework:\nAdapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt\nunlabeled domain-specific knowledge into the model, which brings at least three\nbenefits: (1) it injects representation of normal table-related descriptions to\nbridge the topological gap between tabular data and texts; (2) it enables us to\nuse large amounts of unlabeled domain-specific knowledge fully, which can\nalleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it\nallows us to design various tasks to employ the domain-specific knowledge.\nExtensive experiments and analyses are conducted on three open-domain, few-shot\nnatural language generation (NLG) data sets: Humans, Songs, and Books. Compared\nto previous state-of-the-art approaches, our model achieves superior\nperformance in terms of both fluency and accuracy as judged by human and\nautomatic evaluations.\n","authors":["Zhixin Guo","Minyxuan Yan","Jiexing Qi","Jianping Zhou","Ziwei He","Guanjie Zheng","Xinbing Wang"],"pdf_url":"https://arxiv.org/pdf/2302.12468v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.04415"},{"id":"http://arxiv.org/abs/2403.18295v1","updated":"2024-03-27T06:43:58Z","published":"2024-03-27T06:43:58Z","title":"Dual Instruction Tuning with Large Language Models for Mathematical\n  Reasoning","summary":"  Recent advancements highlight the success of instruction tuning with large\nlanguage models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical\nreasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as\nincorrect, missing, and redundant steps in CoT generation leading to\ninaccuracies in answer predictions. To alleviate this problem, we propose a\ndual instruction tuning strategy to meticulously model mathematical reasoning\nfrom both forward and reverse directions. This involves introducing the\nIntermediate Reasoning State Prediction task (forward reasoning) and the\nInstruction Reconstruction task (reverse reasoning) to enhance the LLMs'\nunderstanding and execution of instructions. Training instances for these tasks\nare constructed based on existing mathematical instruction tuning datasets.\nSubsequently, LLMs undergo multi-task fine-tuning using both existing\nmathematical instructions and the newly created data. Comprehensive experiments\nvalidate the effectiveness and domain generalization of the dual instruction\ntuning strategy across various mathematical reasoning tasks.\n","authors":["Yongwei Zhou","Tiejun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.18295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06201v3","updated":"2024-03-27T06:31:42Z","published":"2024-01-11T15:45:11Z","title":"EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction","summary":"  To address intricate real-world tasks, there has been a rising interest in\ntool utilization in applications of large language models (LLMs). To develop\nLLM-based agents, it usually requires LLMs to understand many tool functions\nfrom different tool documentation. But these documentations could be diverse,\nredundant or incomplete, which immensely affects the capability of LLMs in\nusing tools. To solve this, we introduce EASYTOOL, a framework transforming\ndiverse and lengthy tool documentation into a unified and concise tool\ninstruction for easier tool usage. EasyTool purifies essential information from\nextensive tool documentation of different sources, and elaborates a unified\ninterface (i.e., tool instruction) to offer standardized tool descriptions and\nfunctionalities for LLM-based agents. Extensive experiments on multiple\ndifferent tasks demonstrate that EasyTool can significantly reduce token\nconsumption and improve the performance of tool utilization in real-world\nscenarios. Our code will be available at\n\\url{https://github.com/microsoft/JARVIS/} in the future.\n","authors":["Siyu Yuan","Kaitao Song","Jiangjie Chen","Xu Tan","Yongliang Shen","Ren Kan","Dongsheng Li","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2401.06201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18286v1","updated":"2024-03-27T06:25:40Z","published":"2024-03-27T06:25:40Z","title":"Few-Shot Recalibration of Language Models","summary":"  Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n","authors":["Xiang Lisa Li","Urvashi Khandelwal","Kelvin Guu"],"pdf_url":"https://arxiv.org/pdf/2403.18286v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.16512v2","updated":"2024-03-27T06:25:10Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18277v1","updated":"2024-03-27T06:13:04Z","published":"2024-03-27T06:13:04Z","title":"BlendX: Complex Multi-Intent Detection with Blended Patterns","summary":"  Task-oriented dialogue (TOD) systems are commonly designed with the\npresumption that each utterance represents a single intent. However, this\nassumption may not accurately reflect real-world situations, where users\nfrequently express multiple intents within a single utterance. While there is\nan emerging interest in multi-intent detection (MID), existing in-domain\ndatasets such as MixATIS and MixSNIPS have limitations in their formulation. To\naddress these issues, we present BlendX, a suite of refined datasets featuring\nmore diverse patterns than their predecessors, elevating both its complexity\nand diversity. For dataset construction, we utilize both rule-based heuristics\nas well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a\nsimilarity-driven strategy for utterance selection. To ensure the quality of\nthe proposed datasets, we also introduce three novel metrics that assess the\nstatistical properties of an utterance related to word count, conjunction use,\nand pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art\nMID models struggle with the challenges posed by the new datasets, highlighting\nthe need to reexamine the current state of the MID field. The dataset is\navailable at https://github.com/HYU-NLP/BlendX.\n","authors":["Yejin Yoon","Jungyeon Lee","Kangsan Kim","Chanhee Park","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.18277v1.pdf","comment":"Accepted to LREC-COLING2024"},{"id":"http://arxiv.org/abs/2403.18276v1","updated":"2024-03-27T06:07:05Z","published":"2024-03-27T06:07:05Z","title":"RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers","summary":"  Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate\nreproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17636v2","updated":"2024-03-27T05:55:35Z","published":"2024-03-26T12:11:29Z","title":"Mix-Initiative Response Generation with Dynamic Prefix Tuning","summary":"  Mixed initiative serves as one of the key factors in controlling conversation\ndirections. For a speaker, responding passively or leading proactively would\nresult in rather different responses. However, most dialogue systems focus on\ntraining a holistic response generation model without any distinction among\ndifferent initiatives. It leads to the cross-contamination problem, where the\nmodel confuses different initiatives and generates inappropriate responses.\nMoreover, obtaining plenty of human annotations for initiative labels can be\nexpensive. To address this issue, we propose a general mix-Initiative Dynamic\nPrefix Tuning framework (IDPT) to decouple different initiatives from the\ngeneration model, which learns initiative-aware prefixes in both supervised and\nunsupervised settings. Specifically, IDPT decouples initiative factors into\ndifferent prefix parameters and uses the attention mechanism to adjust the\nselection of initiatives in guiding generation dynamically. The prefix\nparameters can be tuned towards accurate initiative prediction as well as\nmix-initiative response generation. Extensive experiments on two public\ndialogue datasets show that the proposed IDPT outperforms previous baselines on\nboth automatic metrics and human evaluations. It also manages to generate\nappropriate responses with manipulated initiatives.\n","authors":["Yuxiang Nie","Heyan Huang","Xian-Ling Mao","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.17636v2.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2311.08590v2","updated":"2024-03-27T05:53:58Z","published":"2023-11-14T23:20:51Z","title":"PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language\n  Models","summary":"  Pre-trained language models (PLMs) show impressive performance in various\ndownstream NLP tasks. However, pre-training large language models demands\nsubstantial memory and training compute. Furthermore, due to the substantial\nresources required, many PLM weights are confidential. Consequently, users are\ncompelled to share their data with model owners for fine-tuning specific tasks.\nTo overcome the limitations, we introduce Plug-in External Memory Adaptation\n(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM\nfine-tuning without requiring access to all the weights. PEMA integrates with\ncontext representations from test data during inference to perform downstream\ntasks. It uses external memory to store PLM-generated context representations\nmapped with target tokens. Our method utilizes weight matrices of LoRA-like\nbottlenecked adapter in the PLM's final layer to enhance efficiency. Our\napproach also includes Gradual Unrolling, a novel interpolation strategy to\nimprove generation quality. We validate PEMA's effectiveness through\nexperiments on syntactic and real datasets for machine translation and style\ntransfer. Our findings show that PEMA outperforms other PEFT approaches in\nmemory and latency efficiency for training, and also excels in maintaining\nsentence meaning and generating appropriate language and styles.\n","authors":["HyunJin Kim","Young Jin Kim","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2311.08590v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18260v1","updated":"2024-03-27T05:22:06Z","published":"2024-03-27T05:22:06Z","title":"Toward Interactive Regional Understanding in Vision-Large Language\n  Models","summary":"  Recent Vision-Language Pre-training (VLP) models have demonstrated\nsignificant advancements. Nevertheless, these models heavily rely on image-text\npairs that capture only coarse and global information of an image, leading to a\nlimitation in their regional understanding ability. In this work, we introduce\n\\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,\nallowing them to understand user-indicated image regions. To achieve this, we\ndesign a simple yet innovative architecture, requiring no modifications to the\nmodel architecture or objective function. Additionally, we leverage a dataset\nthat contains a novel source of information, namely Localized Narratives, which\nhas been overlooked in previous VLP research. Our experiments demonstrate that\nour single generalist model not only achieves an interactive dialogue system\nbut also exhibits superior performance on various zero-shot region\nunderstanding tasks, without compromising its ability for global image\nunderstanding.\n","authors":["Jungbeom Lee","Sanghyuk Chun","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2403.18260v1.pdf","comment":"NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.09131v2","updated":"2024-03-27T05:02:55Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate\n  Professional and Non-Professional Styled Text","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.07950v3","updated":"2024-03-27T04:51:51Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18253v1","updated":"2024-03-27T04:51:42Z","published":"2024-03-27T04:51:42Z","title":"MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation","summary":"  Metaphors are ubiquitous in daily life, yet detecting them poses a\nsignificant challenge. Previous approaches often struggled with improper\napplication of language rules and overlooked the issue of data sparsity. To\naddress these challenges, we introduce knowledge distillation and prompt\nlearning into metaphor detection. Specifically, we devise a prompt learning\ntemplate tailored for the metaphor detection task. By masking target words and\nproviding relevant prompt information, we guide the model to accurately infer\nthe contextual meaning of these words. This approach not only mitigates the\ninterference from the literal meaning of target words but also ensures the\nproper utilization of MIP language rules for metaphor detection. Moreover, we\nemploy a teacher model equipped with prior knowledge to generate meaningful\nsoft labels, guiding the optimization process of the student model. The\ninclusion of soft labels, akin to label smoothing, helps alleviate the model's\ntendency towards over-confidence and effectively addresses the challenge of\ndata sparsity. Experimental results demonstrate that our proposed model\nachieves state-of-the-art performance across multiple datasets.\n","authors":["Kaidi Jia","Rongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.18251v1","updated":"2024-03-27T04:47:10Z","published":"2024-03-27T04:47:10Z","title":"Since the Scientific Literature Is Multilingual, Our Models Should Be\n  Too","summary":"  English has long been assumed the $\\textit{lingua franca}$ of scientific\nresearch, and this notion is reflected in the natural language processing (NLP)\nresearch involving scientific document representation. In this position piece,\nwe quantitatively show that the literature is largely multilingual and argue\nthat current models and benchmarks should reflect this linguistic diversity. We\nprovide evidence that text-based models fail to create meaningful\nrepresentations for non-English papers and highlight the negative user-facing\nimpacts of using English-only models non-discriminately across a multilingual\ndomain. We end with suggestions for the NLP community on how to improve\nperformance on non-English documents.\n","authors":["Abteen Ebrahimi","Kenneth Church"],"pdf_url":"https://arxiv.org/pdf/2403.18251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18249v1","updated":"2024-03-27T04:39:18Z","published":"2024-03-27T04:39:18Z","title":"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of\n  Real-World Detection Challenges","summary":"  Recent advancements in Large Language Models (LLMs) have enabled the creation\nof fake news, particularly in complex fields like healthcare. Studies highlight\nthe gap in the deceptive power of LLM-generated fake news with and without\nhuman assistance, yet the potential of prompting techniques has not been fully\nexplored. Thus, this work aims to determine whether prompting strategies can\neffectively narrow this gap. Current LLM-based fake news attacks require human\nintervention for information gathering and often miss details and fail to\nmaintain context consistency. Therefore, to better understand threat tactics,\nwe propose a strong fake news attack method called conditional\nVariational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,\nVLPrompt eliminates the need for additional data collection while maintaining\ncontextual coherence and preserving the intricacies of the original text. To\npropel future research on detecting VLPrompt attacks, we created a new dataset\nnamed VLPrompt fake news (VLPFN) containing real and fake texts. Our\nexperiments, including various detection methods and novel human study metrics,\nwere conducted to assess their performance on our dataset, yielding numerous\nfindings.\n","authors":["Yanshen Sun","Jianfeng He","Limeng Cui","Shuo Lei","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14965v4","updated":"2024-03-27T04:38:44Z","published":"2023-05-24T09:57:37Z","title":"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting\n  Jailbreaks","summary":"  Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating their prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited\nstudies have been conducted to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We survey existing jailbreak methods and their\neffectiveness on open-source and commercial LLMs (such as GPT-based models,\nOPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak\ndetection in terms of their effectiveness against known attacks. For further\nanalysis, we release a dataset of model outputs across 3700 jailbreak prompts\nover 4 tasks.\n","authors":["Abhinav Rao","Sachin Vashistha","Atharva Naik","Somak Aditya","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2305.14965v4.pdf","comment":"Accepted at LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2206.08657v6","updated":"2024-03-27T03:53:23Z","published":"2022-06-17T09:42:35Z","title":"BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning","summary":"  Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n","authors":["Xiao Xu","Chenfei Wu","Shachar Rosenman","Vasudev Lal","Wanxiang Che","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08657v6.pdf","comment":"Accepted by AAAI 2023, Oral"},{"id":"http://arxiv.org/abs/2306.04357v5","updated":"2024-03-27T03:06:13Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v5.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.00868v2","updated":"2024-03-27T03:03:00Z","published":"2024-03-01T04:39:16Z","title":"SoftTiger: A Clinical Foundation Model for Healthcare Workflows","summary":"  We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.\n","authors":["Ye Chen","Igor Couto","Wei Cai","Cong Fu","Bruno Dorneles"],"pdf_url":"https://arxiv.org/pdf/2403.00868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17304v2","updated":"2024-03-27T02:59:57Z","published":"2024-02-27T08:27:15Z","title":"Probing Multimodal Large Language Models for Global and Local Semantic\n  Representations","summary":"  The advancement of Multimodal Large Language Models (MLLMs) has greatly\naccelerated the development of applications in understanding integrated texts\nand images. Recent works leverage image-caption datasets to train MLLMs,\nachieving state-of-the-art performance on image-to-text tasks. However, there\nare few studies exploring which layers of MLLMs make the most effort to the\nglobal image information, which plays vital roles in multimodal comprehension\nand generation. In this study, we find that the intermediate layers of models\ncan encode more global semantic information, whose representation vectors\nperform better on visual-language entailment tasks, rather than the topmost\nlayers. We further probe models regarding local semantic representations\nthrough object recognition tasks. We find that the topmost layers may\nexcessively focus on local information, leading to a diminished ability to\nencode global information. Our code and data are released via\nhttps://github.com/kobayashikanna01/probing_MLLM_rep.\n","authors":["Mingxu Tao","Quzhe Huang","Kun Xu","Liwei Chen","Yansong Feng","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17304v2.pdf","comment":"Accepted by LREC-COLING 2024 as a short paper (Camera Ready)"},{"id":"http://arxiv.org/abs/2403.17343v2","updated":"2024-03-27T02:49:16Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.15764v2","updated":"2024-03-27T01:23:58Z","published":"2024-02-24T08:40:30Z","title":"Look Before You Leap: Problem Elaboration Prompting Improves\n  Mathematical Reasoning in Large Language Models","summary":"  Large language models (LLMs) still grapple with complex tasks like\nmathematical reasoning. Despite significant efforts invested in improving\nprefix prompts or reasoning process, the crucial role of problem context might\nhave been neglected. Accurate recognition of inputs is fundamental for solving\nmathematical tasks, as ill-formed problems could potentially mislead LLM's\nreasoning. In this study, we propose a new approach named Problem Elaboration\nPrompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,\nPEP decomposes and elucidates the problem context before reasoning, therefore\nenhancing the context modeling and parsing efficiency. Experiments across\ndatasets and models demonstrate promising performances: (1) PEP demonstrates an\noverall enhancement in various mathematical tasks. For instance, with the\nGPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through\ngreedy decoding and self-consistency, respectively. (2) PEP can be easily\nimplemented and integrated with other prompting methods. (3) PEP shows\nparticular strength in handling distraction problems.\n","authors":["Haoran Liao","Jidong Tian","Shaohua Hu","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2402.15764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18182v1","updated":"2024-03-27T01:19:23Z","published":"2024-03-27T01:19:23Z","title":"ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech\n  Corpus","summary":"  We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech\ncorpus. The corpus comprises twelve hours of Zoom meetings involving multiple\nspeakers role-playing a work situation where Students brainstorm ideas for a\ncertain topic and then discuss it with an Interlocutor. The meetings cover\ndifferent topics and are divided into phases with different language setups.\nThe corpus presents a challenging set for automatic speech recognition (ASR),\nincluding two languages (Arabic and English) with Arabic spoken in multiple\nvariants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English\nused with various accents. Adding to the complexity of the corpus, there is\nalso code-switching between these languages and dialects. As part of our work,\nwe take inspiration from established sets of transcription guidelines to\npresent a set of guidelines handling issues of conversational speech,\ncode-switching and orthography of both languages. We further enrich the corpus\nwith two layers of annotations; (1) dialectness level annotation for the\nportion of the corpus where mixing occurs between different variants of Arabic,\nand (2) automatic morphological annotations, including tokenization,\nlemmatization, and part-of-speech tagging.\n","authors":["Injy Hamed","Fadhl Eryani","David Palfreyman","Nizar Habash"],"pdf_url":"https://arxiv.org/pdf/2403.18182v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2301.10856v3","updated":"2024-03-27T00:47:21Z","published":"2023-01-25T22:27:40Z","title":"Partial Mobilization: Tracking Multilingual Information Flows Amongst\n  Russian Media Outlets and Telegram","summary":"  In response to disinformation and propaganda from Russian online media\nfollowing the invasion of Ukraine, Russian media outlets such as Russia Today\nand Sputnik News were banned throughout Europe. To maintain viewership, many of\nthese Russian outlets began to heavily promote their content on messaging\nservices like Telegram. In this work, we study how 16 Russian media outlets\ninteracted with and utilized 732 Telegram channels throughout 2022. Leveraging\nthe foundational model MPNet, DP-means clustering, and Hawkes processes, we\ntrace how narratives spread between news sites and Telegram channels. We show\nthat news outlets not only propagate existing narratives through Telegram but\nthat they source material from the messaging platform. For example, across the\nwebsites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of\narticles discussed content that originated/resulted from activity on Telegram.\nFinally, tracking the spread of individual topics, we measure the rate at which\nnews outlets and Telegram channels disseminate content within the Russian media\necosystem, finding that websites like ura.news and Telegram channels such as\n@genshab are the most effective at disseminating their content.\n","authors":["Hans W. A. Hanley","Zakir Durumeric"],"pdf_url":"https://arxiv.org/pdf/2301.10856v3.pdf","comment":"Accepted to ICWSM 2024"},{"id":"http://arxiv.org/abs/2308.11138v3","updated":"2024-03-27T00:29:33Z","published":"2023-08-22T02:39:42Z","title":"NLP-based detection of systematic anomalies among the narratives of\n  consumer complaints","summary":"  We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau.\n","authors":["Peiheng Gao","Ning Sun","Xuefeng Wang","Chen Yang","Ričardas Zitikis"],"pdf_url":"https://arxiv.org/pdf/2308.11138v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18167v1","updated":"2024-03-27T00:23:03Z","published":"2024-03-27T00:23:03Z","title":"Mechanisms of non-factual hallucinations in language models","summary":"  State-of-the-art language models (LMs) sometimes generate non-factual\nhallucinations that misalign with world knowledge. Despite extensive efforts to\ndetect and mitigate hallucinations, understanding their internal mechanisms\nremains elusive. Our study investigates the mechanistic causes of\nhallucination, specifically non-factual ones where the LM incorrectly predicts\nobject attributes in response to subject-relation queries. With causal\nmediation analysis and embedding space projection, we identify two general\nmechanistic causes of hallucinations shared across LMs of various scales and\ndesigns: 1) insufficient subject attribute knowledge in lower layer MLPs, and\n2) failing to select the correct object attribute in upper layer attention\nheads and MLPs. These two mechanisms exhibit varying degrees of subject-object\nassociation, predictive uncertainty and perturbation robustness. Additionally,\nwe scrutinize LM pre-training checkpoints, revealing distinct learning dynamics\nfor the two mechanistic causes of hallucinations. We also highlight how\nattribution features from our causal analysis can effectively construct\nhallucination detectors. Our work proposes a mechanistic understanding of LM\nfactual errors.\n","authors":["Lei Yu","Meng Cao","Jackie Chi Kit Cheung","Yue Dong"],"pdf_url":"https://arxiv.org/pdf/2403.18167v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.18821v1","updated":"2024-03-27T17:59:56Z","published":"2024-03-27T17:59:56Z","title":"Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and\n  Benchmark","summary":"  We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/\n","authors":["Ziyang Chen","Israel D. Gebru","Christian Richardt","Anurag Kumar","William Laney","Andrew Owens","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2403.18821v1.pdf","comment":"Accepted to CVPR 2024. Project site:\n  https://facebookresearch.github.io/real-acoustic-fields/"},{"id":"http://arxiv.org/abs/2403.18820v1","updated":"2024-03-27T17:59:54Z","published":"2024-03-27T17:59:54Z","title":"MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view\n  Human Performance Capture and Rendering","summary":"  Faithful human performance capture and free-view rendering from sparse RGB\nobservations is a long-standing problem in Vision and Graphics. The main\nchallenges are the lack of observations and the inherent ambiguities of the\nsetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,\nwhich have shown great promise in capturing high-frequency appearance and\ngeometry details in dense setups, perform poorly when na\\\"ively supervising\nthem on sparse camera views, as the field simply overfits to the sparse-view\ninputs. To address this, we propose MetaCap, a method for efficient and\nhigh-quality geometry recovery and novel view synthesis given very sparse or\neven a single view of the human. Our key idea is to meta-learn the radiance\nfield weights solely from potentially sparse multi-view videos, which can serve\nas a prior when fine-tuning them on sparse imagery depicting the human. This\nprior provides a good network weight initialization, thereby effectively\naddressing ambiguities in sparse-view capture. Due to the articulated structure\nof the human body and motion-induced surface deformations, learning such a\nprior is non-trivial. Therefore, we propose to meta-learn the field weights in\na pose-canonicalized space, which reduces the spatial feature range and makes\nfeature learning more effective. Consequently, one can fine-tune our field\nparameters to quickly generalize to unseen poses, novel illumination conditions\nas well as novel and sparse (even monocular) camera views. For evaluating our\nmethod under different scenarios, we collect a new dataset, WildDynaCap, which\ncontains subjects captured in, both, a dense camera dome and in-the-wild sparse\ncamera rigs, and demonstrate superior results compared to recent\nstate-of-the-art methods on both public and WildDynaCap dataset.\n","authors":["Guoxing Sun","Rishabh Dabral","Pascal Fua","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2403.18820v1.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/MetaCap/"},{"id":"http://arxiv.org/abs/2403.18819v1","updated":"2024-03-27T17:59:53Z","published":"2024-03-27T17:59:53Z","title":"Benchmarking Object Detectors with COCO: A New Path Forward","summary":"  The Common Objects in Context (COCO) dataset has been instrumental in\nbenchmarking object detectors over the past decade. Like every dataset, COCO\ncontains subtle errors and imperfections stemming from its annotation\nprocedure. With the advent of high-performing models, we ask whether these\nerrors of COCO are hindering its utility in reliably benchmarking further\nprogress. In search for an answer, we inspect thousands of masks from COCO\n(2017 version) and uncover different types of errors such as imprecise mask\nboundaries, non-exhaustively annotated instances, and mislabeled masks. Due to\nthe prevalence of COCO, we choose to correct these errors to maintain\ncontinuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner\nset of annotations with visibly better mask quality than COCO-2017. We evaluate\nfifty object detectors and find that models that predict visually sharper masks\nscore higher on COCO-ReM, affirming that they were being incorrectly penalized\ndue to errors in COCO-2017. Moreover, our models trained using COCO-ReM\nconverge faster and score higher than their larger variants trained using\nCOCO-2017, highlighting the importance of data quality in improving object\ndetectors. With these findings, we advocate using COCO-ReM for future object\ndetection research. Our dataset is available at https://cocorem.xyz\n","authors":["Shweta Singh","Aayan Yadav","Jitesh Jain","Humphrey Shi","Justin Johnson","Karan Desai"],"pdf_url":"https://arxiv.org/pdf/2403.18819v1.pdf","comment":"Technical report. Dataset website: https://cocorem.xyz and code:\n  https://github.com/kdexd/coco-rem"},{"id":"http://arxiv.org/abs/2403.18818v1","updated":"2024-03-27T17:59:52Z","published":"2024-03-27T17:59:52Z","title":"ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object\n  Removal and Insertion","summary":"  Diffusion models have revolutionized image editing but often generate images\nthat violate physical laws, particularly the effects of objects on the scene,\ne.g., occlusions, shadows, and reflections. By analyzing the limitations of\nself-supervised approaches, we propose a practical solution centered on a\n\\q{counterfactual} dataset. Our method involves capturing a scene before and\nafter removing a single object, while minimizing other changes. By fine-tuning\na diffusion model on this dataset, we are able to not only remove objects but\nalso their effects on the scene. However, we find that applying this approach\nfor photorealistic object insertion requires an impractically large dataset. To\ntackle this challenge, we propose bootstrap supervision; leveraging our object\nremoval model trained on a small counterfactual dataset, we synthetically\nexpand this dataset considerably. Our approach significantly outperforms prior\nmethods in photorealistic object removal and insertion, particularly at\nmodeling the effects of objects on the scene.\n","authors":["Daniel Winter","Matan Cohen","Shlomi Fruchter","Yael Pritch","Alex Rav-Acha","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2403.18818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18816v1","updated":"2024-03-27T17:59:33Z","published":"2024-03-27T17:59:33Z","title":"Garment3DGen: 3D Garment Stylization and Texture Generation","summary":"  We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. First, we leverage the recent progress of\nimage to 3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nSecond, we introduce carefully designed losses that allow the input base mesh\nto freely deform towards the desired target, yet preserve mesh quality and\ntopology such that they can be simulated. Finally, a texture estimation module\ngenerates high-fidelity texture maps that are globally and locally consistent\nand faithfully capture the input guidance, allowing us to render the generated\n3D assets. With Garment3DGen users can generate the textured 3D garment of\ntheir choice without the need of artist intervention. One can provide a textual\nprompt describing the garment they desire to generate a simulation-ready 3D\nasset. We present a plethora of quantitative and qualitative comparisons on\nvarious assets both real and generated and provide use-cases of how one can\ngenerate simulation-ready 3D garments.\n","authors":["Nikolaos Sarafianos","Tuur Stuyck","Xiaoyu Xiang","Yilei Li","Jovan Popovic","Rakesh Ranjan"],"pdf_url":"https://arxiv.org/pdf/2403.18816v1.pdf","comment":"Project Page: https://nsarafianos.github.io/garment3dgen"},{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18811v1","updated":"2024-03-27T17:57:02Z","published":"2024-03-27T17:57:02Z","title":"Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance\n  Accompaniment","summary":"  We introduce a novel task within the field of 3D dance generation, termed\ndance accompaniment, which necessitates the generation of responsive movements\nfrom a dance partner, the \"follower\", synchronized with the lead dancer's\nmovements and the underlying musical rhythm. Unlike existing solo or group\ndance generation tasks, a duet dance scenario entails a heightened degree of\ninteraction between the two participants, requiring delicate coordination in\nboth pose and position. To support this task, we first build a large-scale and\ndiverse duet interactive dance dataset, DD100, by recording about 117 minutes\nof professional dancers' performances. To address the challenges inherent in\nthis task, we propose a GPT-based model, Duolando, which autoregressively\npredicts the subsequent tokenized motion conditioned on the coordinated\ninformation of the music, the leader's and the follower's movements. To further\nenhance the GPT's capabilities of generating stable results on unseen\nconditions (music and leader motions), we devise an off-policy reinforcement\nlearning strategy that allows the model to explore viable trajectories from\nout-of-distribution samplings, guided by human-defined rewards. Based on the\ncollected dataset and proposed method, we establish a benchmark with several\ncarefully designed metrics.\n","authors":["Li Siyao","Tianpei Gu","Zhitao Yang","Zhengyu Lin","Ziwei Liu","Henghui Ding","Lei Yang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2403.18811v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2311.10319v4","updated":"2024-03-27T17:41:50Z","published":"2023-11-17T04:04:29Z","title":"Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification","summary":"  Advancements in clinical treatment are increasingly constrained by the\nlimitations of supervised learning techniques, which depend heavily on large\nvolumes of annotated data. The annotation process is not only costly but also\ndemands substantial time from clinical specialists. Addressing this issue, we\nintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)\npipeline, a novel approach that leverages advancements in self-supervised and\nsemi-supervised learning. These techniques engage in auxiliary tasks that do\nnot require labeling, thus simplifying the scaling of machine supervision\ncompared to fully-supervised methods. Our study benchmarks these techniques on\nthree distinct medical imaging datasets to evaluate their effectiveness in\nclassification and segmentation tasks. Notably, we observed that self\nsupervised learning significantly surpassed the performance of supervised\nmethods in the classification of all evaluated datasets. Remarkably, the\nsemi-supervised approach demonstrated superior outcomes in segmentation,\noutperforming fully-supervised methods while using 50% fewer labels across all\ndatasets. In line with our commitment to contributing to the scientific\ncommunity, we have made the S4MI code openly accessible, allowing for broader\napplication and further development of these methods.\n","authors":["Pranav Singh","Raviteja Chukkapalli","Shravan Chaudhari","Luoyao Chen","Mei Chen","Jinqian Pan","Craig Smuda","Jacopo Cirrone"],"pdf_url":"https://arxiv.org/pdf/2311.10319v4.pdf","comment":"Seventeen pages (incl. references), five figures, and one table.\n  (Under Review)"},{"id":"http://arxiv.org/abs/2403.18795v1","updated":"2024-03-27T17:40:14Z","published":"2024-03-27T17:40:14Z","title":"Gamba: Marry Gaussian Splatting with Mamba for single view 3D\n  reconstruction","summary":"  We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.\n","authors":["Qiuhong Shen","Xuanyu Yi","Zike Wu","Pan Zhou","Hanwang Zhang","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18791v1","updated":"2024-03-27T17:35:24Z","published":"2024-03-27T17:35:24Z","title":"Object Pose Estimation via the Aggregation of Diffusion Features","summary":"  Estimating the pose of objects from images is a crucial task of 3D scene\nunderstanding, and recent approaches have shown promising results on very large\nbenchmarks. However, these methods experience a significant performance drop\nwhen dealing with unseen objects. We believe that it results from the limited\ngeneralizability of image features. To address this problem, we have an\nin-depth analysis on the features of diffusion models, e.g. Stable Diffusion,\nwhich hold substantial potential for modeling unseen objects. Based on this\nanalysis, we then innovatively introduce these diffusion features for object\npose estimation. To achieve this, we propose three distinct architectures that\ncan effectively capture and aggregate diffusion features of different\ngranularity, greatly improving the generalizability of object pose estimation.\nOur approach outperforms the state-of-the-art methods by a considerable margin\non three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our\nmethod achieves higher accuracy than the previous best arts on unseen objects:\n98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the\nstrong generalizability of our method. Our code is released at\nhttps://github.com/Tianfu18/diff-feats-pose.\n","authors":["Tianfu Wang","Guosheng Hu","Hongguang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18791v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.18784v1","updated":"2024-03-27T17:32:04Z","published":"2024-03-27T17:32:04Z","title":"SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable\n  Surface","summary":"  We present SplatFace, a novel Gaussian splatting framework designed for 3D\nhuman face reconstruction without reliance on accurate pre-determined geometry.\nOur method is designed to simultaneously deliver both high-quality novel view\nrendering and accurate 3D mesh reconstructions. We incorporate a generic 3D\nMorphable Model (3DMM) to provide a surface geometric structure, making it\npossible to reconstruct faces with a limited set of input images. We introduce\na joint optimization strategy that refines both the Gaussians and the morphable\nsurface through a synergistic non-rigid alignment process. A novel distance\nmetric, splat-to-surface, is proposed to improve alignment by considering both\nthe Gaussian position and covariance. The surface information is also utilized\nto incorporate a world-space densification process, resulting in superior\nreconstruction quality. Our experimental analysis demonstrates that the\nproposed method is competitive with both other Gaussian splatting techniques in\nnovel view synthesis and other 3D reconstruction methods in producing 3D face\nmeshes with high geometric precision.\n","authors":["Jiahao Luo","Jing Liu","James Davis"],"pdf_url":"https://arxiv.org/pdf/2403.18784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.01220v2","updated":"2024-03-27T17:23:16Z","published":"2023-12-02T20:11:48Z","title":"Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation","summary":"  Detecting objects in low-light scenarios presents a persistent challenge, as\ndetectors trained on well-lit data exhibit significant performance degradation\non low-light data due to low visibility. Previous methods mitigate this issue\nby exploring image enhancement or object detection techniques with real\nlow-light image datasets. However, the progress is impeded by the inherent\ndifficulties about collecting and annotating low-light images. To address this\nchallenge, we propose to boost low-light object detection with zero-shot\nday-night domain adaptation, which aims to generalize a detector from well-lit\nscenarios to low-light ones without requiring real low-light data. Revisiting\nRetinex theory in the low-level vision, we first design a reflectance\nrepresentation learning module to learn Retinex-based illumination invariance\nin images with a carefully designed illumination invariance reinforcement\nstrategy. Next, an interchange-redecomposition-coherence procedure is\nintroduced to improve over the vanilla Retinex image decomposition process by\nperforming two sequential image decompositions and introducing a\nredecomposition cohering loss. Extensive experiments on ExDark, DARK FACE, and\nCODaN datasets show strong low-light generalizability of our method. Our code\nis available at https://github.com/ZPDu/DAI-Net.\n","authors":["Zhipeng Du","Miaojing Shi","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2312.01220v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18762v1","updated":"2024-03-27T17:01:10Z","published":"2024-03-27T17:01:10Z","title":"ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place\n  Recognition","summary":"  Place recognition is an important task for robots and autonomous cars to\nlocalize themselves and close loops in pre-built maps. While single-modal\nsensor-based methods have shown satisfactory performance, cross-modal place\nrecognition that retrieving images from a point-cloud database remains a\nchallenging problem. Current cross-modal methods transform images into 3D\npoints using depth estimation for modality conversion, which are usually\ncomputationally intensive and need expensive labeled data for depth\nsupervision. In this work, we introduce a fast and lightweight framework to\nencode images and point clouds into place-distinctive descriptors. We propose\nan effective Field of View (FoV) transformation module to convert point clouds\ninto an analogous modality as images. This module eliminates the necessity for\ndepth estimation and helps subsequent modules achieve real-time performance. We\nfurther design a non-negative factorization-based encoder to extract mutually\nconsistent semantic features between point clouds and images. This encoder\nyields more distinctive global descriptors for retrieval. Experimental results\non the KITTI dataset show that our proposed methods achieve state-of-the-art\nperformance while running in real time. Additional evaluation on the HAOMO\ndataset covering a 17 km trajectory further shows the practical generalization\ncapabilities. We have released the implementation of our methods as open source\nat: https://github.com/haomo-ai/ModaLink.git.\n","authors":["Weidong Xie","Lun Luo","Nanfei Ye","Yi Ren","Shaoyi Du","Minhang Wang","Jintao Xu","Rui Ai","Weihao Gu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18762v1.pdf","comment":"8 pages, 11 figures, conference"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2303.09817v2","updated":"2024-03-27T16:52:59Z","published":"2023-03-17T07:53:18Z","title":"Interpretable machine learning for time-to-event prediction in medicine\n  and healthcare","summary":"  Time-to-event prediction, e.g. cancer survival analysis or hospital length of\nstay, is a highly prominent machine learning task in medical and healthcare\napplications. However, only a few interpretable machine learning methods comply\nwith its challenges. To facilitate a comprehensive explanatory analysis of\nsurvival models, we formally introduce time-dependent feature effects and\nglobal feature importance explanations. We show how post-hoc interpretation\nmethods allow for finding biases in AI systems predicting length of stay using\na novel multi-modal dataset created from 1235 X-ray images with textual\nradiology reports annotated by human experts. Moreover, we evaluate cancer\nsurvival models beyond predictive performance to include the importance of\nmulti-omics feature groups based on a large-scale benchmark comprising 11\ndatasets from The Cancer Genome Atlas (TCGA). Model developers can use the\nproposed methods to debug and improve machine learning algorithms, while\nphysicians can discover disease biomarkers and assess their significance. We\nhope the contributed open data and code resources facilitate future work in the\nemerging research direction of explainable survival analysis.\n","authors":["Hubert Baniecki","Bartlomiej Sobieski","Patryk Szatkowski","Przemyslaw Bombinski","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2303.09817v2.pdf","comment":"An extended version of an AIME 2023 paper submitted to Artificial\n  Intelligence in Medicine"},{"id":"http://arxiv.org/abs/2403.14623v2","updated":"2024-03-27T16:49:35Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/checkcrab/SDSB.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11107v2","updated":"2024-03-27T16:48:34Z","published":"2024-03-17T06:21:21Z","title":"Self-supervised co-salient object detection via feature correspondence\n  at multiple scales","summary":"  Our paper introduces a novel two-stage self-supervised approach for detecting\nco-occurring salient objects (CoSOD) in image groups without requiring\nsegmentation annotations. Unlike existing unsupervised methods that rely solely\non patch-level information (e.g. clustering patch descriptors) or on\ncomputation heavy off-the-shelf components for CoSOD, our lightweight model\nleverages feature correspondences at both patch and region levels,\nsignificantly improving prediction performance. In the first stage, we train a\nself-supervised network that detects co-salient regions by computing local\npatch-level feature correspondences across images. We obtain the segmentation\npredictions using confidence-based adaptive thresholding. In the next stage, we\nrefine these intermediate segmentations by eliminating the detected regions\n(within each image) whose averaged feature representations are dissimilar to\nthe foreground feature representation averaged across all the cross-attention\nmaps (from the previous stage). Extensive experiments on three CoSOD benchmark\ndatasets show that our self-supervised model outperforms the corresponding\nstate-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model\nhas a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably,\nour self-supervised model also outperforms several recent fully supervised\nCoSOD models on the three test datasets (e.g., on the CoCA dataset, our model\nhas a 4.6% F-measure gain over a recent supervised CoSOD model).\n","authors":["Souradeep Chakraborty","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2403.11107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18734v1","updated":"2024-03-27T16:22:45Z","published":"2024-03-27T16:22:45Z","title":"A vascular synthetic model for improved aneurysm segmentation and\n  detection via Deep Neural Networks","summary":"  We hereby present a full synthetic model, able to mimic the various\nconstituents of the cerebral vascular tree: the cerebral arteries, the\nbifurcations and the intracranial aneurysms. By building this model, our goal\nwas to provide a substantial dataset of brain arteries which could be used by a\n3D Convolutional Neural Network (CNN) to either segment or detect/recognize\nvarious vascular diseases (such as artery dissection/thrombosis) or even some\nportions of the cerebral vasculature, such as the bifurcations or aneurysms. In\nthis study, we will particularly focus on Intra-Cranial Aneurysm (ICA)\ndetection and segmentation. The cerebral aneurysms most often occur on a\nparticular structure of the vascular tree named the Circle of Willis. Various\nstudies have been conducted to detect and monitor the ICAs and those based on\nDeep Learning (DL) achieve the best performances. Specifically, in this work,\nwe propose a full synthetic 3D model able to mimic the brain vasculature as\nacquired by Magnetic Resonance Angiography (MRA), and more particularly the\nTime Of Flight (TOF) principle. Among the various MRI modalities, the MRA-TOF\nallows to have a relatively good rendering of the blood vessels and is\nnon-invasive (no contrast liquid injection). Our model has been designed to\nsimultaneously mimic the arteries geometry, the ICA shape and the background\nnoise. The geometry of the vascular tree is modeled thanks to an interpolation\nwith 3D Spline functions, and the statistical properties of the background MRI\nnoise is collected from MRA acquisitions and reproduced within the model. In\nthis work, we thoroughly describe the synthetic vasculature model, we build up\na neural network designed for ICA segmentation and detection, and finally, we\ncarry out an in-depth evaluation of the performance gap gained thanks to the\nsynthetic model data augmentation.\n","authors":["Rafic Nader","Florent Autrusseau","Vincent L'Allinec","Romain Bourcier"],"pdf_url":"https://arxiv.org/pdf/2403.18734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18730v1","updated":"2024-03-27T16:20:55Z","published":"2024-03-27T16:20:55Z","title":"Towards Image Ambient Lighting Normalization","summary":"  Lighting normalization is a crucial but underexplored restoration task with\nbroad applications. However, existing works often simplify this task within the\ncontext of shadow removal, limiting the light sources to one and\noversimplifying the scene, thus excluding complex self-shadows and restricting\nsurface classes to smooth ones. Although promising, such simplifications hinder\ngeneralizability to more realistic settings encountered in daily use. In this\npaper, we propose a new challenging task termed Ambient Lighting Normalization\n(ALN), which enables the study of interactions between shadows, unifying image\nrestoration and shadow removal in a broader context. To address the lack of\nappropriate datasets for ALN, we introduce the large-scale high-resolution\ndataset Ambient6K, comprising samples obtained from multiple light sources and\nincluding self-shadows resulting from complex geometries, which is the first of\nits kind. For benchmarking, we select various mainstream methods and rigorously\nevaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong\nbaseline that maximizes Image-Frequency joint entropy to selectively restore\nlocal areas under different lighting conditions, without relying on shadow\nlocalization priors. Experiments show that IFBlend achieves SOTA scores on\nAmbient6K and exhibits competitive performance on conventional shadow removal\nbenchmarks compared to shadow-specific models with mask priors. The dataset,\nbenchmark, and code are available at https://github.com/fvasluianu97/IFBlend.\n","authors":["Florin-Alexandru Vasluianu","Tim Seizinger","Zongwei Wu","Rakesh Ranjan","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2403.18730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09992v3","updated":"2024-03-27T16:20:52Z","published":"2023-03-17T14:07:55Z","title":"LION: Implicit Vision Prompt Tuning","summary":"  Despite recent competitive performance across a range of vision tasks, vision\nTransformers still have an issue of heavy computational costs. Recently, vision\nprompt learning has provided an economic solution to this problem without\nfine-tuning the whole large-scale models. However, the efficiency of existing\nmodels are still far from satisfactory due to insertion of extensive prompts\nblocks and trick prompt designs. In this paper, we propose an efficient vision\nmodel named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep\nimplicit models with stable memory costs for various complex tasks. In\nparticular, we merely insect two equilibrium implicit layers in two ends of the\npre-trained main backbone with parameters in the backbone frozen. Moreover, we\nprune the parameters in these two layers according to lottery hypothesis. The\nperformance obtained by our LION are promising on a wide range of datasets. In\nparticular, our LION reduces up to 11.5% of training parameter numbers while\nobtaining higher performance compared with the state-of-the-art baseline VPT,\nespecially under challenging scenes. Furthermore, we find that our proposed\nLION had a good generalization performance, making it an easy way to boost\ntransfer learning in the future.\n","authors":["Haixin Wang","Jianlong Chang","Xiao Luo","Jinan Sun","Zhouchen Lin","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2303.09992v3.pdf","comment":"Accepted by AAAI2024; 9 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18714v1","updated":"2024-03-27T16:02:00Z","published":"2024-03-27T16:02:00Z","title":"Bringing Textual Prompt to AI-Generated Image Quality Assessment","summary":"  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike\ntraditional image quality assessment (IQA) on natural scenarios, AGIs quality\nassessment (AGIQA) takes the correspondence of image and its textual prompt\ninto consideration. This is coupled in the ground truth score, which confuses\nthe unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs\nQuality Assessment via Image and Prompt), a multimodal framework for AGIQA via\ncorresponding image and prompt incorporation. Specifically, we propose a novel\nincremental pretraining task named Image2Prompt for better understanding of\nAGIs and their corresponding textual prompts. An effective and efficient\nimage-prompt fusion module, along with a novel special [QA] token, are also\napplied. Both are plug-and-play and beneficial for the cooperation of image and\nits corresponding prompt. Experiments demonstrate that our IP-IQA achieves the\nstate-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.\n","authors":["Bowen Qu","Haohui Li","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18714v1.pdf","comment":"6 pages, 3 figures, accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.18711v1","updated":"2024-03-27T15:58:25Z","published":"2024-03-27T15:58:25Z","title":"SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable\n  Transient-Free 3D reconstruction from Satellite Imagery","summary":"  Current stereo-vision pipelines produce high accuracy 3D reconstruction when\nusing multiple pairs or triplets of satellite images. However, these pipelines\nare sensitive to the changes between images that can occur as a result of\nmulti-date acquisitions. Such variations are mainly due to variable shadows,\nreflexions and transient objects (cars, vegetation). To take such changes into\naccount, Neural Radiance Fields (NeRF) have recently been applied to multi-date\nsatellite imagery. However, Neural methods are very compute-intensive, taking\ndozens of hours to learn, compared with minutes for standard stereo-vision\npipelines. Following the ideas of Instant Neural Graphics Primitives we propose\nto use an efficient sampling strategy and multi-resolution hash encoding to\naccelerate the learning. Our model, Satellite Neural Graphics Primitives\n(SAT-NGP) decreases the learning time to 15 minutes while maintaining the\nquality of the 3D reconstruction.\n","authors":["Camille Billouard","Dawa Derksen","Emmanuelle Sarrazin","Bruno Vallet"],"pdf_url":"https://arxiv.org/pdf/2403.18711v1.pdf","comment":"5 pages, 3 figures, 1 table; Accepted to International Geoscience and\n  Remote Sensing Symposium (IGARSS) 2024; Code available at\n  https://github.com/Ellimac0/SAT-NGP"},{"id":"http://arxiv.org/abs/2403.18708v1","updated":"2024-03-27T15:56:42Z","published":"2024-03-27T15:56:42Z","title":"Dense Vision Transformer Compression with Few Samples","summary":"  Few-shot model compression aims to compress a large model into a more compact\none with only a tiny training set (even without labels). Block-level pruning\nhas recently emerged as a leading technique in achieving high accuracy and low\nlatency in few-shot CNN compression. But, few-shot compression for Vision\nTransformers (ViT) remains largely unexplored, which presents a new challenge.\nIn particular, the issue of sparse compression exists in traditional CNN\nfew-shot methods, which can only produce very few compressed models of\ndifferent model sizes. This paper proposes a novel framework for few-shot ViT\ncompression named DC-ViT. Instead of dropping the entire block, DC-ViT\nselectively eliminates the attention module while retaining and reusing\nportions of the MLP module. DC-ViT enables dense compression, which outputs\nnumerous compressed models that densely populate the range of model complexity.\nDC-ViT outperforms state-of-the-art few-shot compression methods by a\nsignificant margin of 10 percentage points, along with lower latency in the\ncompression of ViT and its variants.\n","authors":["Hanxiao Zhang","Yifan Zhou","Guo-Hua Wang","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18708v1.pdf","comment":"Accepted to CVPR 2024. Note: Jianxin Wu is a contributing author for\n  the arXiv version of this paper but is not listed as an author in the CVPR\n  version due to his role as Program Chair"},{"id":"http://arxiv.org/abs/2401.15120v2","updated":"2024-03-27T15:49:52Z","published":"2024-01-26T03:44:58Z","title":"Incorporating simulated spatial context information improves the\n  effectiveness of contrastive learning models","summary":"  Visual learning often occurs in a specific context, where an agent acquires\nskills through exploration and tracking of its location in a consistent\nenvironment. The historical spatial context of the agent provides a similarity\nsignal for self-supervised contrastive learning. We present a unique approach,\ntermed Environmental Spatial Similarity (ESS), that complements existing\ncontrastive learning methods. Using images from simulated, photorealistic\nenvironments as an experimental setting, we demonstrate that ESS outperforms\ntraditional instance discrimination approaches. Moreover, sampling additional\ndata from the same environment substantially improves accuracy and provides new\naugmentations. ESS allows remarkable proficiency in room classification and\nspatial prediction tasks, especially in unfamiliar environments. This learning\nparadigm has the potential to enable rapid visual learning in agents operating\nin new environments with unique visual characteristics. Potentially\ntransformative applications span from robotics to space exploration. Our proof\nof concept demonstrates improved efficiency over methods that rely on\nextensive, disconnected datasets.\n","authors":["Lizhen Zhu","James Z. Wang","Wonseuk Lee","Brad Wyble"],"pdf_url":"https://arxiv.org/pdf/2401.15120v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18690v1","updated":"2024-03-27T15:41:23Z","published":"2024-03-27T15:41:23Z","title":"Annolid: Annotate, Segment, and Track Anything You Need","summary":"  Annolid is a deep learning-based software package designed for the\nsegmentation, labeling, and tracking of research targets within video files,\nfocusing primarily on animal behavior analysis. Based on state-of-the-art\ninstance segmentation methods, Annolid now harnesses the Cutie video object\nsegmentation model to achieve resilient, markerless tracking of multiple\nanimals from single annotated frames, even in environments in which they may be\npartially or entirely concealed by environmental features or by one another.\nOur integration of Segment Anything and Grounding-DINO strategies additionally\nenables the automatic masking and segmentation of recognizable animals and\nobjects by text command, removing the need for manual annotation. Annolid's\ncomprehensive approach to object segmentation flexibly accommodates a broad\nspectrum of behavior analysis applications, enabling the classification of\ndiverse behavioral states such as freezing, digging, pup huddling, and social\ninteractions in addition to the tracking of animals and their body parts.\n","authors":["Chen Yang","Thomas A. Cleland"],"pdf_url":"https://arxiv.org/pdf/2403.18690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08479v2","updated":"2024-03-27T15:38:27Z","published":"2023-12-13T19:38:50Z","title":"Vision Transformer-Based Deep Learning for Histologic Classification of\n  Endometrial Cancer","summary":"  Endometrial cancer, the fourth most common cancer in females in the United\nStates, with the lifetime risk for developing this disease is approximately\n2.8% in women. Precise histologic evaluation and molecular classification of\nendometrial cancer is important for effective patient management and\ndetermining the best treatment modalities. This study introduces EndoNet, which\nuses convolutional neural networks for extracting histologic features and a\nvision transformer for aggregating these features and classifying slides based\non their visual characteristics into high- and low- grade. The model was\ntrained on 929 digitized hematoxylin and eosin-stained whole-slide images of\nendometrial cancer from hysterectomy cases at Dartmouth-Health. It classifies\nthese slides into low-grade (Endometroid Grades 1 and 2) and high-grade\n(endometroid carcinoma FIGO grade 3, uterine serous carcinoma, carcinosarcoma)\ncategories. EndoNet was evaluated on an internal test set of 110 patients and\nan external test set of 100 patients from the public TCGA database. The model\nachieved a weighted average F1-score of 0.91 (95% CI: 0.86-0.95) and an AUC of\n0.95 (95% CI: 0.89-0.99) on the internal test, and 0.86 (95% CI: 0.80-0.94) for\nF1-score and 0.86 (95% CI: 0.75-0.93) for AUC on the external test. Pending\nfurther validation, EndoNet has the potential to support pathologists without\nthe need of manual annotations in classifying the grades of gynecologic\npathology tumors.\n","authors":["Manu Goyal","Laura J. Tafe","James X. Feng","Kristen E. Muller","Liesbeth Hondelink","Jessica L. Bentz","Saeed Hassanpour"],"pdf_url":"https://arxiv.org/pdf/2312.08479v2.pdf","comment":"4 Tables and 3 Figures"},{"id":"http://arxiv.org/abs/2308.06098v2","updated":"2024-03-27T15:26:44Z","published":"2023-08-11T12:18:53Z","title":"Automated Construction of Time-Space Diagrams for Traffic Analysis Using\n  Street-View Video Sequence","summary":"  Time-space diagrams are essential tools for analyzing traffic patterns and\noptimizing transportation infrastructure and traffic management strategies.\nTraditional data collection methods for these diagrams have limitations in\nterms of temporal and spatial coverage. Recent advancements in camera\ntechnology have overcome these limitations and provided extensive urban data.\nIn this study, we propose an innovative approach to constructing time-space\ndiagrams by utilizing street-view video sequences captured by cameras mounted\non moving vehicles. Using the state-of-the-art YOLOv5, StrongSORT, and\nphotogrammetry techniques for distance calculation, we can infer vehicle\ntrajectories from the video data and generate time-space diagrams. To evaluate\nthe effectiveness of our proposed method, we utilized datasets from the KITTI\ncomputer vision benchmark suite. The evaluation results demonstrate that our\napproach can generate trajectories from video data, although there are some\nerrors that can be mitigated by improving the performance of the detector,\ntracker, and distance calculation components. In conclusion, the utilization of\nstreet-view video sequences captured by cameras mounted on moving vehicles,\ncombined with state-of-the-art computer vision techniques, has immense\npotential for constructing comprehensive time-space diagrams. These diagrams\noffer valuable insights into traffic patterns and contribute to the design of\ntransportation infrastructure and traffic management strategies.\n","authors":["Tanay Rastogi","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2308.06098v2.pdf","comment":"The paper is published in 2023 IEEE 26th International Conference on\n  Intelligent Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2403.18674v1","updated":"2024-03-27T15:17:10Z","published":"2024-03-27T15:17:10Z","title":"Deep Learning for Robust and Explainable Models in Computer Vision","summary":"  Recent breakthroughs in machine and deep learning (ML and DL) research have\nprovided excellent tools for leveraging enormous amounts of data and optimizing\nhuge models with millions of parameters to obtain accurate networks for image\nprocessing. These developments open up tremendous opportunities for using\nartificial intelligence (AI) in the automation and human assisted AI industry.\nHowever, as more and more models are deployed and used in practice, many\nchallenges have emerged. This thesis presents various approaches that address\nrobustness and explainability challenges for using ML and DL in practice.\n  Robustness and reliability are the critical components of any model before\ncertification and deployment in practice. Deep convolutional neural networks\n(CNNs) exhibit vulnerability to transformations of their inputs, such as\nrotation and scaling, or intentional manipulations as described in the\nadversarial attack literature. In addition, building trust in AI-based models\nrequires a better understanding of current models and developing methods that\nare more explainable and interpretable a priori.\n  This thesis presents developments in computer vision models' robustness and\nexplainability. Furthermore, this thesis offers an example of using vision\nmodels' feature response visualization (models' interpretations) to improve\nrobustness despite interpretability and robustness being seemingly unrelated in\nthe related research. Besides methodological developments for robust and\nexplainable vision models, a key message of this thesis is introducing model\ninterpretation techniques as a tool for understanding vision models and\nimproving their design and robustness. In addition to the theoretical\ndevelopments, this thesis demonstrates several applications of ML and DL in\ndifferent contexts, such as medical imaging and affective computing.\n","authors":["Mohammadreza Amirian"],"pdf_url":"https://arxiv.org/pdf/2403.18674v1.pdf","comment":"150 pages, 37 figures, 12 tables"},{"id":"http://arxiv.org/abs/2311.15803v3","updated":"2024-03-27T15:05:19Z","published":"2023-11-27T13:25:47Z","title":"SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using\n  Neural Radiance Fields","summary":"  In rapidly-evolving domains such as autonomous driving, the use of multiple\nsensors with different modalities is crucial to ensure high operational\nprecision and stability. To correctly exploit the provided information by each\nsensor in a single common frame, it is essential for these sensors to be\naccurately calibrated. In this paper, we leverage the ability of Neural\nRadiance Fields (NeRF) to represent different sensors modalities in a common\nvolumetric representation to achieve robust and accurate spatio-temporal sensor\ncalibration. By designing a partitioning approach based on the visible part of\nthe scene for each sensor, we formulate the calibration problem using only the\noverlapping areas. This strategy results in a more robust and accurate\ncalibration that is less prone to failure. We demonstrate that our approach\nworks on outdoor urban scenes by validating it on multiple established driving\ndatasets. Results show that our method is able to get better accuracy and\nrobustness compared to existing methods.\n","authors":["Quentin Herau","Nathan Piasco","Moussab Bennehar","Luis Roldão","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2311.15803v3.pdf","comment":"Accepted at CVPR 2024. Project page: https://qherau.github.io/SOAC/"},{"id":"http://arxiv.org/abs/2403.18660v1","updated":"2024-03-27T15:03:38Z","published":"2024-03-27T15:03:38Z","title":"InstructBrush: Learning Attention-based Instruction Optimization for\n  Image Editing","summary":"  In recent years, instruction-based image editing methods have garnered\nsignificant attention in image editing. However, despite encompassing a wide\nrange of editing priors, these methods are helpless when handling editing tasks\nthat are challenging to accurately describe through language. We propose\nInstructBrush, an inversion method for instruction-based image editing methods\nto bridge this gap. It extracts editing effects from exemplar image pairs as\nediting instructions, which are further applied for image editing. Two key\ntechniques are introduced into InstructBrush, Attention-based Instruction\nOptimization and Transformation-oriented Instruction Initialization, to address\nthe limitations of the previous method in terms of inversion effects and\ninstruction generalization. To explore the ability of instruction inversion\nmethods to guide image editing in open scenarios, we establish a\nTransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set\nof scenes and editing types. The creation of this benchmark paves the way for\nfurther exploration of instruction inversion. Quantitatively and qualitatively,\nour approach achieves superior performance in editing and is more semantically\nconsistent with the target editing effects.\n","authors":["Ruoyu Zhao","Qingnan Fan","Fei Kou","Shuai Qin","Hong Gu","Wei Wu","Pengcheng Xu","Mingrui Zhu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18660v1.pdf","comment":"Project Page: https://royzhao926.github.io/InstructBrush/"},{"id":"http://arxiv.org/abs/2311.12386v3","updated":"2024-03-27T15:01:44Z","published":"2023-11-21T06:55:21Z","title":"Point, Segment and Count: A Generalized Framework for Object Counting","summary":"  Class-agnostic object counting aims to count all objects in an image with\nrespect to example boxes or class names, \\emph{a.k.a} few-shot and zero-shot\ncounting. In this paper, we propose a generalized framework for both few-shot\nand zero-shot object counting based on detection. Our framework combines the\nsuperior advantages of two foundation models without compromising their\nzero-shot capability: (\\textbf{i}) SAM to segment all possible objects as mask\nproposals, and (\\textbf{ii}) CLIP to classify proposals to obtain accurate\nobject counts. However, this strategy meets the obstacles of efficiency\noverhead and the small crowded objects that cannot be localized and\ndistinguished. To address these issues, our framework, termed PseCo, follows\nthree steps: point, segment, and count. Specifically, we first propose a\nclass-agnostic object localization to provide accurate but least point prompts\nfor SAM, which consequently not only reduces computation costs but also avoids\nmissing small objects. Furthermore, we propose a generalized object\nclassification that leverages CLIP image/text embeddings as the classifier,\nfollowing a hierarchical knowledge distillation to obtain discriminative\nclassifications among hierarchical mask proposals. Extensive experimental\nresults on FSC-147, COCO, and LVIS demonstrate that PseCo achieves\nstate-of-the-art performance in both few-shot/zero-shot object\ncounting/detection. Code: https://github.com/Hzzone/PseCo\n","authors":["Zhizhong Huang","Mingliang Dai","Yi Zhang","Junping Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2311.12386v3.pdf","comment":"Accepted by CVPR 2024. Camera ready"},{"id":"http://arxiv.org/abs/2311.17532v3","updated":"2024-03-27T15:01:22Z","published":"2023-11-29T11:10:40Z","title":"Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech\n  Gesture Generation","summary":"  Generating vivid and emotional 3D co-speech gestures is crucial for virtual\navatar animation in human-machine interaction applications. While the existing\nmethods enable generating the gestures to follow a single emotion label, they\noverlook that long gesture sequence modeling with emotion transition is more\npractical in real scenes. In addition, the lack of large-scale available\ndatasets with emotional transition speech and corresponding 3D human gestures\nalso limits the addressing of this task. To fulfill this goal, we first\nincorporate the ChatGPT-4 and an audio inpainting approach to construct the\nhigh-fidelity emotion transition human speeches. Considering obtaining the\nrealistic 3D pose annotations corresponding to the dynamically inpainted\nemotion transition audio is extremely difficult, we propose a novel weakly\nsupervised training strategy to encourage authority gesture transitions.\nSpecifically, to enhance the coordination of transition gestures w.r.t\ndifferent emotional ones, we model the temporal association representation\nbetween two different emotional gesture sequences as style guidance and infuse\nit into the transition generation. We further devise an emotion mixture\nmechanism that provides weak supervision based on a learnable mixed emotion\nlabel for transition gestures. Last, we present a keyframe sampler to supply\neffective initial posture cues in long sequences, enabling us to generate\ndiverse gestures. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art models constructed by adapting single emotion-conditioned\ncounterparts on our newly defined emotion transition task and datasets. Our\ncode and dataset will be released on the project page:\nhttps://xingqunqi-lab.github.io/Emo-Transition-Gesture/.\n","authors":["Xingqun Qi","Jiahao Pan","Peng Li","Ruibin Yuan","Xiaowei Chi","Mengfei Li","Wenhan Luo","Wei Xue","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2311.17532v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18649v1","updated":"2024-03-27T14:56:44Z","published":"2024-03-27T14:56:44Z","title":"Addressing Data Annotation Challenges in Multiple Sensors: A Solution\n  for Scania Collected Datasets","summary":"  Data annotation in autonomous vehicles is a critical step in the development\nof Deep Neural Network (DNN) based models or the performance evaluation of the\nperception system. This often takes the form of adding 3D bounding boxes on\ntime-sequential and registered series of point-sets captured from active\nsensors like Light Detection and Ranging (LiDAR) and Radio Detection and\nRanging (RADAR). When annotating multiple active sensors, there is a need to\nmotion compensate and translate the points to a consistent coordinate frame and\ntimestamp respectively. However, highly dynamic objects pose a unique\nchallenge, as they can appear at different timestamps in each sensor's data.\nWithout knowing the speed of the objects, their position appears to be\ndifferent in different sensor outputs. Thus, even after motion compensation,\nhighly dynamic objects are not matched from multiple sensors in the same frame,\nand human annotators struggle to add unique bounding boxes that capture all\nobjects. This article focuses on addressing this challenge, primarily within\nthe context of Scania collected datasets. The proposed solution takes a track\nof an annotated object as input and uses the Moving Horizon Estimation (MHE) to\nrobustly estimate its speed. The estimated speed profile is utilized to correct\nthe position of the annotated box and add boxes to object clusters missed by\nthe original annotation.\n","authors":["Ajinkya Khoche","Aron Asefaw","Alejandro Gonzalez","Bogdan Timus","Sina Sharif Mansouri","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.18649v1.pdf","comment":"Accepted to European Control Conference 2024"},{"id":"http://arxiv.org/abs/2403.18637v1","updated":"2024-03-27T14:42:08Z","published":"2024-03-27T14:42:08Z","title":"Transformers-based architectures for stroke segmentation: A review","summary":"  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n","authors":["Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.18637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.11041v3","updated":"2024-03-27T14:29:27Z","published":"2022-04-23T10:19:58Z","title":"Learning by Erasing: Conditional Entropy based Transferable\n  Out-Of-Distribution Detection","summary":"  Out-of-distribution (OOD) detection is essential to handle the distribution\nshifts between training and test scenarios. For a new in-distribution (ID)\ndataset, existing methods require retraining to capture the dataset-specific\nfeature representation or data distribution. In this paper, we propose a deep\ngenerative models (DGM) based transferable OOD detection method, which is\nunnecessary to retrain on a new ID dataset. We design an image erasing strategy\nto equip exclusive conditional entropy distribution for each ID dataset, which\ndetermines the discrepancy of DGM's posteriori ucertainty distribution on\ndifferent ID datasets. Owing to the powerful representation capacity of\nconvolutional neural networks, the proposed model trained on complex dataset\ncan capture the above discrepancy between ID datasets without retraining and\nthus achieve transferable OOD detection. We validate the proposed method on\nfive datasets and verity that ours achieves comparable performance to the\nstate-of-the-art group based OOD detection methods that need to be retrained to\ndeploy on new ID datasets. Our code is available at\nhttps://github.com/oOHCIOo/CETOOD.\n","authors":["Meng Xing","Zhiyong Feng","Yong Su","Changjae Oh"],"pdf_url":"https://arxiv.org/pdf/2204.11041v3.pdf","comment":"update new experimental results"},{"id":"http://arxiv.org/abs/2403.18605v1","updated":"2024-03-27T14:24:30Z","published":"2024-03-27T14:24:30Z","title":"FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image\n  Editing","summary":"  Our work addresses limitations seen in previous approaches for object-centric\nediting problems, such as unrealistic results due to shape discrepancies and\nlimited control in object replacement or insertion. To this end, we introduce\nFlexEdit, a flexible and controllable editing framework for objects where we\niteratively adjust latents at each denoising step using our FlexEdit block.\nInitially, we optimize latents at test time to align with specified object\nconstraints. Then, our framework employs an adaptive mask, automatically\nextracted during denoising, to protect the background while seamlessly blending\nnew content into the target image. We demonstrate the versatility of FlexEdit\nin various object editing tasks and curate an evaluation test suite with\nsamples from both real and synthetic images, along with novel evaluation\nmetrics designed for object-centric editing. We conduct extensive experiments\non different editing scenarios, demonstrating the superiority of our editing\nframework over recent advanced text-guided image editing methods. Our project\npage is published at https://flex-edit.github.io/.\n","authors":["Trong-Tung Nguyen","Duc-Anh Nguyen","Anh Tran","Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2403.18605v1.pdf","comment":"Our project page: https://flex-edit.github.io/"},{"id":"http://arxiv.org/abs/2403.18600v1","updated":"2024-03-27T14:22:40Z","published":"2024-03-27T14:22:40Z","title":"RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in\n  Instructional Videos","summary":"  Procedure Planning in instructional videos entails generating a sequence of\naction steps based on visual observations of the initial and target states.\nDespite the rapid progress in this task, there remain several critical\nchallenges to be solved: (1) Adaptive procedures: Prior works hold an\nunrealistic assumption that the number of action steps is known and fixed,\nleading to non-generalizable models in real-world scenarios where the sequence\nlength varies. (2) Temporal relation: Understanding the step temporal relation\nknowledge is essential in producing reasonable and executable plans. (3)\nAnnotation cost: Annotating instructional videos with step-level labels (i.e.,\ntimestamp) or sequence-level labels (i.e., action category) is demanding and\nlabor-intensive, limiting its generalizability to large-scale datasets.In this\nwork, we propose a new and practical setting, called adaptive procedure\nplanning in instructional videos, where the procedure length is not fixed or\npre-determined. To address these challenges we introduce Retrieval-Augmented\nPlanner (RAP) model. Specifically, for adaptive procedures, RAP adaptively\ndetermines the conclusion of actions using an auto-regressive model\narchitecture. For temporal relation, RAP establishes an external memory module\nto explicitly retrieve the most relevant state-action pairs from the training\nvideos and revises the generated procedures. To tackle high annotation cost,\nRAP utilizes a weakly-supervised learning manner to expand the training dataset\nto other task-relevant, unannotated videos by generating pseudo labels for\naction steps. Experiments on CrossTask and COIN benchmarks show the superiority\nof RAP over traditional fixed-length models, establishing it as a strong\nbaseline solution for adaptive procedure planning.\n","authors":["Ali Zare","Yulei Niu","Hammad Ayyubi","Shih-fu Chang"],"pdf_url":"https://arxiv.org/pdf/2403.18600v1.pdf","comment":"23 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2403.18593v1","updated":"2024-03-27T14:18:09Z","published":"2024-03-27T14:18:09Z","title":"Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote\n  Sensing Image Understanding","summary":"  The tokenizer, as one of the fundamental components of large models, has long\nbeen overlooked or even misunderstood in visual tasks. One key factor of the\ngreat comprehension power of the large language model is that natural language\ntokenizers utilize meaningful words or subwords as the basic elements of\nlanguage. In contrast, mainstream visual tokenizers, represented by patch-based\nmethods such as Patch Embed, rely on meaningless rectangular patches as basic\nelements of vision, which cannot serve as effectively as words or subwords in\nlanguage. Starting from the essence of the tokenizer, we defined semantically\nindependent regions (SIRs) for vision. We designed a simple HOmogeneous visual\ntOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception\nModule (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,\nthe OPM splits the image into 4*4 pixel seeds and then utilizes the attention\nmechanism to perceive SIRs. The OVM employs cross-attention to merge seeds\nwithin the same SIR. To achieve adaptability, the OVM defines a variable number\nof learnable vectors as cross-attention queries, allowing for the adjustment of\ntoken quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19\nclassification dataset, and GID5 segmentation dataset for sparse and dense\ntasks. The results demonstrate that the visual tokens obtained by HOOK\ncorrespond to individual objects, which demonstrates homogeneity. HOOK\noutperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved\nstate-of-the-art performance compared to the baselines used for comparison.\nCompared to Patch Embed, which requires more than one hundred tokens for one\nimage, HOOK requires only 6 and 8 tokens for sparse and dense tasks,\nrespectively, resulting in efficiency improvements of 1.5 to 2.8 times. The\ncode is available at https://github.com/GeoX-Lab/Hook.\n","authors":["Run Shao","Zhaoyang Zhang","Chao Tao","Yunsheng Zhang","Chengli Peng","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18593v1.pdf","comment":"20 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.18589v1","updated":"2024-03-27T14:12:56Z","published":"2024-03-27T14:12:56Z","title":"Users prefer Jpegli over same-sized libjpeg-turbo or MozJPEG","summary":"  We performed pairwise comparisons by human raters of JPEG images from\nMozJPEG, libjpeg-turbo and our new Jpegli encoder. When compressing images at a\nquality similar to libjpeg-turbo quality 95, the Jpegli images were 54% likely\nto be preferred over both libjpeg-turbo and MozJPEG images, but used only 2.8\nbits per pixel compared to libjpeg-turbo and MozJPEG that used 3.8 and 3.5 bits\nper pixel respectively. The raw ratings and source images are publicly\navailable for further analysis and study.\n","authors":["Martin Bruse","Luca Versari","Zoltan Szabadka","Jyrki Alakuijala"],"pdf_url":"https://arxiv.org/pdf/2403.18589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18587v1","updated":"2024-03-27T14:11:23Z","published":"2024-03-27T14:11:23Z","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency\n  Attacks in Computer Vision","summary":"  Resource efficiency plays an important role for machine learning nowadays.\nThe energy and decision latency are two critical aspects to ensure a\nsustainable and practical application. Unfortunately, the energy consumption\nand decision latency are not robust against adversaries. Researchers have\nrecently demonstrated that attackers can compute and submit so-called sponge\nexamples at inference time to increase the energy consumption and decision\nlatency of neural networks. In computer vision, the proposed strategy crafts\ninputs with less activation sparsity which could otherwise be used to\naccelerate the computation. In this paper, we analyze the mechanism how these\nenergy-latency attacks reduce activation sparsity. In particular, we find that\ninput uniformity is a key enabler. A uniform image, that is, an image with\nmostly flat, uniformly colored surfaces, triggers more activations due to a\nspecific interplay of convolution, batch normalization, and ReLU activation.\nBased on these insights, we propose two new simple, yet effective strategies\nfor crafting sponge examples: sampling images from a probability distribution\nand identifying dense, yet inconspicuous inputs in natural datasets. We\nempirically examine our findings in a comprehensive evaluation with multiple\nimage classification models and show that our attack achieves the same sparsity\neffect as prior sponge-example methods, but at a fraction of computation\neffort. We also show that our sponge examples transfer between different neural\nnetworks. Finally, we discuss applications of our findings for the good by\nimproving efficiency by increasing sparsity.\n","authors":["Andreas Müller","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2403.18587v1.pdf","comment":"Accepted at the DLSP 2024"},{"id":"http://arxiv.org/abs/2312.07264v2","updated":"2024-03-27T14:09:10Z","published":"2023-12-12T13:44:53Z","title":"Dual Structure-Aware Image Filterings for Semi-supervised Medical Image\n  Segmentation","summary":"  Semi-supervised image segmentation has attracted great attention recently.\nThe key is how to leverage unlabeled images in the training process. Most\nmethods maintain consistent predictions of the unlabeled images under\nvariations (e.g., adding noise/perturbations, or creating alternative versions)\nin the image and/or model level. In most image-level variation, medical images\noften have prior structure information, which has not been well explored. In\nthis paper, we propose novel dual structure-aware image filterings (DSAIF) as\nthe image-level variations for semi-supervised medical image segmentation.\nMotivated by connected filtering that simplifies image via filtering in\nstructure-aware tree-based image representation, we resort to the dual contrast\ninvariant Max-tree and Min-tree representation. Specifically, we propose a\nnovel connected filtering that removes topologically equivalent nodes (i.e.\nconnected components) having no siblings in the Max/Min-tree. This results in\ntwo filtered images preserving topologically critical structure. Applying the\nproposed DSAIF to mutually supervised networks decreases the consensus of their\nerroneous predictions on unlabeled images. This helps to alleviate the\nconfirmation bias issue of overfitting to noisy pseudo labels of unlabeled\nimages, and thus effectively improves the segmentation performance. Extensive\nexperimental results on three benchmark datasets demonstrate that the proposed\nmethod significantly/consistently outperforms some state-of-the-art methods.\nThe source codes will be publicly available.\n","authors":["Yuliang Gu","Zhichao Sun","Tian Chen","Xin Xiao","Yepeng Liu","Yongchao Xu","Laurent Najman"],"pdf_url":"https://arxiv.org/pdf/2312.07264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18575v1","updated":"2024-03-27T13:56:08Z","published":"2024-03-27T13:56:08Z","title":"HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional\n  Synthesis and Sampling of Hand-Object Interactions","summary":"  Reconstructing 3D hand mesh robustly from a single image is very challenging,\ndue to the lack of diversity in existing real-world datasets. While data\nsynthesis helps relieve the issue, the syn-to-real gap still hinders its usage.\nIn this work, we present HandBooster, a new approach to uplift the data\ndiversity and boost the 3D hand-mesh reconstruction performance by training a\nconditional generative space on hand-object interactions and purposely sampling\nthe space to synthesize effective data samples. First, we construct versatile\ncontent-aware conditions to guide a diffusion model to produce realistic images\nwith diverse hand appearances, poses, views, and backgrounds; favorably,\naccurate 3D annotations are obtained for free. Then, we design a novel\ncondition creator based on our similarity-aware distribution sampling\nstrategies to deliberately find novel and realistic interaction poses that are\ndistinctive from the training set. Equipped with our method, several baselines\ncan be significantly improved beyond the SOTA on the HO3D and DexYCB\nbenchmarks. Our code will be released on\nhttps://github.com/hxwork/HandBooster_Pytorch.\n","authors":["Hao Xu","Haipeng Li","Yinqiao Wang","Shuaicheng Liu","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2403.18575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07636v3","updated":"2024-03-27T13:51:59Z","published":"2024-03-12T13:18:22Z","title":"Decomposing Disease Descriptions for Enhanced Pathology Detection: A\n  Multi-Aspect Vision-Language Pre-training Framework","summary":"  Medical vision language pre-training (VLP) has emerged as a frontier of\nresearch, enabling zero-shot pathological recognition by comparing the query\nimage with the textual descriptions for each disease. Due to the complex\nsemantics of biomedical texts, current methods struggle to align medical images\nwith key pathological findings in unstructured reports. This leads to the\nmisalignment with the target disease's textual representation. In this paper,\nwe introduce a novel VLP framework designed to dissect disease descriptions\ninto their fundamental aspects, leveraging prior knowledge about the visual\nmanifestations of pathologies. This is achieved by consulting a large language\nmodel and medical experts. Integrating a Transformer module, our approach\naligns an input image with the diverse elements of a disease, generating\naspect-centric image representations. By consolidating the matches from each\naspect, we improve the compatibility between an image and its associated\ndisease. Additionally, capitalizing on the aspect-oriented representations, we\npresent a dual-head Transformer tailored to process known and unknown diseases,\noptimizing the comprehensive detection efficacy. Conducting experiments on\nseven downstream datasets, ours improves the accuracy of recent methods by up\nto 8.56% and 17.0% for seen and unseen categories, respectively. Our code is\nreleased at https://github.com/HieuPhan33/MAVL.\n","authors":["Vu Minh Hieu Phan","Yutong Xie","Yuankai Qi","Lingqiao Liu","Liyang Liu","Bowen Zhang","Zhibin Liao","Qi Wu","Minh-Son To","Johan W. Verjans"],"pdf_url":"https://arxiv.org/pdf/2403.07636v3.pdf","comment":"Accepted at CVPR2024. Pre-print before final camera-ready version"},{"id":"http://arxiv.org/abs/2403.18565v1","updated":"2024-03-27T13:46:01Z","published":"2024-03-27T13:46:01Z","title":"Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images\n  with Deep Learning -- A Review","summary":"  Deep learning based approaches have been used to improve image quality in\ncone-beam computed tomography (CBCT), a medical imaging technique often used in\napplications such as image-guided radiation therapy, implant dentistry or\northopaedics. In particular, while deep learning methods have been applied to\nreduce various types of CBCT image artifacts arising from motion, metal\nobjects, or low-dose acquisition, a comprehensive review summarizing the\nsuccesses and shortcomings of these approaches, with a primary focus on the\ntype of artifacts rather than the architecture of neural networks, is lacking\nin the literature. In this review, the data generation and simulation\npipelines, and artifact reduction techniques are specifically investigated for\neach type of artifact. We provide an overview of deep learning techniques that\nhave successfully been shown to reduce artifacts in 3D, as well as in\ntime-resolved (4D) CBCT through the use of projection- and/or volume-domain\noptimizations, or by introducing neural networks directly within the CBCT\nreconstruction algorithms. Research gaps are identified to suggest avenues for\nfuture exploration. One of the key findings of this work is an observed trend\ntowards the use of generative models including GANs and score-based or\ndiffusion models, accompanied with the need for more diverse and open training\ndatasets and simulations.\n","authors":["Mohammadreza Amirian","Daniel Barco","Ivo Herzig","Frank-Peter Schilling"],"pdf_url":"https://arxiv.org/pdf/2403.18565v1.pdf","comment":"16 pages, 4 figures, 1 Table, published in IEEE Access Journal"},{"id":"http://arxiv.org/abs/2403.09700v2","updated":"2024-03-27T13:42:25Z","published":"2024-03-05T22:19:21Z","title":"Shapley Values-Powered Framework for Fair Reward Split in Content\n  Produced by GenAI","summary":"  It is evident that, currently, generative models are surpassed in quality by\nhuman professionals. However, with the advancements in Artificial Intelligence,\nthis gap will narrow, leading to scenarios where individuals who have dedicated\nyears of their lives to mastering a skill become obsolete due to their high\ncosts, which are inherently linked to the time they require to complete a task\n-- a task that AI could accomplish in minutes or seconds. To avoid future\nsocial upheavals, we must, even now, contemplate how to fairly assess the\ncontributions of such individuals in training generative models and how to\ncompensate them for the reduction or complete loss of their incomes. In this\nwork, we propose a method to structure collaboration between model developers\nand data providers. To achieve this, we employ Shapley Values to quantify the\ncontribution of artist(s) in an image generated by the Stable Diffusion-v1.5\nmodel and to equitably allocate the reward among them.\n","authors":["Alex Glinsky","Alexey Sokolsky"],"pdf_url":"https://arxiv.org/pdf/2403.09700v2.pdf","comment":"36 pages, 32 figures"},{"id":"http://arxiv.org/abs/2403.18554v1","updated":"2024-03-27T13:33:14Z","published":"2024-03-27T13:33:14Z","title":"CosalPure: Learning Concept from Group Images for Robust Co-Saliency\n  Detection","summary":"  Co-salient object detection (CoSOD) aims to identify the common and salient\n(usually in the foreground) regions across a given group of images. Although\nachieving significant progress, state-of-the-art CoSODs could be easily\naffected by some adversarial perturbations, leading to substantial accuracy\nreduction. The adversarial perturbations can mislead CoSODs but do not change\nthe high-level semantic information (e.g., concept) of the co-salient objects.\nIn this paper, we propose a novel robustness enhancement framework by first\nlearning the concept of the co-salient objects based on the input group images\nand then leveraging this concept to purify adversarial perturbations, which are\nsubsequently fed to CoSODs for robustness enhancement. Specifically, we propose\nCosalPure containing two modules, i.e., group-image concept learning and\nconcept-guided diffusion purification. For the first module, we adopt a\npre-trained text-to-image diffusion model to learn the concept of co-salient\nobjects within group images where the learned concept is robust to adversarial\nexamples. For the second module, we map the adversarial image to the latent\nspace and then perform diffusion generation by embedding the learned concept\ninto the noise prediction function as an extra condition. Our method can\neffectively alleviate the influence of the SOTA adversarial attack containing\ndifferent adversarial patterns, including exposure and noise. The extensive\nresults demonstrate that our method could enhance the robustness of CoSODs\nsignificantly.\n","authors":["Jiayi Zhu","Qing Guo","Felix Juefei-Xu","Yihao Huang","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2403.18554v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.18551v1","updated":"2024-03-27T13:31:39Z","published":"2024-03-27T13:31:39Z","title":"Attention Calibration for Disentangled Text-to-Image Personalization","summary":"  Recent thrilling progress in large-scale text-to-image (T2I) models has\nunlocked unprecedented synthesis quality of AI-generated content (AIGC)\nincluding image generation, 3D and video composition. Further, personalized\ntechniques enable appealing customized production of a novel concept given only\nseveral images as reference. However, an intriguing problem persists: Is it\npossible to capture multiple, novel concepts from one single reference image?\nIn this paper, we identify that existing approaches fail to preserve visual\nconsistency with the reference image and eliminate cross-influence from\nconcepts. To alleviate this, we propose an attention calibration mechanism to\nimprove the concept-level understanding of the T2I model. Specifically, we\nfirst introduce new learnable modifiers bound with classes to capture\nattributes of multiple concepts. Then, the classes are separated and\nstrengthened following the activation of the cross-attention operation,\nensuring comprehensive and self-contained concepts. Additionally, we suppress\nthe attention activation of different classes to mitigate mutual influence\namong concepts. Together, our proposed method, dubbed DisenDiff, can learn\ndisentangled multiple concepts from one single image and produce novel\ncustomized images with learned concepts. We demonstrate that our method\noutperforms the current state of the art in both qualitative and quantitative\nevaluations. More importantly, our proposed techniques are compatible with LoRA\nand inpainting pipelines, enabling more interactive experiences.\n","authors":["Yanbing Zhang","Mengping Yang","Qin Zhou","Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18551v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18550v1","updated":"2024-03-27T13:30:48Z","published":"2024-03-27T13:30:48Z","title":"OrCo: Towards Better Generalization via Orthogonality and Contrast for\n  Few-Shot Class-Incremental Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which\nthe problem space expands with limited data. FSCIL methods inherently face the\nchallenge of catastrophic forgetting as data arrives incrementally, making\nmodels susceptible to overwriting previously acquired knowledge. Moreover,\ngiven the scarcity of labeled samples available at any given time, models may\nbe prone to overfitting and find it challenging to strike a balance between\nextensive pretraining and the limited incremental data. To address these\nchallenges, we propose the OrCo framework built on two core principles:\nfeatures' orthogonality in the representation space, and contrastive learning.\nIn particular, we improve the generalization of the embedding space by\nemploying a combination of supervised and self-supervised contrastive losses\nduring the pretraining phase. Additionally, we introduce OrCo loss to address\nchallenges arising from data limitations during incremental sessions. Through\nfeature space perturbations and orthogonality between classes, the OrCo loss\nmaximizes margins and reserves space for the following incremental data. This,\nin turn, ensures the accommodation of incoming classes in the feature space\nwithout compromising previously acquired knowledge. Our experimental results\nshowcase state-of-the-art performance across three benchmark datasets,\nincluding mini-ImageNet, CIFAR100, and CUB datasets. Code is available at\nhttps://github.com/noorahmedds/OrCo\n","authors":["Noor Ahmed","Anna Kukleva","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2403.18550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18548v1","updated":"2024-03-27T13:27:02Z","published":"2024-03-27T13:27:02Z","title":"A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency\n  Aware and Realistic Brightness Constraint","summary":"  Existing research based on deep learning has extensively explored the problem\nof daytime image dehazing. However, few studies have considered the\ncharacteristics of nighttime hazy scenes. There are two distinctions between\nnighttime and daytime haze. First, there may be multiple active colored light\nsources with lower illumination intensity in nighttime scenes, which may cause\nhaze, glow and noise with localized, coupled and frequency inconsistent\ncharacteristics. Second, due to the domain discrepancy between simulated and\nreal-world data, unrealistic brightness may occur when applying a dehazing\nmodel trained on simulated data to real-world data. To address the above two\nissues, we propose a semi-supervised model for real-world nighttime dehazing.\nFirst, the spatial attention and frequency spectrum filtering are implemented\nas a spatial-frequency domain information interaction module to handle the\nfirst issue. Second, a pseudo-label-based retraining strategy and a local\nwindow-based brightness loss for semi-supervised training process is designed\nto suppress haze and glow while achieving realistic brightness. Experiments on\npublic benchmarks validate the effectiveness of the proposed method and its\nsuperiority over state-of-the-art methods. The source code and Supplementary\nMaterials are placed in the https://github.com/Xiaofeng-life/SFSNiD.\n","authors":["Xiaofeng Cong","Jie Gui","Jing Zhang","Junming Hou","Hao Shen"],"pdf_url":"https://arxiv.org/pdf/2403.18548v1.pdf","comment":"This paper is accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.18546v1","updated":"2024-03-27T13:24:58Z","published":"2024-03-27T13:24:58Z","title":"Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes","summary":"  Fast and robust object grasping in clutter is a crucial component of\nrobotics. Most current works resort to the whole observed point cloud for 6-Dof\ngrasp generation, ignoring the guidance information excavated from global\nsemantics, thus limiting high-quality grasp generation and real-time\nperformance. In this work, we show that the widely used heatmaps are\nunderestimated in the efficiency of 6-Dof grasp generation. Therefore, we\npropose an effective local grasp generator combined with grasp heatmaps as\nguidance, which infers in a global-to-local semantic-to-point way.\nSpecifically, Gaussian encoding and the grid-based strategy are applied to\npredict grasp heatmaps as guidance to aggregate local points into graspable\nregions and provide global semantic information. Further, a novel non-uniform\nanchor sampling mechanism is designed to improve grasp accuracy and diversity.\nBenefiting from the high-efficiency encoding in the image space and focusing on\npoints in local graspable regions, our framework can perform high-quality grasp\ndetection in real-time and achieve state-of-the-art results. In addition, real\nrobot experiments demonstrate the effectiveness of our method with a success\nrate of 94% and a clutter completion rate of 100%. Our code is available at\nhttps://github.com/THU-VCLab/HGGD.\n","authors":["Siang Chen","Wei Tang","Pengwei Xie","Wenming Yang","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18546v1.pdf","comment":"Extensive results on GraspNet-1B dataset"},{"id":"http://arxiv.org/abs/2310.15081v3","updated":"2024-03-27T13:23:28Z","published":"2023-10-23T16:41:13Z","title":"E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion","summary":"  This paper proposes a novel approach to face swapping from the perspective of\nfine-grained facial editing, dubbed \"editing for swapping\" (E4S). The\ntraditional face swapping methods rely on global feature extraction and fail to\npreserve the detailed source identity. In contrast, we propose a Regional GAN\nInversion (RGI) method, which allows the explicit disentanglement of shape and\ntexture. Specifically, our E4S performs face swapping in the latent space of a\npretrained StyleGAN, where a multi-scale mask-guided encoder is applied to\nproject the texture of each facial component into regional style codes and a\nmask-guided injection module manipulating feature maps with the style codes.\nBased on this disentanglement, face swapping can be simplified as style and\nmask swapping. Besides, due to the large lighting condition gap, transferring\nthe source skin into the target image may lead to disharmony lighting. We\npropose a re-coloring network to make the swapped face maintain the target\nlighting condition while preserving the source skin. Further, to deal with the\npotential mismatch areas during mask exchange, we design a face inpainting\nmodule to refine the face shape. The extensive comparisons with\nstate-of-the-art methods demonstrate that our E4S outperforms existing methods\nin preserving texture, shape, and lighting. Our implementation is available at\nhttps://github.com/e4s2024/E4S2024.\n","authors":["Maomao Li","Ge Yuan","Cairong Wang","Zhian Liu","Yong Zhang","Yongwei Nie","Jue Wang","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15081v3.pdf","comment":"Project Page: https://e4s2024.github.io/ ;. arXiv admin note: text\n  overlap with arXiv:2211.14068"},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2403.18514v1","updated":"2024-03-27T12:44:57Z","published":"2024-03-27T12:44:57Z","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection\n  of Pathological Pulmonary CT scans","summary":"  Unsupervised pathology detection can be implemented by training a model on\nhealthy data only and measuring the deviation from the training set upon\ninference, for example with CNN-based feature extraction and one-class\nclassifiers, or reconstruction-score-based methods such as AEs, GANs and\nDiffusion models. Normalizing Flows (NF) have the ability to directly learn the\nprobability distribution of training examples through an invertible\narchitecture. We leverage this property in a novel 3D NF-based model named\nCT-3DFlow, specifically tailored for patient-level pulmonary pathology\ndetection in chest CT data. Our model is trained unsupervised on healthy 3D\npulmonary CT patches, and detects deviations from its log-likelihood\ndistribution as anomalies. We aggregate patches-level likelihood values from a\npatient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.\nOut-of-distribution detection performance is evaluated using expert annotations\non a separate chest CT test dataset, outperforming other state-of-the-art\nmethods.\n","authors":["Aissam Djahnine","Alexandre Popoff","Emilien Jupin-Delevaux","Vincent Cottin","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2403.18514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04344v3","updated":"2024-03-27T12:44:55Z","published":"2023-06-07T11:18:53Z","title":"ViDA: Homeostatic Visual Domain Adapter for Continual Test Time\n  Adaptation","summary":"  Since real-world machine systems are running in non-stationary environments,\nContinual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained\nmodel to continually changing target domains. Recently, existing methods mainly\nfocus on model-based adaptation, which aims to leverage a self-training manner\nto extract the target domain knowledge. However, pseudo labels can be noisy and\nthe updated model parameters are unreliable under dynamic data distributions,\nleading to error accumulation and catastrophic forgetting in the continual\nadaptation process. To tackle these challenges and maintain the model\nplasticity, we design a Visual Domain Adapter (ViDA) for CTTA, explicitly\nhandling both domain-specific and domain-shared knowledge. Specifically, we\nfirst comprehensively explore the different domain representations of the\nadapters with trainable high-rank or low-rank embedding spaces. Then we inject\nViDAs into the pre-trained model, which leverages high-rank and low-rank\nfeatures to adapt the current domain distribution and maintain the continual\ndomain-shared knowledge, respectively. To exploit the low-rank and high-rank\nViDAs more effectively, we further propose a Homeostatic Knowledge Allotment\n(HKA) strategy, which adaptively combines different knowledge from each ViDA.\nExtensive experiments conducted on four widely used benchmarks demonstrate that\nour proposed method achieves state-of-the-art performance in both\nclassification and segmentation CTTA tasks. Note that, our method can be\nregarded as a novel transfer paradigm for large-scale models, delivering\npromising results in adaptation to continually changing distributions. Project\npage: https://sites.google.com/view/iclr2024-vida/home.\n","authors":["Jiaming Liu","Senqiao Yang","Peidong Jia","Renrui Zhang","Ming Lu","Yandong Guo","Wei Xue","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.04344v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2403.18512v1","updated":"2024-03-27T12:41:30Z","published":"2024-03-27T12:41:30Z","title":"ParCo: Part-Coordinating Text-to-Motion Synthesis","summary":"  We study a challenging task: text-to-motion synthesis, aiming to generate\nmotions that align with textual descriptions and exhibit coordinated movements.\nCurrently, the part-based methods introduce part partition into the motion\nsynthesis process to achieve finer-grained generation. However, these methods\nencounter challenges such as the lack of coordination between different part\nmotions and difficulties for networks to understand part concepts. Moreover,\nintroducing finer-grained part concepts poses computational complexity\nchallenges. In this paper, we propose Part-Coordinating Text-to-Motion\nSynthesis (ParCo), endowed with enhanced capabilities for understanding part\nmotions and communication among different part motion generators, ensuring a\ncoordinated and fined-grained motion synthesis. Specifically, we discretize\nwhole-body motion into multiple part motions to establish the prior concept of\ndifferent parts. Afterward, we employ multiple lightweight generators designed\nto synthesize different part motions and coordinate them through our part\ncoordination module. Our approach demonstrates superior performance on common\nbenchmarks with economic computations, including HumanML3D and KIT-ML,\nproviding substantial evidence of its effectiveness. Code is available at\nhttps://github.com/qrzou/ParCo .\n","authors":["Qiran Zou","Shangyuan Yuan","Shian Du","Yu Wang","Chang Liu","Yi Xu","Jie Chen","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16516v2","updated":"2024-03-27T12:32:31Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v2.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2312.06358v2","updated":"2024-03-27T12:24:29Z","published":"2023-12-11T13:05:54Z","title":"Intraoperative 2D/3D Image Registration via Differentiable X-ray\n  Rendering","summary":"  Surgical decisions are informed by aligning rapid portable 2D intraoperative\nimages (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g.,\nCT). 2D/3D image registration often fails in practice: conventional\noptimization methods are prohibitively slow and susceptible to local minima,\nwhile neural networks trained on small datasets fail on new patients or require\nimpractical landmark supervision. We present DiffPose, a self-supervised\napproach that leverages patient-specific simulation and differentiable\nphysics-based rendering to achieve accurate 2D/3D registration without relying\non manually labeled data. Preoperatively, a CNN is trained to regress the pose\nof a randomly oriented synthetic X-ray rendered from the preoperative CT. The\nCNN then initializes rapid intraoperative test-time optimization that uses the\ndifferentiable X-ray renderer to refine the solution. Our work further proposes\nseveral geometrically principled methods for sampling camera poses from\n$\\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving\nregistration in the tangent space $\\mathfrak{se}(3)$ with geodesic and\nmultiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy\nacross surgical datasets at intraoperative speeds, improving upon existing\nunsupervised methods by an order of magnitude and even outperforming supervised\nbaselines. Our code is available at https://github.com/eigenvivek/DiffPose.\n","authors":["Vivek Gopalakrishnan","Neel Dey","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2312.06358v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18501v1","updated":"2024-03-27T12:24:20Z","published":"2024-03-27T12:24:20Z","title":"HEMIT: H&E to Multiplex-immunohistochemistry Image Translation with\n  Dual-Branch Pix2pix Generator","summary":"  Computational analysis of multiplexed immunofluorescence histology data is\nemerging as an important method for understanding the tumour micro-environment\nin cancer. This work presents HEMIT, a dataset designed for translating\nHematoxylin and Eosin (H&E) sections to multiplex-immunohistochemistry (mIHC)\nimages, featuring DAPI, CD3, and panCK markers. Distinctively, HEMIT's mIHC\nimages are multi-component and cellular-level aligned with H&E, enriching\nsupervised stain translation tasks. To our knowledge, HEMIT is the first\npublicly available cellular-level aligned dataset that enables H&E to\nmulti-target mIHC image translation. This dataset provides the computer vision\ncommunity with a valuable resource to develop novel computational methods which\nhave the potential to gain new insights from H&E slide archives.\n  We also propose a new dual-branch generator architecture, using residual\nConvolutional Neural Networks (CNNs) and Swin Transformers which achieves\nbetter translation outcomes than other popular algorithms. When evaluated on\nHEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the\nhighest overall score on key metrics including the Structural Similarity Index\nMeasure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio\n(PSNR). Additionally, downstream analysis has been used to further validate the\nquality of the generated mIHC images. These results set a new benchmark in the\nfield of stain translation tasks.\n","authors":["Chang Bian","Beth Philips","Tim Cootes","Martin Fergie"],"pdf_url":"https://arxiv.org/pdf/2403.18501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.18495v1","updated":"2024-03-27T12:15:22Z","published":"2024-03-27T12:15:22Z","title":"Direct mineral content prediction from drill core images via transfer\n  learning","summary":"  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n","authors":["Romana Boiger","Sergey V. Churakov","Ignacio Ballester Llagaria","Georg Kosakowski","Raphael Wüst","Nikolaos I. Prasianakis"],"pdf_url":"https://arxiv.org/pdf/2403.18495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02203v5","updated":"2024-03-27T12:12:45Z","published":"2023-07-05T10:54:50Z","title":"Neural Fields for Interactive Visualization of Statistical Dependencies\n  in 3D Simulation Ensembles","summary":"  We present the first neural network that has learned to compactly represent\nand can efficiently reconstruct the statistical dependencies between the values\nof physical variables at different spatial locations in large 3D simulation\nensembles. Going beyond linear dependencies, we consider mutual information as\na measure of non-linear dependence. We demonstrate learning and reconstruction\nwith a large weather forecast ensemble comprising 1000 members, each storing\nmultiple physical variables at a 250 x 352 x 20 simulation grid. By\ncircumventing compute-intensive statistical estimators at runtime, we\ndemonstrate significantly reduced memory and computation requirements for\nreconstructing the major dependence structures. This enables embedding the\nestimator into a GPU-accelerated direct volume renderer and interactively\nvisualizing all mutual dependencies for a selected domain point.\n","authors":["Fatemeh Farokhmanesh","Kevin Höhlein","Christoph Neuhauser","Tobias Necker","Martin Weissmann","Takemasa Miyoshi","Rüdiger Westermann"],"pdf_url":"https://arxiv.org/pdf/2307.02203v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18493v1","updated":"2024-03-27T12:08:41Z","published":"2024-03-27T12:08:41Z","title":"VersaT2I: Improving Text-to-Image Models with Versatile Reward","summary":"  Recent text-to-image (T2I) models have benefited from large-scale and\nhigh-quality data, demonstrating impressive performance. However, these T2I\nmodels still struggle to produce images that are aesthetically pleasing,\ngeometrically accurate, faithful to text, and of good low-level quality. We\npresent VersaT2I, a versatile training framework that can boost the performance\nwith multiple rewards of any T2I model. We decompose the quality of the image\ninto several aspects such as aesthetics, text-image alignment, geometry,\nlow-level quality, etc. Then, for every quality aspect, we select high-quality\nimages in this aspect generated by the model as the training set to finetune\nthe T2I model using the Low-Rank Adaptation (LoRA). Furthermore, we introduce a\ngating function to combine multiple quality aspects, which can avoid conflicts\nbetween different quality aspects. Our method is easy to extend and does not\nrequire any manual annotation, reinforcement learning, or model architecture\nchanges. Extensive experiments demonstrate that VersaT2I outperforms the\nbaseline methods across various quality criteria.\n","authors":["Jianshu Guo","Wenhao Chai","Jie Deng","Hsiang-Wei Huang","Tian Ye","Yichen Xu","Jiawei Zhang","Jenq-Neng Hwang","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18490v1","updated":"2024-03-27T12:05:22Z","published":"2024-03-27T12:05:22Z","title":"I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic\n  Segmentation","summary":"  This paper proposes a new knowledge distillation method tailored for image\nsemantic segmentation, termed Intra- and Inter-Class Knowledge Distillation\n(I2CKD). The focus of this method is on capturing and transferring knowledge\nbetween the intermediate layers of teacher (cumbersome model) and student\n(compact model). For knowledge extraction, we exploit class prototypes derived\nfrom feature maps. To facilitate knowledge transfer, we employ a triplet loss\nin order to minimize intra-class variances and maximize inter-class variances\nbetween teacher and student prototypes. Consequently, I2CKD enables the student\nto better mimic the feature representation of the teacher for each class,\nthereby enhancing the segmentation performance of the compact network.\nExtensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal\nVOC and CamVid, using various teacher-student network pairs demonstrate the\neffectiveness of the proposed method.\n","authors":["Ayoub Karine","Thibault Napoléon","Maher Jridi"],"pdf_url":"https://arxiv.org/pdf/2403.18490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16943v2","updated":"2024-03-27T11:46:36Z","published":"2023-12-28T10:40:11Z","title":"SAR-Net: Multi-scale Direction-aware SAR Network via Global Information\n  Fusion","summary":"  Deep learning has driven significant progress in object detection using\nSynthetic Aperture Radar (SAR) imagery. Existing methods, while achieving\npromising results, often struggle to effectively integrate local and global\ninformation, particularly direction-aware features. This paper proposes\nSAR-Net, a novel framework specifically designed for global fusion of\ndirection-aware information in SAR object detection. SAR-Net leverages two key\ninnovations: the Unity Compensation Mechanism (UCM) and the Direction-aware\nAttention Module (DAM). UCM facilitates the establishment of complementary\nrelationships among features across different scales, enabling efficient global\ninformation fusion. Among them, Multi-scale Alignment Module (MAM) and distinct\nMulti-level Fusion Module (MFM) enhance feature integration by capturing both\ntexture detail and semantic information. Then, Multi-feature Embedding Module\n(MEM) feeds back global features into the primary branches, further improving\ninformation transmission. Additionally, DAM, through bidirectional attention\npolymerization, captures direction-aware information, effectively eliminating\nbackground interference. Extensive experiments demonstrate the effectiveness of\nSAR-Net, achieving state-of-the-art results on aircraft (SAR-AIRcraft-1.0) and\nship datasets (SSDD, HRSID), confirming its generalization capability and\nrobustness.\n","authors":["Mingxiang Cao","Jie Lei","Weiying Xie","Jiaqing Zhang","Daixun Li","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2312.16943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18476v1","updated":"2024-03-27T11:45:08Z","published":"2024-03-27T11:45:08Z","title":"Modeling uncertainty for Gaussian Splatting","summary":"  We present Stochastic Gaussian Splatting (SGS): the first framework for\nuncertainty estimation using Gaussian Splatting (GS). GS recently advanced the\nnovel-view synthesis field by achieving impressive reconstruction quality at a\nfraction of the computational cost of Neural Radiance Fields (NeRF). However,\ncontrary to the latter, it still lacks the ability to provide information about\nthe confidence associated with their outputs. To address this limitation, in\nthis paper, we introduce a Variational Inference-based approach that seamlessly\nintegrates uncertainty prediction into the common rendering pipeline of GS.\nAdditionally, we introduce the Area Under Sparsification Error (AUSE) as a new\nterm in the loss function, enabling optimization of uncertainty estimation\nalongside image reconstruction. Experimental results on the LLFF dataset\ndemonstrate that our method outperforms existing approaches in terms of both\nimage rendering quality and uncertainty estimation accuracy. Overall, our\nframework equips practitioners with valuable insights into the reliability of\nsynthesized views, facilitating safer decision-making in real-world\napplications.\n","authors":["Luca Savant","Diego Valsesia","Enrico Magli"],"pdf_url":"https://arxiv.org/pdf/2403.18476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.18471v1","updated":"2024-03-27T11:32:44Z","published":"2024-03-27T11:32:44Z","title":"DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face\n  Forgery Analysis","summary":"  The rapid progress in deep learning has given rise to hyper-realistic facial\nforgery methods, leading to concerns related to misinformation and security\nrisks. Existing face forgery datasets have limitations in generating\nhigh-quality facial images and addressing the challenges posed by evolving\ngenerative techniques. To combat this, we present DiffusionFace, the first\ndiffusion-based face forgery dataset, covering various forgery categories,\nincluding unconditional and Text Guide facial image generation, Img2Img,\nInpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace\ndataset stands out with its extensive collection of 11 diffusion models and the\nhigh-quality of the generated images, providing essential metadata and a\nreal-world internet-sourced forgery facial image dataset for evaluation.\nAdditionally, we provide an in-depth analysis of the data and introduce\npractical evaluation protocols to rigorously assess discriminative models'\neffectiveness in detecting counterfeit facial images, aiming to enhance\nsecurity in facial image authentication processes. The dataset is available for\ndownload at \\url{https://github.com/Rapisurazurite/DiffFace}.\n","authors":["Zhongxi Chen","Ke Sun","Ziyin Zhou","Xianming Lin","Xiaoshuai Sun","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18469v1","updated":"2024-03-27T11:28:57Z","published":"2024-03-27T11:28:57Z","title":"Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain\n  Adaptive Segmentation of 3D Point Clouds","summary":"  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to\nannotating new domains. Self-training is a competitive approach for this task,\nbut its performance is limited by different sensor sampling patterns (i.e.,\nvariations in point density) and incomplete training strategies. In this work,\nwe propose a density-guided translator (DGT), which translates point density\nbetween domains, and integrates it into a two-stage self-training pipeline\nnamed DGT-ST. First, in contrast to existing works that simultaneously conduct\ndata generation and feature/output alignment within unstable adversarial\ntraining, we employ the non-learnable DGT to bridge the domain gap at the input\nlevel. Second, to provide a well-initialized model for self-training, we\npropose a category-level adversarial network in stage one that utilizes the\nprototype to prevent negative transfer. Finally, by leveraging the designs\nabove, a domain-mixed self-training method with source-aware consistency loss\nis proposed in stage two to narrow the domain gap further. Experiments on two\nsynthetic-to-real segmentation tasks (SynLiDAR $\\rightarrow$ semanticKITTI and\nSynLiDAR $\\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms\nstate-of-the-art methods, achieving 9.4$\\%$ and 4.3$\\%$ mIoU improvements,\nrespectively. Code is available at \\url{https://github.com/yuan-zm/DGT-ST}.\n","authors":["Zhimin Yuan","Wankang Zeng","Yanfei Su","Weiquan Liu","Ming Cheng","Yulan Guo","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18469v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.18468v1","updated":"2024-03-27T11:28:32Z","published":"2024-03-27T11:28:32Z","title":"Deep Learning Segmentation and Classification of Red Blood Cells Using a\n  Large Multi-Scanner Dataset","summary":"  Digital pathology has recently been revolutionized by advancements in\nartificial intelligence, deep learning, and high-performance computing. With\nits advanced tools, digital pathology can help improve and speed up the\ndiagnostic process, reduce human errors, and streamline the reporting step. In\nthis paper, we report a new large red blood cell (RBC) image dataset and\npropose a two-stage deep learning framework for RBC image segmentation and\nclassification. The dataset is a highly diverse dataset of more than 100K RBCs\ncontaining eight different classes. The dataset, which is considerably larger\nthan any publicly available hematopathology dataset, was labeled independently\nby two hematopathologists who also manually created masks for RBC cell\nsegmentation. Subsequently, in the proposed framework, first, a U-Net model was\ntrained to achieve automatic RBC image segmentation. Second, an EfficientNetB0\nmodel was trained to classify RBC images into one of the eight classes using a\ntransfer learning approach with a 5X2 cross-validation scheme. An IoU of 98.03%\nand an average classification accuracy of 96.5% were attained on the test set.\nMoreover, we have performed experimental comparisons against several prominent\nCNN models. These comparisons show the superiority of the proposed model with a\ngood balance between performance and computational cost.\n","authors":["Mohamed Elmanna","Ahmed Elsafty","Yomna Ahmed","Muhammad Rushdi","Ahmed Morsy"],"pdf_url":"https://arxiv.org/pdf/2403.18468v1.pdf","comment":"15 pages, 12 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.18461v1","updated":"2024-03-27T11:19:34Z","published":"2024-03-27T11:19:34Z","title":"DiffStyler: Diffusion-based Localized Image Style Transfer","summary":"  Image style transfer aims to imbue digital imagery with the distinctive\nattributes of style targets, such as colors, brushstrokes, shapes, whilst\nconcurrently preserving the semantic integrity of the content. Despite the\nadvancements in arbitrary style transfer methods, a prevalent challenge remains\nthe delicate equilibrium between content semantics and style attributes. Recent\ndevelopments in large-scale text-to-image diffusion models have heralded\nunprecedented synthesis capabilities, albeit at the expense of relying on\nextensive and often imprecise textual descriptions to delineate artistic\nstyles. Addressing these limitations, this paper introduces DiffStyler, a novel\napproach that facilitates efficient and precise arbitrary image style transfer.\nDiffStyler lies the utilization of a text-to-image Stable Diffusion model-based\nLoRA to encapsulate the essence of style targets. This approach, coupled with\nstrategic cross-LoRA feature and attention injection, guides the style transfer\nprocess. The foundation of our methodology is rooted in the observation that\nLoRA maintains the spatial feature consistency of UNet, a discovery that\nfurther inspired the development of a mask-wise style transfer technique. This\ntechnique employs masks extracted through a pre-trained FastSAM model,\nutilizing mask prompts to facilitate feature fusion during the denoising\nprocess, thereby enabling localized style transfer that preserves the original\nimage's unaffected regions. Moreover, our approach accommodates multiple style\ntargets through the use of corresponding masks. Through extensive\nexperimentation, we demonstrate that DiffStyler surpasses previous methods in\nachieving a more harmonious balance between content preservation and style\nintegration.\n","authors":["Shaoxu Li"],"pdf_url":"https://arxiv.org/pdf/2403.18461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10522v4","updated":"2024-03-27T11:18:51Z","published":"2023-11-17T13:43:43Z","title":"Enhancing Object Coherence in Layout-to-Image Synthesis","summary":"  Layout-to-image synthesis is an emerging technique in conditional image\ngeneration. It aims to generate complex scenes, where users require fine\ncontrol over the layout of the objects in a scene. However, it remains\nchallenging to control the object coherence, including semantic coherence\n(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the\nhand and the racket should not be misaligned). In this paper, we propose a\nnovel diffusion model with effective global semantic fusion (GSF) and\nself-similarity feature enhancement modules to guide the object coherence for\nthis task. For semantic coherence, we argue that the image caption contains\nrich information for defining the semantic relationship within the objects in\nthe images. Instead of simply employing cross-attention between captions and\ngenerated images, which addresses the highly relevant layout restriction and\nsemantic coherence separately and thus leads to unsatisfying results shown in\nour experiments, we develop GSF to fuse the supervision from the layout\nrestriction and semantic coherence requirement and exploit it to guide the\nimage synthesis process. Moreover, to improve the physical coherence, we\ndevelop a Self-similarity Coherence Attention (SCA) module to explicitly\nintegrate local contextual physical coherence into each pixel's generation\nprocess. Specifically, we adopt a self-similarity map to encode the coherence\nrestrictions and employ it to extract coherent features from text embedding.\nThrough visualization of our self-similarity map, we explore the essence of\nSCA, revealing that its effectiveness is not only in capturing reliable\nphysical coherence patterns but also in enhancing complex texture generation.\nExtensive experiments demonstrate the superiority of our proposed method in\nboth image generation quality and controllability.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10522v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18454v1","updated":"2024-03-27T11:13:20Z","published":"2024-03-27T11:13:20Z","title":"Scaling Vision-and-Language Navigation With Offline RL","summary":"  The study of vision-and-language navigation (VLN) has typically relied on\nexpert trajectories, which may not always be available in real-world situations\ndue to the significant effort required to collect them. On the other hand,\nexisting approaches to training VLN agents that go beyond available expert data\ninvolve data augmentations or online exploration which can be tedious and\nrisky. In contrast, it is easy to access large repositories of suboptimal\noffline trajectories. Inspired by research in offline reinforcement learning\n(ORL), we introduce a new problem setup of VLN-ORL which studies VLN using\nsuboptimal demonstration data. We introduce a simple and effective\nreward-conditioned approach that can account for dataset suboptimality for\ntraining VLN agents, as well as benchmarks to evaluate progress and promote\nresearch in this area. We empirically study various noise models for\ncharacterizing dataset suboptimality among other unique challenges in VLN-ORL\nand instantiate it for the VLN$\\circlearrowright$BERT and MTVM architectures in\nthe R2R and RxR environments. Our experiments demonstrate that the proposed\nreward-conditioned approach leads to significant performance improvements, even\nin complex and intricate environments.\n","authors":["Valay Bundele","Mahesh Bhupati","Biplab Banerjee","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2403.18454v1.pdf","comment":"Published in Transactions on Machine Learning Research (04/2024)"},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18443v1","updated":"2024-03-27T11:00:33Z","published":"2024-03-27T11:00:33Z","title":"$\\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation\n  via Optical Flow Consistency and Feature Map Synthesis","summary":"  Self-supervised monocular depth estimation methods have been increasingly\ngiven much attention due to the benefit of not requiring large, labelled\ndatasets. Such self-supervised methods require high-quality salient features\nand consequently suffer from severe performance drop for indoor scenes, where\nlow-textured regions dominant in the scenes are almost indiscriminative. To\naddress the issue, we propose a self-supervised indoor monocular depth\nestimation framework called $\\mathrm{F^2Depth}$. A self-supervised optical flow\nestimation network is introduced to supervise depth learning. To improve\noptical flow estimation performance in low-textured areas, only some patches of\npoints with more discriminative features are adopted for finetuning based on\nour well-designed patch-based photometric loss. The finetuned optical flow\nestimation network generates high-accuracy optical flow as a supervisory signal\nfor depth estimation. Correspondingly, an optical flow consistency loss is\ndesigned. Multi-scale feature maps produced by finetuned optical flow\nestimation network perform warping to compute feature map synthesis loss as\nanother supervisory signal for depth learning. Experimental results on the NYU\nDepth V2 dataset demonstrate the effectiveness of the framework and our\nproposed losses. To evaluate the generalization ability of our\n$\\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of\napproximately 1500 points selected from 99 images in 18 scenes. Zero-shot\ngeneralization experiments on 7-Scenes dataset and Campus Indoor achieve\n$\\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show\nthat our model can generalize well to monocular images captured in unknown\nindoor scenes.\n","authors":["Xiaotong Guo","Huijie Zhao","Shuwei Shao","Xudong Li","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.17126v2","updated":"2024-03-27T10:50:54Z","published":"2022-11-30T16:03:24Z","title":"BEVUDA: Multi-geometric Space Alignments for Domain Adaptive BEV 3D\n  Object Detection","summary":"  Vision-centric bird-eye-view (BEV) perception has shown promising potential\nin autonomous driving. Recent works mainly focus on improving efficiency or\naccuracy but neglect the challenges when facing environment changing, resulting\nin severe degradation of transfer performance. For BEV perception, we figure\nout the significant domain gaps existing in typical real-world cross-domain\nscenarios and comprehensively solve the Domain Adaption (DA) problem for\nmulti-view 3D object detection. Since BEV perception approaches are complicated\nand contain several components, the domain shift accumulation on multiple\ngeometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In\nthis paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework\nto ease the domain shift accumulation, which consists of a Depth-Aware Teacher\n(DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines\ntarget lidar and reliable depth prediction to construct depth-aware\ninformation, extracting target domain-specific knowledge in Voxel and BEV\nfeature spaces. It then transfers the sufficient domain knowledge of multiple\nspaces to the student model. In order to jointly alleviate the domain shift,\nGAS projects multi-geometric space features to a shared geometric embedding\nspace and decreases data distribution distance between two domains. To verify\nthe effectiveness of our method, we conduct BEV 3D object detection experiments\non three cross-domain scenarios and achieve state-of-the-art performance.\n","authors":["Jiaming Liu","Rongyu Zhang","Xiaoqi Li","Xiaowei Chi","Zehui Chen","Ming Lu","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.17126v2.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2403.18442v1","updated":"2024-03-27T10:50:24Z","published":"2024-03-27T10:50:24Z","title":"Backpropagation-free Network for 3D Test-time Adaptation","summary":"  Real-world systems often encounter new data over time, which leads to\nexperiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods\ntend to apply computationally heavy and memory-intensive backpropagation-based\napproaches to handle this. Here, we propose a novel method that uses a\nbackpropagation-free approach for TTA for the specific case of 3D data. Our\nmodel uses a two-stream architecture to maintain knowledge about the source\ndomain as well as complementary target-domain-specific information. The\nbackpropagation-free property of our model helps address the well-known\nforgetting problem and mitigates the error accumulation issue. The proposed\nmethod also eliminates the need for the usually noisy process of\npseudo-labeling and reliance on costly self-supervised training. Moreover, our\nmethod leverages subspace learning, effectively reducing the distribution\nvariance between the two domains. Furthermore, the source-domain-specific and\nthe target-domain-specific streams are aligned using a novel entropy-based\nadaptive fusion strategy. Extensive experiments on popular benchmarks\ndemonstrate the effectiveness of our method. The code will be available at\nhttps://github.com/abie-e/BFTT3D.\n","authors":["Yanshuo Wang","Ali Cheraghian","Zeeshan Hayder","Jie Hong","Sameera Ramasinghe","Shafin Rahman","David Ahmedt-Aristizabal","Xuesong Li","Lars Petersson","Mehrtash Harandi"],"pdf_url":"https://arxiv.org/pdf/2403.18442v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.18113v2","updated":"2024-03-27T10:46:59Z","published":"2023-11-29T21:58:41Z","title":"Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D\n  Features","summary":"  With the immense growth of dataset sizes and computing resources in recent\nyears, so-called foundation models have become popular in NLP and vision tasks.\nIn this work, we propose to explore foundation models for the task of keypoint\ndetection on 3D shapes. A unique characteristic of keypoint detection is that\nit requires semantic and geometric awareness while demanding high localization\naccuracy. To address this problem, we propose, first, to back-project features\nfrom large pre-trained 2D vision models onto 3D shapes and employ them for this\ntask. We show that we obtain robust 3D features that contain rich semantic\ninformation and analyze multiple candidate features stemming from different 2D\nfoundation models. Second, we employ a keypoint candidate optimization module\nwhich aims to match the average observed distribution of keypoints on the shape\nand is guided by the back-projected features. The resulting approach achieves a\nnew state of the art for few-shot keypoint detection on the KeyPointNet\ndataset, almost doubling the performance of the previous best methods.\n","authors":["Thomas Wimmer","Peter Wonka","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2311.18113v2.pdf","comment":"Accepted to CVPR 2024, Project page:\n  https://wimmerth.github.io/back-to-3d.html"},{"id":"http://arxiv.org/abs/2401.08742v2","updated":"2024-03-27T10:33:02Z","published":"2024-01-16T18:58:36Z","title":"Fast Dynamic 3D Object Generation from a Single-view Video","summary":"  Generating dynamic 3D object from a single-view video is challenging due to\nthe lack of 4D labeled data. Extending image-to-3D pipelines by transferring\noff-the-shelf image generation models such as score distillation sampling,\nexisting methods tend to be slow and expensive to scale due to the need for\nback-propagating the information-limited supervision signals through a large\npretrained model. To address this, we propose an efficient video-to-4D object\ngeneration framework called Efficient4D. It generates high-quality\nspacetime-consistent images under different camera views, and then uses them as\nlabeled data to directly train a novel 4D Gaussian splatting model with\nexplicit point cloud geometry, enabling real-time rendering under continuous\ncamera trajectories. Extensive experiments on synthetic and real videos show\nthat Efficient4D offers a remarkable 20-fold increase in speed when compared to\nprior art alternatives while preserving the quality of novel view synthesis.\nFor example, Efficient4D takes only 6 mins to model a dynamic object, vs 120\nmins by Consistent4D.\n","authors":["Zijie Pan","Zeyu Yang","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.08742v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15098v2","updated":"2024-03-27T10:26:23Z","published":"2024-03-22T10:36:50Z","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","summary":"  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, e.g., in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\nhttps://github.com/vita-epfl/UniTraj\n","authors":["Lan Feng","Mohammadhossein Bahari","Kaouther Messaoud Ben Amor","Éloi Zablocki","Matthieu Cord","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.15098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12359v2","updated":"2024-03-27T10:18:04Z","published":"2023-12-19T17:40:27Z","title":"CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary\n  semantic segmentation","summary":"  The popular CLIP model displays impressive zero-shot capabilities thanks to\nits seamless interaction with arbitrary text prompts. However, its lack of\nspatial awareness makes it unsuitable for dense computer vision tasks, e.g.,\nsemantic segmentation, without an additional fine-tuning step that often uses\nannotations and can potentially suppress its original open-vocabulary\nproperties. Meanwhile, self-supervised representation methods have demonstrated\ngood localization properties without human-made annotations nor explicit\nsupervision. In this work, we take the best of both worlds and propose an\nopen-vocabulary semantic segmentation method, which does not require any\nannotations. We propose to locally improve dense MaskCLIP features, which are\ncomputed with a simple modification of CLIP's last pooling layer, by\nintegrating localization priors extracted from self-supervised features. By\ndoing so, we greatly improve the performance of MaskCLIP and produce smooth\noutputs. Moreover, we show that the used self-supervised feature properties can\ndirectly be learnt from CLIP features. Our method CLIP-DINOiser needs only a\nsingle forward pass of CLIP and two light convolutional layers at inference, no\nextra supervision nor extra memory and reaches state-of-the-art results on\nchallenging and fine-grained benchmarks such as COCO, Pascal Context,\nCityscapes and ADE20k. The code to reproduce our results is available at\nhttps://github.com/wysoczanska/clip_dinoiser.\n","authors":["Monika Wysoczańska","Oriane Siméoni","Michaël Ramamonjisoa","Andrei Bursuc","Tomasz Trzciński","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2312.12359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12480v2","updated":"2024-03-27T10:12:32Z","published":"2023-12-19T15:34:52Z","title":"Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual\n  Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) is proposed to migrate a source\npre-trained model to continually changing target distributions, addressing\nreal-world dynamism. Existing CTTA methods mainly rely on entropy minimization\nor teacher-student pseudo-labeling schemes for knowledge extraction in\nunlabeled target domains. However, dynamic data distributions cause\nmiscalibrated predictions and noisy pseudo-labels in existing self-supervised\nlearning methods, hindering the effective mitigation of error accumulation and\ncatastrophic forgetting problems during the continual adaptation process. To\ntackle these issues, we propose a continual self-supervised method, Adaptive\nDistribution Masked Autoencoders (ADMA), which enhances the extraction of\ntarget domain knowledge while mitigating the accumulation of distribution\nshifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism\nto adaptively sample masked positions, followed by establishing consistency\nconstraints between the masked target samples and the original target samples.\nAdditionally, for masked tokens, we utilize an efficient decoder to reconstruct\na hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients),\nleveraging its invariant properties to boost task-relevant representations.\nThrough conducting extensive experiments on four widely recognized benchmarks,\nour proposed method attains state-of-the-art performance in both classification\nand segmentation CTTA tasks. Our project page:\nhttps://sites.google.com/view/continual-mae/home.\n","authors":["Jiaming Liu","Ran Xu","Senqiao Yang","Renrui Zhang","Qizhe Zhang","Zehui Chen","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.12480v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.18417v1","updated":"2024-03-27T10:09:38Z","published":"2024-03-27T10:09:38Z","title":"ECNet: Effective Controllable Text-to-Image Diffusion Models","summary":"  The conditional text-to-image diffusion models have garnered significant\nattention in recent years. However, the precision of these models is often\ncompromised mainly for two reasons, ambiguous condition input and inadequate\ncondition guidance over single denoising loss. To address the challenges, we\nintroduce two innovative solutions. Firstly, we propose a Spatial Guidance\nInjector (SGI) which enhances conditional detail by encoding text inputs with\nprecise annotation information. This method directly tackles the issue of\nambiguous control inputs by providing clear, annotated guidance to the model.\nSecondly, to overcome the issue of limited conditional supervision, we\nintroduce Diffusion Consistency Loss (DCL), which applies supervision on the\ndenoised latent code at any given time step. This encourages consistency\nbetween the latent code at each time step and the input signal, thereby\nenhancing the robustness and accuracy of the output. The combination of SGI and\nDCL results in our Effective Controllable Network (ECNet), which offers a more\naccurate controllable end-to-end text-to-image generation framework with a more\nprecise conditioning input and stronger controllable supervision. We validate\nour approach through extensive experiments on generation under various\nconditions, such as human body skeletons, facial landmarks, and sketches of\ngeneral objects. The results consistently demonstrate that our method\nsignificantly enhances the controllability and robustness of the generated\nimages, outperforming existing state-of-the-art controllable text-to-image\nmodels.\n","authors":["Sicheng Li","Keqiang Sun","Zhixin Lai","Xiaoshi Wu","Feng Qiu","Haoran Xie","Kazunori Miyata","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06075v2","updated":"2024-03-27T09:51:15Z","published":"2023-09-12T09:12:37Z","title":"A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation","summary":"  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n","authors":["Francesco Galati","Daniele Falcetta","Rosa Cortese","Barbara Casolla","Ferran Prados","Ninon Burgos","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2309.06075v2.pdf","comment":"Accepted at the 34th British Machine Vision Conference (BMVC)"},{"id":"http://arxiv.org/abs/2403.18407v1","updated":"2024-03-27T09:49:37Z","published":"2024-03-27T09:49:37Z","title":"A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is\n  Critical for Semi-supervised Classification","summary":"  Semi-supervised learning (SSL) is a practical challenge in computer vision.\nPseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of\nThe Art (SOTA) performances in SSL. These approaches employ a\nthreshold-to-pseudo-label (T2L) process to generate PLs by truncating the\nconfidence scores of unlabeled data predicted by the self-training method.\nHowever, self-trained models typically yield biased and high-variance\npredictions, especially in the scenarios when a little labeled data are\nsupplied. To address this issue, we propose a lightweight channel-based\nensemble method to effectively consolidate multiple inferior PLs into the\ntheoretically guaranteed unbiased and low-variance one. Importantly, our\napproach can be readily extended to any SSL framework, such as FixMatch or\nFreeMatch. Experimental results demonstrate that our method significantly\noutperforms state-of-the-art techniques on CIFAR10/100 in terms of\neffectiveness and efficiency.\n","authors":["Jiaqi Wu","Junbiao Pang","Baochang Zhang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.05262v2","updated":"2024-03-27T09:43:41Z","published":"2024-03-08T12:35:07Z","title":"Debiasing Multimodal Large Language Models","summary":"  In the realms of computer vision and natural language processing, Large\nVision-Language Models (LVLMs) have become indispensable tools, proficient in\ngenerating textual descriptions based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias in the generated\ncontent, where the output is primarily influenced by the underlying Large\nLanguage Models (LLMs) prior rather than the input image. Our empirical\nexperiments underscore the persistence of this bias, as LVLMs often provide\nconfident answers even in the absence of relevant images or given incongruent\nvisual input. To rectify these biases and redirect the model's focus toward\nvision information, we introduce two simple, training-free strategies. Firstly,\nfor tasks such as classification or multi-choice question-answering (QA), we\npropose a ``calibration'' step through affine transformation to adjust the\noutput distribution. This ``Post-Hoc debias'' approach ensures uniform scores\nfor each answer when the image is absent, serving as an effective\nregularization technique to alleviate the influence of LLM priors. For more\nintricate open-ended generation tasks, we extend this method to ``Debias\nsampling'', drawing inspirations from contrastive decoding methods.\nFurthermore, our investigation sheds light on the instability of LVLMs across\nvarious decoding configurations. Through systematic exploration of different\nsettings, we significantly enhance performance, surpassing reported results and\nraising concerns about the fairness of existing evaluations. Comprehensive\nexperiments substantiate the effectiveness of our proposed strategies in\nmitigating biases. These strategies not only prove beneficial in minimizing\nhallucinations but also contribute to the generation of more helpful and\nprecise illustrations.\n","authors":["Yi-Fan Zhang","Weichen Yu","Qingsong Wen","Xue Wang","Zhang Zhang","Liang Wang","Rong Jin","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2403.05262v2.pdf","comment":"38 pages, 17 figures"},{"id":"http://arxiv.org/abs/2401.01647v2","updated":"2024-03-27T09:39:41Z","published":"2024-01-03T09:46:43Z","title":"SIGNeRF: Scene Integrated Generation for Neural Radiance Fields","summary":"  Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.\n","authors":["Jan-Niklas Dihlmann","Andreas Engelhardt","Hendrik Lensch"],"pdf_url":"https://arxiv.org/pdf/2401.01647v2.pdf","comment":"Project Page: https://signerf.jdihlmann.com"},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.11656v2","updated":"2024-03-27T09:34:44Z","published":"2024-03-18T10:53:00Z","title":"LocalStyleFool: Regional Video Style Transfer Attack Using Segment\n  Anything Model","summary":"  Previous work has shown that well-crafted adversarial perturbations can\nthreaten the security of video recognition systems. Attackers can invade such\nmodels with a low query budget when the perturbations are semantic-invariant,\nsuch as StyleFool. Despite the query efficiency, the naturalness of the minutia\nareas still requires amelioration, since StyleFool leverages style transfer to\nall pixels in each frame. To close the gap, we propose LocalStyleFool, an\nimproved black-box video adversarial attack that superimposes regional\nstyle-transfer-based perturbations on videos. Benefiting from the popularity\nand scalably usability of Segment Anything Model (SAM), we first extract\ndifferent regions according to semantic information and then track them through\nthe video stream to maintain the temporal consistency. Then, we add\nstyle-transfer-based perturbations to several regions selected based on the\nassociative criterion of transfer-based gradient information and regional area.\nPerturbation fine adjustment is followed to make stylized videos adversarial.\nWe demonstrate that LocalStyleFool can improve both intra-frame and inter-frame\nnaturalness through a human-assessed survey, while maintaining competitive\nfooling rate and query efficiency. Successful experiments on the\nhigh-resolution dataset also showcase that scrupulous segmentation of SAM helps\nto improve the scalability of adversarial attacks under high-resolution data.\n","authors":["Yuxin Cao","Jinghao Li","Xi Xiao","Derui Wang","Minhui Xue","Hao Ge","Wei Liu","Guangwu Hu"],"pdf_url":"https://arxiv.org/pdf/2403.11656v2.pdf","comment":"Accepted to 2024 IEEE Security and Privacy Workshops (SPW)"},{"id":"http://arxiv.org/abs/2403.18388v1","updated":"2024-03-27T09:25:20Z","published":"2024-03-27T09:25:20Z","title":"FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion","summary":"  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient\ncomputing compared with Artificial Neural Networks (ANNs), closely mirroring\nbiological neural processes. However, this potential comes with inherent\nchallenges in directly training SNNs through spatio-temporal backpropagation --\nstemming from the temporal dynamics of spiking neurons and their discrete\nsignal processing -- which necessitates alternative ways of training, most\nnotably through ANN-SNN conversion. In this work, we introduce a lightweight\nForward Temporal Bias Correction (FTBC) technique, aimed at enhancing\nconversion accuracy without the computational overhead. We ground our method on\nprovided theoretical findings that through proper temporal bias calibration the\nexpected error of ANN-SNN conversion can be reduced to be zero after each time\nstep. We further propose a heuristic algorithm for finding the temporal bias\nonly in the forward pass, thus eliminating the computational burden of\nbackpropagation and we evaluate our method on CIFAR-10/100 and ImageNet\ndatasets, achieving a notable increase in accuracy on all datasets. Codes are\nreleased at a GitHub repository.\n","authors":["Xiaofeng Wu","Velibor Bojkovic","Bin Gu","Kun Suo","Kai Zou"],"pdf_url":"https://arxiv.org/pdf/2403.18388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06733v3","updated":"2024-03-27T09:24:56Z","published":"2023-12-11T10:43:28Z","title":"TULIP: Transformer for Upsampling of LiDAR Point Cloud","summary":"  LiDAR Upsampling is a challenging task for the perception systems of robots\nand autonomous vehicles, due to the sparse and irregular structure of\nlarge-scale scene contexts. Recent works propose to solve this problem by\nconverting LiDAR data from 3D Euclidean space into an image super-resolution\nproblem in 2D image space. Although their methods can generate high-resolution\nrange images with fine-grained details, the resulting 3D point clouds often\nblur out details and predict invalid points. In this paper, we propose TULIP, a\nnew method to reconstruct high-resolution LiDAR point clouds from\nlow-resolution LiDAR input. We also follow a range image-based approach but\nspecifically modify the patch and window geometries of a Swin-Transformer-based\nnetwork to better fit the characteristics of range images. We conducted several\nexperiments on three public real-world and simulated datasets. TULIP\noutperforms state-of-the-art methods in all relevant metrics and generates\nrobust and more realistic point clouds than prior works.\n","authors":["Bin Yang","Patrick Pfreundschuh","Roland Siegwart","Marco Hutter","Peyman Moghadam","Vaishakh Patil"],"pdf_url":"https://arxiv.org/pdf/2312.06733v3.pdf","comment":"The paper was accepted by CVPR20224"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.18766v1","updated":"2024-03-27T17:05:03Z","published":"2024-03-27T17:05:03Z","title":"Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means","summary":"  This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.\n","authors":["Rustam Mussabayev","Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2403.18766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18684v1","updated":"2024-03-27T15:27:36Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18667v1","updated":"2024-03-27T15:11:00Z","published":"2024-03-27T15:11:00Z","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users","summary":"  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n","authors":["Yejin Kim","Scott Rome","Kevin Foley","Mayur Nankani","Rimon Melamed","Javier Morales","Abhay Yadav","Maria Peifer","Sardar Hamidian","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18667v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18628v1","updated":"2024-03-27T14:37:01Z","published":"2024-03-27T14:37:01Z","title":"To Recommend or Not: Recommendability Identification in Conversations\n  with Pre-trained Language Models","summary":"  Most current recommender systems primarily focus on what to recommend,\nassuming users always require personalized recommendations. However, with the\nwidely spread of ChatGPT and other chatbots, a more crucial problem in the\ncontext of conversational systems is how to minimize user disruption when we\nprovide recommendation services for users. While previous research has\nextensively explored different user intents in dialogue systems, fewer efforts\nare made to investigate whether recommendations should be provided. In this\npaper, we formally define the recommendability identification problem, which\naims to determine whether recommendations are necessary in a specific scenario.\nFirst, we propose and define the recommendability identification task, which\ninvestigates the need for recommendations in the current conversational\ncontext. A new dataset is constructed. Subsequently, we discuss and evaluate\nthe feasibility of leveraging pre-trained language models (PLMs) for\nrecommendability identification. Finally, through comparative experiments, we\ndemonstrate that directly employing PLMs with zero-shot results falls short of\nmeeting the task requirements. Besides, fine-tuning or utilizing soft prompt\ntechniques yields comparable results to traditional classification methods. Our\nwork is the first to study recommendability before recommendation and provides\npreliminary ways to make it a fundamental component of the future\nrecommendation system.\n","authors":["Zhefan Wang","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18623v1","updated":"2024-03-27T14:34:22Z","published":"2024-03-27T14:34:22Z","title":"Antitrust, Amazon, and Algorithmic Auditing","summary":"  In digital markets, antitrust law and special regulations aim to ensure that\nmarkets remain competitive despite the dominating role that digital platforms\nplay today in everyone's life. Unlike traditional markets, market participant\nbehavior is easily observable in these markets. We present a series of\nempirical investigations into the extent to which Amazon engages in practices\nthat are typically described as self-preferencing. We discuss how the computer\nscience tools used in this paper can be used in a regulatory environment that\nis based on algorithmic auditing and requires regulating digital markets at\nscale.\n","authors":["Abhisek Dash","Abhijnan Chakraborty","Saptarshi Ghosh","Animesh Mukherjee","Jens Frankenreiter","Stefan Bechtold","Krishna P. Gummadi"],"pdf_url":"https://arxiv.org/pdf/2403.18623v1.pdf","comment":"The paper has been accepted to appear at Journal of Institutional and\n  Theoretical Economics (JITE) 2024"},{"id":"http://arxiv.org/abs/2403.18604v1","updated":"2024-03-27T14:24:28Z","published":"2024-03-27T14:24:28Z","title":"Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity,\n  and Seasonality into Tourism Recommender Systems","summary":"  In an era of information overload and complex decision-making processes,\nRecommender Systems (RS) have emerged as indispensable tools across diverse\ndomains, particularly travel and tourism. These systems simplify trip planning\nby offering personalized recommendations that consider individual preferences\nand address broader challenges like seasonality, travel regulations, and\ncapacity constraints. The intricacies of the tourism domain, characterized by\nmultiple stakeholders, including consumers, item providers, platforms, and\nsociety, underscore the complexity of achieving balance among diverse\ninterests. Although previous research has focused on fairness in Tourism\nRecommender Systems (TRS) from a multistakeholder perspective, limited work has\nfocused on generating sustainable recommendations.\n  Our paper introduces a novel approach for assigning a sustainability\nindicator (SF index) for city trips accessible from the users' starting point,\nintegrating Co2e analysis, destination popularity, and seasonal demand. Our\nmethodology involves comprehensive data gathering on transportation modes and\nemissions, complemented by analyses of destination popularity and seasonal\ndemand. A user study validates our index, showcasing its practicality and\nefficacy in providing well-rounded and sustainable city trip recommendations.\nOur findings contribute significantly to the evolution of responsible tourism\nstrategies, harmonizing the interests of tourists, local communities, and the\nenvironment while paving the way for future research in responsible and\nequitable tourism practices.\n","authors":["Ashmi Banerjee","Tunar Mahmudov","Emil Adler","Fitri Nur Aisyah","Wolfgang Wörndl"],"pdf_url":"https://arxiv.org/pdf/2403.18604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v3","updated":"2024-03-27T13:59:57Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18536v1","updated":"2024-03-27T13:12:41Z","published":"2024-03-27T13:12:41Z","title":"A Novel Behavior-Based Recommendation System for E-commerce","summary":"  The majority of existing recommender systems rely on user ratings, which are\nlimited by the lack of user collaboration and the sparsity problem. To address\nthese issues, this study proposes a behavior-based recommender system that\nleverages customers' natural behaviors, such as browsing and clicking, on\ne-commerce platforms. The proposed recommendation system involves clustering\nactive customers, determining neighborhoods, collecting similar users,\ncalculating product reputation based on similar users, and recommending\nhigh-reputation products. To overcome the complexity of customer behaviors and\ntraditional clustering methods, an unsupervised clustering approach based on\nproduct categories is developed to enhance the recommendation methodology. This\nstudy makes notable contributions in several aspects. Firstly, a groundbreaking\nbehavior-based recommendation methodology is developed, incorporating customer\nbehavior to generate accurate and tailored recommendations leading to improved\ncustomer satisfaction and engagement. Secondly, an original unsupervised\nclustering method, focusing on product categories, enables more precise\nclustering and facilitates accurate recommendations. Finally, an approach to\ndetermine neighborhoods for active customers within clusters is established,\nensuring grouping of customers with similar behavioral patterns to enhance\nrecommendation accuracy and relevance. The proposed recommendation methodology\nand clustering method contribute to improved recommendation performance,\noffering valuable insights for researchers and practitioners in the field of\ne-commerce recommendation systems. Additionally, the proposed method\noutperforms benchmark methods in experiments conducted using a behavior dataset\nfrom the well-known e-commerce site Alibaba.\n","authors":["Reza Barzegar Nozari","Mahdi Divsalar","Sepehr Akbarzadeh Abkenar","Mohammadreza Fadavi Amiri","Ali Divsalar"],"pdf_url":"https://arxiv.org/pdf/2403.18536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18480v1","updated":"2024-03-27T11:49:58Z","published":"2024-03-27T11:49:58Z","title":"Enhanced Generative Recommendation via Content and Collaboration\n  Integration","summary":"  Generative recommendation has emerged as a promising paradigm aimed at\naugmenting recommender systems with recent advancements in generative\nartificial intelligence. This task has been formulated as a\nsequence-to-sequence generation process, wherein the input sequence encompasses\ndata pertaining to the user's previously interacted items, and the output\nsequence denotes the generative identifier for the suggested item. However,\nexisting generative recommendation approaches still encounter challenges in (i)\neffectively integrating user-item collaborative signals and item content\ninformation within a unified generative framework, and (ii) executing an\nefficient alignment between content information and collaborative signals.\n  In this paper, we introduce content-based collaborative generation for\nrecommender systems, denoted as ColaRec. To capture collaborative signals, the\ngenerative item identifiers are derived from a pretrained collaborative\nfiltering model, while the user is represented through the aggregation of\ninteracted items' content. Subsequently, the aggregated textual description of\nitems is fed into a language model to encapsulate content information. This\nintegration enables ColaRec to amalgamate collaborative signals and content\ninformation within an end-to-end framework. Regarding the alignment, we propose\nan item indexing task to facilitate the mapping between the content-based\nsemantic space and the interaction-based collaborative space. Additionally, a\ncontrastive loss is introduced to ensure that items with similar collaborative\nGIDs possess comparable content representations, thereby enhancing alignment.\nTo validate the efficacy of ColaRec, we conduct experiments on three benchmark\ndatasets. Empirical results substantiate the superior performance of ColaRec.\n","authors":["Yidan Wang","Zhaochun Ren","Weiwei Sun","Jiyuan Yang","Zhixiang Liang","Xin Chen","Ruobing Xie","Su Yan","Xu Zhang","Pengjie Ren","Zhumin Chen","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2403.18480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18479v1","updated":"2024-03-27T11:49:55Z","published":"2024-03-27T11:49:55Z","title":"Lightweight Embeddings for Graph Collaborative Filtering","summary":"  Graph neural networks (GNNs) are currently one of the most performant\ncollaborative filtering methods. Meanwhile, owing to the use of an embedding\ntable to represent each user/item as a distinct vector, GNN-based recommenders\nhave inherited the long-standing defect of parameter inefficiency. As a common\npractice for scalable embeddings, parameter sharing enables the use of fewer\nembedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most\nexisting methods are a heuristically designed, predefined mapping from each\nuser's/item's ID to the corresponding meta-embedding indexes, thus simplifying\nthe optimization problem into learning only the meta-embeddings. However, in\nthe context of GNN-based collaborative filtering, such a fixed mapping omits\nthe semantic correlations between entities that are evident in the user-item\ninteraction graph, leading to suboptimal recommendation performance. To this\nend, we propose Lightweight Embeddings for Graph Collaborative Filtering\n(LEGCF), a parameter-efficient embedding framework dedicated to GNN-based\nrecommenders. LEGCF innovatively introduces an assignment matrix as an extra\nlearnable component on top of meta-embeddings. To jointly optimize these two\nheavily entangled components, aside from learning the meta-embeddings by\nminimizing the recommendation loss, LEGCF further performs efficient assignment\nupdate by enforcing a novel semantic similarity constraint and finding its\nclosed-form solution based on matrix pseudo-inverse. The meta-embeddings and\nassignment matrix are alternately updated, where the latter is sparsified on\nthe fly to ensure negligible storage overhead. Extensive experiments on three\nbenchmark datasets have verified LEGCF's smallest trade-off between size and\nperformance, with consistent accuracy gain over state-of-the-art baselines. The\ncodebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.\n","authors":["Xurong Liang","Tong Chen","Lizhen Cui","Yang Wang","Meng Wang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2403.18479v1.pdf","comment":"Accepted by SIGIR '24"},{"id":"http://arxiv.org/abs/2311.07838v3","updated":"2024-03-27T11:36:46Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v3.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.18462v1","updated":"2024-03-27T11:20:48Z","published":"2024-03-27T11:20:48Z","title":"Decoy Effect In Search Interaction: Understanding User Behavior and\n  Measuring System Vulnerability","summary":"  This study examines the decoy effect's underexplored influence on user search\ninteractions and methods for measuring information retrieval (IR) systems'\nvulnerability to this effect. It explores how decoy results alter users'\ninteractions on search engine result pages, focusing on metrics like\nclick-through likelihood, browsing time, and perceived document usefulness. By\nanalyzing user interaction logs from multiple datasets, the study demonstrates\nthat decoy results significantly affect users' behavior and perceptions.\nFurthermore, it investigates how different levels of task difficulty and user\nknowledge modify the decoy effect's impact, finding that easier tasks and lower\nknowledge levels lead to higher engagement with target documents. In terms of\nIR system evaluation, the study introduces the DEJA-VU metric to assess\nsystems' susceptibility to the decoy effect, testing it on specific retrieval\ntasks. The results show differences in systems' effectiveness and\nvulnerability, contributing to our understanding of cognitive biases in search\nbehavior and suggesting pathways for creating more balanced and bias-aware IR\nevaluations.\n","authors":["Nuo Chen","Jiqun Liu","Hanpei Fang","Yuankai Luo","Tetsuya Sakai","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18435v1","updated":"2024-03-27T10:40:14Z","published":"2024-03-27T10:40:14Z","title":"DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via\n  Structural Word Alignment","summary":"  Recent research demonstrates the effectiveness of using pre-trained language\nmodels for legal case retrieval. Most of the existing works focus on improving\nthe representation ability for the contextualized embedding of the [CLS] token\nand calculate relevance using textual semantic similarity. However, in the\nlegal domain, textual semantic similarity does not always imply that the cases\nare relevant enough. Instead, relevance in legal cases primarily depends on the\nsimilarity of key facts that impact the final judgment. Without proper\ntreatments, the discriminative ability of learned representations could be\nlimited since legal cases are lengthy and contain numerous non-key facts. To\nthis end, we introduce DELTA, a discriminative model designed for legal case\nretrieval. The basic idea involves pinpointing key facts in legal cases and\npulling the contextualized embedding of the [CLS] token closer to the key facts\nwhile pushing away from the non-key facts, which can warm up the case embedding\nspace in an unsupervised manner. To be specific, this study brings the word\nalignment mechanism to the contextual masked auto-encoder. First, we leverage\nshallow decoders to create information bottlenecks, aiming to enhance the\nrepresentation ability. Second, we employ the deep decoder to enable\ntranslation between different structures, with the goal of pinpointing key\nfacts to enhance discriminative ability. Comprehensive experiments conducted on\npublicly available legal benchmarks show that our approach can outperform\nexisting state-of-the-art methods in legal case retrieval. It provides a new\nperspective on the in-depth understanding and processing of legal case\ndocuments.\n","authors":["Haitao Li","Qingyao Ai","Xinyan Han","Jia Chen","Qian Dong","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18435v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.06747v5","updated":"2024-03-27T10:32:29Z","published":"2024-03-11T14:13:41Z","title":"MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation","summary":"  Compared to business-to-consumer (B2C) e-commerce systems,\nconsumer-to-consumer (C2C) e-commerce platforms usually encounter the\nlimited-stock problem, that is, a product can only be sold one time in a C2C\nsystem. This poses several unique challenges for click-through rate (CTR)\nprediction. Due to limited user interactions for each product (i.e. item), the\ncorresponding item embedding in the CTR model may not easily converge. This\nmakes the conventional sequence modeling based approaches cannot effectively\nutilize user history information since historical user behaviors contain a\nmixture of items with different volume of stocks. Particularly, the attention\nmechanism in a sequence model tends to assign higher score to products with\nmore accumulated user interactions, making limited-stock products being ignored\nand contribute less to the final output. To this end, we propose the Meta-Split\nNetwork (MSNet) to split user history sequence regarding to the volume of stock\nfor each product, and adopt differentiated modeling approaches for different\nsequences. As for the limited-stock products, a meta-learning approach is\napplied to address the problem of inconvergence, which is achieved by designing\nmeta scaling and shifting networks with ID and side information. In addition,\ntraditional approach can hardly update item embedding once the product is\nconsumed. Thereby, we propose an auxiliary loss that makes the parameters\nupdatable even when the product is no longer in distribution. To the best of\nour knowledge, this is the first solution addressing the recommendation of\nlimited-stock product. Experimental results on the production dataset and\nonline A/B testing demonstrate the effectiveness of our proposed method.\n","authors":["Wenhao Wu","Jialiang Zhou","Ailong He","Shuguang Han","Jufeng Chen","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.06747v5.pdf","comment":"Accepted at WWW 2024. This work has already been deployed on the\n  Xianyu platform in Alibaba. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.18348v1","updated":"2024-03-27T08:39:42Z","published":"2024-03-27T08:39:42Z","title":"Sequential Recommendation with Latent Relations based on Large Language\n  Model","summary":"  Sequential recommender systems predict items that may interest users by\nmodeling their preferences based on historical interactions. Traditional\nsequential recommendation methods rely on capturing implicit collaborative\nfiltering signals among items. Recent relation-aware sequential recommendation\nmodels have achieved promising performance by explicitly incorporating item\nrelations into the modeling of user historical sequences, where most relations\nare extracted from knowledge graphs. However, existing methods rely on manually\npredefined relations and suffer the sparsity issue, limiting the generalization\nability in diverse scenarios with varied item relations. In this paper, we\npropose a novel relation-aware sequential recommendation framework with Latent\nRelation Discovery (LRD). Different from previous relation-aware models that\nrely on predefined rules, we propose to leverage the Large Language Model (LLM)\nto provide new types of relations and connections between items. The motivation\nis that LLM contains abundant world knowledge, which can be adopted to mine\nlatent relations of items for recommendation. Specifically, inspired by that\nhumans can describe relations between items using natural language, LRD\nharnesses the LLM that has demonstrated human-like knowledge to obtain language\nknowledge representations of items. These representations are fed into a latent\nrelation discovery module based on the discrete state variational autoencoder\n(DVAE). Then the self-supervised relation discovery tasks and recommendation\ntasks are jointly optimized. Experimental results on multiple public datasets\ndemonstrate our proposed latent relations discovery method can be incorporated\nwith existing relation-aware sequential recommendation models and significantly\nimprove the performance. Further analysis experiments indicate the\neffectiveness and reliability of the discovered latent relations.\n","authors":["Shenghao Yang","Weizhi Ma","Peijie Sun","Qingyao Ai","Yiqun Liu","Mingchen Cai","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18348v1.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18325v1","updated":"2024-03-27T08:06:56Z","published":"2024-03-27T08:06:56Z","title":"Common Sense Enhanced Knowledge-based Recommendation with Large Language\n  Model","summary":"  Knowledge-based recommendation models effectively alleviate the data sparsity\nissue leveraging the side information in the knowledge graph, and have achieved\nconsiderable performance. Nevertheless, the knowledge graphs used in previous\nwork, namely metadata-based knowledge graphs, are usually constructed based on\nthe attributes of items and co-occurring relations (e.g., also buy), in which\nthe former provides limited information and the latter relies on sufficient\ninteraction data and still suffers from cold start issue. Common sense, as a\nform of knowledge with generality and universality, can be used as a supplement\nto the metadata-based knowledge graph and provides a new perspective for\nmodeling users' preferences. Recently, benefiting from the emergent world\nknowledge of the large language model, efficient acquisition of common sense\nhas become possible. In this paper, we propose a novel knowledge-based\nrecommendation framework incorporating common sense, CSRec, which can be\nflexibly coupled to existing knowledge-based methods. Considering the challenge\nof the knowledge gap between the common sense-based knowledge graph and\nmetadata-based knowledge graph, we propose a knowledge fusion approach based on\nmutual information maximization theory. Experimental results on public datasets\ndemonstrate that our approach significantly improves the performance of\nexisting knowledge-based recommendation models.\n","authors":["Shenghao Yang","Weizhi Ma","Peijie Sun","Min Zhang","Qingyao Ai","Yiqun Liu","Mingchen Cai"],"pdf_url":"https://arxiv.org/pdf/2403.18325v1.pdf","comment":"Accepted by DASFAA 2024"},{"id":"http://arxiv.org/abs/2403.18317v1","updated":"2024-03-27T07:40:05Z","published":"2024-03-27T07:40:05Z","title":"A Situation-aware Enhancer for Personalized Recommendation","summary":"  When users interact with Recommender Systems (RecSys), current situations,\nsuch as time, location, and environment, significantly influence their\npreferences. Situations serve as the background for interactions, where\nrelationships between users and items evolve with situation changes. However,\nexisting RecSys treat situations, users, and items on the same level. They can\nonly model the relations between situations and users/items respectively,\nrather than the dynamic impact of situations on user-item associations (i.e.,\nuser preferences). In this paper, we provide a new perspective that takes\nsituations as the preconditions for users' interactions. This perspective\nallows us to separate situations from user/item representations, and capture\nsituations' influences over the user-item relationship, offering a more\ncomprehensive understanding of situations. Based on it, we propose a novel\nSituation-Aware Recommender Enhancer (SARE), a pluggable module to integrate\nsituations into various existing RecSys. Since users' perception of situations\nand situations' impact on preferences are both personalized, SARE includes a\nPersonalized Situation Fusion (PSF) and a User-Conditioned Preference Encoder\n(UCPE) to model the perception and impact of situations, respectively. We\nconduct experiments of applying SARE on seven backbones in various settings on\ntwo real-world datasets. Experimental results indicate that SARE improves the\nrecommendation performances significantly compared with backbones and SOTA\nsituation-aware baselines.\n","authors":["Jiayu Li","Peijie Sun","Chumeng Jiang","Weizhi Ma","Qingyao Ai","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18317v1.pdf","comment":"Accepted at the International Conference on Database Systems for\n  Advanced Applications (DASFAA 2024)"},{"id":"http://arxiv.org/abs/2402.18355v2","updated":"2024-03-27T07:22:40Z","published":"2024-02-28T14:26:52Z","title":"COPR -- Efficient, large-scale log storage and retrieval","summary":"  Modern, large scale monitoring systems have to process and store vast amounts\nof log data in near real-time. At query time the systems have to find relevant\nlogs based on the content of the log message using support structures that can\nscale to these amounts of data while still being efficient to use. We present\nour novel Compressed Probabilistic Retrieval algorithm (COPR), capable of\nanswering Multi-Set Multi-Membership-Queries, that can be used as an\nalternative to existing indexing structures for streamed log data. In our\nexperiments, COPR required up to 93% less storage space than the tested\nstate-of-the-art inverted index and had up to four orders of magnitude less\nfalse-positives than the tested state-of-the-art membership sketch.\nAdditionally, COPR achieved up to 250 times higher query throughput than the\ntested inverted index and up to 240 times higher query throughput than the\ntested membership sketch.\n","authors":["Julian Reichinger","Thomas Krismayer","Jan Rellermeyer"],"pdf_url":"https://arxiv.org/pdf/2402.18355v2.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.18305v1","updated":"2024-03-27T06:59:39Z","published":"2024-03-27T06:59:39Z","title":"A Recommender System for NFT Collectibles with Item Feature","summary":"  Recommender systems have been actively studied and applied in various domains\nto deal with information overload. Although there are numerous studies on\nrecommender systems for movies, music, and e-commerce, comparatively less\nattention has been paid to the recommender system for NFTs despite the\ncontinuous growth of the NFT market. This paper presents a recommender system\nfor NFTs that utilizes a variety of data sources, from NFT transaction records\nto external item features, to generate precise recommendations that cater to\nindividual preferences. We develop a data-efficient graph-based recommender\nsystem to efficiently capture the complex relationship between each item and\nusers and generate node(item) embeddings which incorporate both node feature\ninformation and graph structure. Furthermore, we exploit inputs beyond\nuser-item interactions, such as image feature, text feature, and price feature.\nNumerical experiments verify the performance of the graph-based recommender\nsystem improves significantly after utilizing all types of item features as\nside information, thereby outperforming all other baselines.\n","authors":["Minjoo Choi","Seonmi Kim","Yejin Kim","Youngbin Lee","Joohwan Hong","Yongjae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.18305v1.pdf","comment":"Presented at the AAAI 2023 Bridge on AI for Financial Services\n  (https://sites.google.com/view/aaai-ai-fin/home)"},{"id":"http://arxiv.org/abs/2403.17421v2","updated":"2024-03-27T06:28:53Z","published":"2024-03-26T06:34:23Z","title":"MA4DIV: Multi-Agent Reinforcement Learning for Search Result\n  Diversification","summary":"  The objective of search result diversification (SRD) is to ensure that\nselected documents cover as many different subtopics as possible. Existing\nmethods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one\ndocument with the highest diversity score at a time. These approaches tend to\nbe inefficient and are easily trapped in a suboptimal state. In addition, some\nother methods aim to approximately optimize the diversity metric, such as\n$\\alpha$-NDCG, but the results still remain suboptimal. To address these\nchallenges, we introduce Multi-Agent reinforcement learning (MARL) for search\nresult DIVersity, which called MA4DIV. In this approach, each document is an\nagent and the search result diversification is modeled as a cooperative task\namong multiple agents. This approach allows for directly optimizing the\ndiversity metrics, such as $\\alpha$-NDCG, while achieving high training\nefficiency. We conducted preliminary experiments on public TREC datasets to\ndemonstrate the effectiveness and potential of MA4DIV. Considering the limited\nnumber of queries in public TREC datasets, we construct a large-scale dataset\nfrom industry sources and show that MA4DIV achieves substantial improvements in\nboth effectiveness and efficiency than existing baselines on a industrial scale\ndataset.\n","authors":["Yiqun Chen","Jiaxin Mao","Yi Zhang","Dehong Ma","Long Xia","Jun Fan","Daiting Shi","Zhicong Cheng","Simiu Gu","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18280v1","updated":"2024-03-27T06:16:14Z","published":"2024-03-27T06:16:14Z","title":"Improving Out-of-Vocabulary Handling in Recommendation Systems","summary":"  Recommendation systems (RS) are an increasingly relevant area for both\nacademic and industry researchers, given their widespread impact on the daily\nonline experiences of billions of users. One common issue in real RS is the\ncold-start problem, where users and items may not contain enough information to\nproduce high-quality recommendations. This work focuses on a complementary\nproblem: recommending new users and items unseen (out-of-vocabulary, or OOV) at\ntraining time. This setting is known as the inductive setting and is especially\nproblematic for factorization-based models, which rely on encoding only those\nusers/items seen at training time with fixed parameter vectors. Many existing\nsolutions applied in practice are often naive, such as assigning OOV\nusers/items to random buckets. In this work, we tackle this problem and propose\napproaches that better leverage available user/item features to improve OOV\nhandling at the embedding table level. We discuss general-purpose plug-and-play\napproaches that are easily applicable to most RS models and improve inductive\nperformance without negatively impacting transductive model performance. We\nextensively evaluate 9 OOV embedding methods on 5 models across 4 datasets\n(spanning different domains). One of these datasets is a proprietary production\ndataset from a prominent RS employed by a large social platform serving\nhundreds of millions of daily active users. In our experiments, we find that\nseveral proposed methods that exploit feature similarity using LSH consistently\noutperform alternatives on most model-dataset combinations, with the best\nmethod showing a mean improvement of 3.74% over the industry standard baseline\nin inductive performance. We release our code and hope our work helps\npractitioners make more informed decisions when handling OOV for their RS and\nfurther inspires academic research into improving OOV support in RS.\n","authors":["William Shiao","Mingxuan Ju","Zhichun Guo","Xin Chen","Evangelos Papalexakis","Tong Zhao","Neil Shah","Yozen Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18280v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.18276v1","updated":"2024-03-27T06:07:05Z","published":"2024-03-27T06:07:05Z","title":"RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers","summary":"  Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate\nreproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18227v1","updated":"2024-03-27T03:31:05Z","published":"2024-03-27T03:31:05Z","title":"One Backpropagation in Two Tower Recommendation Models","summary":"  Recent years have witnessed extensive researches on developing two tower\nrecommendation models for relieving information overload. Four building modules\ncan be identified in such models, namely, user-item encoding, negative\nsampling, loss computing and back-propagation updating. To the best of our\nknowledge, existing algorithms have researched only on the first three modules,\nyet neglecting the backpropagation module. They all adopt a kind of two\nbackpropagation strategy, which are based on an implicit assumption of equally\ntreating users and items in the training phase. In this paper, we challenge\nsuch an equal training assumption and propose a novel one backpropagation\nupdating strategy, which keeps the normal gradient backpropagation for the item\nencoding tower, but cuts off the backpropagation for the user encoding tower.\nInstead, we propose a moving-aggregation updating strategy to update a user\nencoding in each training epoch. Except the proposed backpropagation updating\nmodule, we implement the other three modules with the most straightforward\nchoices. Experiments on four public datasets validate the effectiveness and\nefficiency of our model in terms of improved recommendation performance and\nreduced computation overload over the state-of-the-art competitors.\n","authors":["Erjia Chen","Bang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18227v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18183v1","updated":"2024-03-27T01:21:48Z","published":"2024-03-27T01:21:48Z","title":"Can AI Models Appreciate Document Aesthetics? An Exploration of\n  Legibility and Layout Quality in Relation to Prediction Confidence","summary":"  A well-designed document communicates not only through its words but also\nthrough its visual eloquence. Authors utilize aesthetic elements such as\ncolors, fonts, graphics, and layouts to shape the perception of information.\nThoughtful document design, informed by psychological insights, enhances both\nthe visual appeal and the comprehension of the content. While state-of-the-art\ndocument AI models demonstrate the benefits of incorporating layout and image\ndata, it remains unclear whether the nuances of document aesthetics are\neffectively captured. To bridge the gap between human cognition and AI\ninterpretation of aesthetic elements, we formulated hypotheses concerning AI\nbehavior in document understanding tasks, specifically anchored in document\ndesign principles. With a focus on legibility and layout quality, we tested\nfour aspects of aesthetic effects: noise, font-size contrast, alignment, and\ncomplexity, on model confidence using correlational analysis. The results and\nobservations highlight the value of model analysis rooted in document design\ntheories. Our work serves as a trailhead for further studies and we advocate\nfor continued research in this topic to deepen our understanding of how AI\ninterprets document aesthetics.\n","authors":["Hsiu-Wei Yang","Abhinav Agrawal","Pavlos Fragkogiannis","Shubham Nitin Mulay"],"pdf_url":"https://arxiv.org/pdf/2403.18183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18173v1","updated":"2024-03-27T01:01:09Z","published":"2024-03-27T01:01:09Z","title":"LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval\n  and Responsible Research Practices","summary":"  Efficient and accurate information extraction from scientific papers is\nsignificant in the rapidly developing human-computer interaction research in\nthe literature review process. Our paper introduces and analyses a new\ninformation retrieval system using state-of-the-art Large Language Models\n(LLMs) in combination with structured text analysis techniques to extract\nexperimental data from HCI literature, emphasizing key elements. Then We\nanalyze the challenges and risks of using LLMs in the world of research. We\nperformed a comprehensive analysis on our conducted dataset, which contained\nthe specified information of 300 CHI 2020-2022 papers, to evaluate the\nperformance of the two large language models, GPT-3.5 (text-davinci-003) and\nLlama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model\ngains an accuracy of 58\\% and a mean absolute error of 7.00. In contrast, the\nLlama2 model indicates an accuracy of 56\\% with a mean absolute error of 7.63.\nThe ability to answer questions was also included in the system in order to\nwork with streamlined data. By evaluating the risks and opportunities presented\nby LLMs, our work contributes to the ongoing dialogue on establishing\nmethodological validity and ethical guidelines for LLM use in HCI data work.\n","authors":["Neda Taghizadeh Serajeh","Iman Mohammadi","Vittorio Fuccella","Mattia De Rosa"],"pdf_url":"https://arxiv.org/pdf/2403.18173v1.pdf","comment":"5 pages, CHI2024 Workshop on LLMs as Research Tools: Applications and\n  Evaluations in HCI Data Work"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13525v2","updated":"2024-03-27T17:47:56Z","published":"2023-05-22T22:41:49Z","title":"A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs","summary":"  Large communication costs are a critical bottleneck in training\nstate-of-the-art neural networks on distributed systems. This paper introduces\nAxoNN, a novel four-dimensional (4D) parallelization approach, inspired by\nAgarwal's algorithm for matrix multiplication, for parallelizing tensor\ncomputations in deep learning, AxoNN employs two key strategies to minimize\ncommunication overhead. First, we optimize communication by overlapping\nexpensive collective operations (reduce-scatter, all-gather, all-reduce) with\ncomputations. Our experiments with a 20-billion parameter transformer model\ndemonstrate that these optimizations deliver nearly 53\\% improvement. Second,\nwe present an analytical model to assist users in identifying\ncommunication-minimizing configurations within the vast search space defined by\nour 4D algorithm. This model empowers practitioners by simplifying the tuning\nprocess for their specific training workloads. When training an 80-billion\nparameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a\nstate-of-the-art framework, by a significant 26%. Additionally, it achieves 57%\nof the theoretical peak FLOP/s.\n","authors":["Siddharth Singh","Prajwal Singhania","Aditya K. Ranjan","Zack Sating","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2305.13525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13483v4","updated":"2024-03-27T17:38:27Z","published":"2023-02-27T02:42:27Z","title":"CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems","summary":"  We present CrystalBox, a novel, model-agnostic, posthoc explainability\nframework for Deep Reinforcement Learning (DRL) controllers in the large family\nof input-driven environments which includes computer systems. We combine the\nnatural decomposability of reward functions in input-driven environments with\nthe explanatory power of decomposed returns. We propose an efficient algorithm\nto generate future-based explanations across both discrete and continuous\ncontrol environments. Using applications such as adaptive bitrate streaming and\ncongestion control, we demonstrate CrystalBox's capability to generate\nhigh-fidelity explanations. We further illustrate its higher utility across\nthree practical use cases: contrastive explanations, network observability, and\nguided reward design, as opposed to prior explainability techniques that\nidentify salient features.\n","authors":["Sagar Patel","Sangeetha Abdu Jyothi","Nina Narodytska"],"pdf_url":"https://arxiv.org/pdf/2302.13483v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2309.04381v2","updated":"2024-03-27T17:07:47Z","published":"2023-09-08T15:23:40Z","title":"Generalization Bounds: Perspectives from Information Theory and\n  PAC-Bayes","summary":"  A fundamental question in theoretical machine learning is generalization.\nOver the past decades, the PAC-Bayesian approach has been established as a\nflexible framework to address the generalization capabilities of machine\nlearning algorithms, and design new ones. Recently, it has garnered increased\ninterest due to its potential applicability for a variety of learning\nalgorithms, including deep neural networks. In parallel, an\ninformation-theoretic view of generalization has developed, wherein the\nrelation between generalization and various information measures has been\nestablished. This framework is intimately connected to the PAC-Bayesian\napproach, and a number of results have been independently discovered in both\nstrands. In this monograph, we highlight this strong connection and present a\nunified treatment of PAC-Bayesian and information-theoretic generalization\nbounds. We present techniques and results that the two perspectives have in\ncommon, and discuss the approaches and interpretations that differ. In\nparticular, we demonstrate how many proofs in the area share a modular\nstructure, through which the underlying ideas can be intuited. We pay special\nattention to the conditional mutual information (CMI) framework; analytical\nstudies of the information complexity of learning algorithms; and the\napplication of the proposed methods to deep learning. This monograph is\nintended to provide a comprehensive introduction to information-theoretic\ngeneralization bounds and their connection to PAC-Bayes, serving as a\nfoundation from which the most recent developments are accessible. It is aimed\nbroadly towards researchers with an interest in generalization and theoretical\nmachine learning.\n","authors":["Fredrik Hellström","Giuseppe Durisi","Benjamin Guedj","Maxim Raginsky"],"pdf_url":"https://arxiv.org/pdf/2309.04381v2.pdf","comment":"228 pages"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18766v1","updated":"2024-03-27T17:05:03Z","published":"2024-03-27T17:05:03Z","title":"Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means","summary":"  This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.\n","authors":["Rustam Mussabayev","Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2403.18766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18765v1","updated":"2024-03-27T17:03:31Z","published":"2024-03-27T17:03:31Z","title":"CaT: Constraints as Terminations for Legged Locomotion Reinforcement\n  Learning","summary":"  Deep Reinforcement Learning (RL) has demonstrated impressive results in\nsolving complex robotic tasks such as quadruped locomotion. Yet, current\nsolvers fail to produce efficient policies respecting hard constraints. In this\nwork, we advocate for integrating constraints into robot learning and present\nConstraints as Terminations (CaT), a novel constrained RL algorithm. Departing\nfrom classical constrained RL formulations, we reformulate constraints through\nstochastic terminations during policy learning: any violation of a constraint\ntriggers a probability of terminating potential future rewards the RL agent\ncould attain. We propose an algorithmic approach to this formulation, by\nminimally modifying widely used off-the-shelf RL algorithms in robot learning\n(such as Proximal Policy Optimization). Our approach leads to excellent\nconstraint adherence without introducing undue complexity and computational\noverhead, thus mitigating barriers to broader adoption. Through empirical\nevaluation on the real quadruped robot Solo crossing challenging obstacles, we\ndemonstrate that CaT provides a compelling solution for incorporating\nconstraints into RL frameworks. Videos and code are available at\nhttps://constraints-as-terminations.github.io.\n","authors":["Elliot Chane-Sane","Pierre-Alexandre Leziart","Thomas Flayols","Olivier Stasse","Philippe Souères","Nicolas Mansard"],"pdf_url":"https://arxiv.org/pdf/2403.18765v1.pdf","comment":"Project webpage: https://constraints-as-terminations.github.io"},{"id":"http://arxiv.org/abs/2311.01483v3","updated":"2024-03-27T16:56:23Z","published":"2023-11-02T14:47:06Z","title":"FedSN: A Novel Federated Learning Framework over LEO Satellite Networks","summary":"  Recently, a large number of Low Earth Orbit (LEO) satellites have been\nlaunched and deployed successfully in space by commercial companies, such as\nSpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve\nnot only for communication but also for various machine learning applications,\nsuch as space modulation recognition, remote sensing image classification, etc.\nHowever, the ground station (GS) may be incapable of downloading such a large\nvolume of raw sensing data for centralized model training due to the limited\ncontact time with LEO satellites (e.g. 5 minutes). Therefore, federated\nlearning (FL) has emerged as the promising solution to address this problem via\non-device training. Unfortunately, to enable FL on LEO satellites, we still\nface three critical challenges that are i) heterogeneous computing and memory\ncapabilities, ii) limited uplink rate, and iii) model staleness. To this end,\nwe propose FedSN as a general FL framework to tackle the above challenges, and\nfully explore data diversity on LEO satellites. Specifically, we first present\na novel sub-structure scheme to enable heterogeneous local model training\nconsidering different computing, memory, and communication constraints on LEO\nsatellites. Additionally, we propose a pseudo-synchronous model aggregation\nstrategy to dynamically schedule model aggregation for compensating model\nstaleness. To further demonstrate the effectiveness of the FedSN, we evaluate\nit using space modulation recognition and remote sensing image classification\ntasks by leveraging the data from real-world satellite networks. Extensive\nexperimental results demonstrate that FedSN framework achieves higher accuracy,\nlower computing, and communication overhead than the state-of-the-art\nbenchmarks and the effectiveness of each components in FedSN.\n","authors":["Zheng Lin","Zhe Chen","Zihan Fang","Xianhao Chen","Xiong Wang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2311.01483v3.pdf","comment":"14 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2403.14623v2","updated":"2024-03-27T16:49:35Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/checkcrab/SDSB.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03683v2","updated":"2024-03-27T16:44:22Z","published":"2023-11-07T03:19:16Z","title":"Preventing Arbitrarily High Confidence on Far-Away Data in\n  Point-Estimated Discriminative Neural Networks","summary":"  Discriminatively trained, deterministic neural networks are the de facto\nchoice for classification problems. However, even though they achieve\nstate-of-the-art results on in-domain test sets, they tend to be overconfident\non out-of-distribution (OOD) data. For instance, ReLU networks - a popular\nclass of neural network architectures - have been shown to almost always yield\nhigh confidence predictions when the test data are far away from the training\nset, even when they are trained with OOD data. We overcome this problem by\nadding a term to the output of the neural network that corresponds to the logit\nof an extra class, that we design to dominate the logits of the original\nclasses as we move away from the training data.This technique provably prevents\narbitrarily high confidence on far-away test data while maintaining a simple\ndiscriminative point-estimate training. Evaluation on various benchmarks\ndemonstrates strong performance against competitive baselines on both far-away\nand realistic OOD data.\n","authors":["Ahmad Rashid","Serena Hacker","Guojun Zhang","Agustinus Kristiadi","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2311.03683v2.pdf","comment":"Accepted at AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.18742v1","updated":"2024-03-27T16:39:28Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18739v1","updated":"2024-03-27T16:32:32Z","published":"2024-03-27T16:32:32Z","title":"Usage-Specific Survival Modeling Based on Operational Data and Neural\n  Networks","summary":"  Accurate predictions of when a component will fail are crucial when planning\nmaintenance, and by modeling the distribution of these failure times, survival\nmodels have shown to be particularly useful in this context. The presented\nmethodology is based on conventional neural network-based survival models that\nare trained using data that is continuously gathered and stored at specific\ntimes, called snapshots. An important property of this type of training data is\nthat it can contain more than one snapshot from a specific individual which\nresults in that standard maximum likelihood training can not be directly\napplied since the data is not independent. However, the papers show that if the\ndata is in a specific format where all snapshot times are the same for all\nindividuals, called homogeneously sampled, maximum likelihood training can be\napplied and produce desirable results. In many cases, the data is not\nhomogeneously sampled and in this case, it is proposed to resample the data to\nmake it homogeneously sampled. How densely the dataset is sampled turns out to\nbe an important parameter; it should be chosen large enough to produce good\nresults, but this also increases the size of the dataset which makes training\nslow. To reduce the number of samples needed during training, the paper also\nproposes a technique to, instead of resampling the dataset once before the\ntraining starts, randomly resample the dataset at the start of each epoch\nduring the training. The proposed methodology is evaluated on both a simulated\ndataset and an experimental dataset of starter battery failures. The results\nshow that if the data is homogeneously sampled the methodology works as\nintended and produces accurate survival models. The results also show that\nrandomly resampling the dataset on each epoch is an effective way to reduce the\nsize of the training data.\n","authors":["Olov Holmer","Mattias Krysander","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.18739v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.18735v1","updated":"2024-03-27T16:24:26Z","published":"2024-03-27T16:24:26Z","title":"Nonlinear model reduction for operator learning","summary":"  Operator learning provides methods to approximate mappings between\ninfinite-dimensional function spaces. Deep operator networks (DeepONets) are a\nnotable architecture in this field. Recently, an extension of DeepONet based on\nmodel reduction and neural networks, proper orthogonal decomposition\n(POD)-DeepONet, has been able to outperform other architectures in terms of\naccuracy for several benchmark tests. We extend this idea towards nonlinear\nmodel order reduction by proposing an efficient framework that combines neural\nnetworks with kernel principal component analysis (KPCA) for operator learning.\nOur results demonstrate the superior performance of KPCA-DeepONet over\nPOD-DeepONet.\n","authors":["Hamidreza Eivazi","Stefan Wittek","Andreas Rausch"],"pdf_url":"https://arxiv.org/pdf/2403.18735v1.pdf","comment":"Published as a Tiny Paper at ICLR 2024 (Notable)"},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2402.07868v2","updated":"2024-03-27T16:12:43Z","published":"2024-02-12T18:29:17Z","title":"Nesting Particle Filters for Experimental Design in Dynamical Systems","summary":"  In this paper, we propose a novel approach to Bayesian experimental design\nfor non-exchangeable data that formulates it as risk-sensitive policy\noptimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential\nMonte Carlo technique to infer optimal designs, and embed it into a particle\nMarkov chain Monte Carlo framework to perform gradient-based policy\namortization. Our approach is distinct from other amortized experimental design\ntechniques, as it does not rely on contrastive estimators. Numerical validation\non a set of dynamical systems showcases the efficacy of our method in\ncomparison to other state-of-the-art strategies.\n","authors":["Sahel Iqbal","Adrien Corenflos","Simo Särkkä","Hany Abdulsamad"],"pdf_url":"https://arxiv.org/pdf/2402.07868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11798v3","updated":"2024-03-27T16:12:18Z","published":"2023-09-21T06:04:06Z","title":"A Comprehensive Review of Community Detection in Graphs","summary":"  The study of complex networks has significantly advanced our understanding of\ncommunity structures which serves as a crucial feature of real-world graphs.\nDetecting communities in graphs is a challenging problem with applications in\nsociology, biology, and computer science. Despite the efforts of an\ninterdisciplinary community of scientists, a satisfactory solution to this\nproblem has not yet been achieved. This review article delves into the topic of\ncommunity detection in graphs, which serves as a thorough exposition of various\ncommunity detection methods from perspectives of modularity-based method,\nspectral clustering, probabilistic modelling, and deep learning. Along with the\nmethods, a new community detection method designed by us is also presented.\nAdditionally, the performance of these methods on the datasets with and without\nground truth is compared. In conclusion, this comprehensive review provides a\ndeep understanding of community detection in graphs.\n","authors":["Jiakang Li","Songning Lai","Zhihao Shuai","Yuan Tan","Yifan Jia","Mianyang Yu","Zichen Song","Xiaokang Peng","Ziyang Xu","Yongxin Ni","Haifeng Qiu","Jiayu Yang","Yutong Liu","Yonggang Lu"],"pdf_url":"https://arxiv.org/pdf/2309.11798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07494v3","updated":"2024-03-27T16:06:34Z","published":"2024-01-15T06:26:53Z","title":"Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering\n  Tasks","summary":"  Computational efficiency and non-adversarial robustness are critical factors\nin real-world engineering applications. Yet, conventional neural networks often\nfall short in addressing both simultaneously, or even separately. Drawing\ninsights from natural physical systems and existing literature, it is known\nthat an input convex architecture enhances computational efficiency, while a\nLipschitz-constrained architecture bolsters non-adversarial robustness. By\nleveraging the strengths of convexity and Lipschitz continuity, we develop a\nnovel network architecture, termed Input Convex Lipschitz Recurrent Neural\nNetworks. This model is explicitly designed for fast and robust\noptimization-based tasks and outperforms existing recurrent units across a\nspectrum of engineering tasks in terms of computational efficiency and\nnon-adversarial robustness, including real-world solar irradiance prediction\nfor Solar PV system planning at LHT Holdings in Singapore and real-time Model\nPredictive Control optimization for a nonlinear chemical reactor.\n","authors":["Zihao Wang","P S Pravin","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2401.07494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.17878v2","updated":"2024-03-27T16:01:00Z","published":"2024-03-26T17:10:15Z","title":"Empowering Data Mesh with Federated Learning","summary":"  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n","authors":["Haoyuan Li","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2403.17878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18710v1","updated":"2024-03-27T15:57:42Z","published":"2024-03-27T15:57:42Z","title":"Deep Learning for Traffic Flow Prediction using Cellular Automata-based\n  Model and CNN-LSTM architecture","summary":"  Recent works have attempted to use deep learning to predict future states of\ntraffic flow, but have met with mixed results. These approaches face two key\nchallenges. First, training deep learning neural networks requires large\namounts of training data which are not yet easily available for traffic flow\nsystems. Second, even when data is available, the neural networks require\naccess to historical data that covers most possible traffic flow dynamics to\nsuccessfully predict future traffic states. Specifically, these deep learning\napproaches do not fully leverage domain-knowledge about traffic flow dynamics,\ndespite a significant existing knowledge-base. In this work, we propose to\nsolve both issues using a Convolutional Neural Network (CNNs) with Long Short\nTerm Memory (LSTM) deep learning architecture to successfully predict traffic\nflow, while leveraging a cellular automata-based statistical mechanics model of\ntraffic flow to generate training and test data. Another major contribution of\nthis paper is the insight that training data for a large traffic system can\nactually be sampled from the simulations of a much smaller traffic system. This\nis achieved through observing that the normalized energy distribution of the\nstatistical mechanics model is scale invariant, which significantly eases the\nburden of data generation for large scale traffic systems. The resulting\nsimulations indicate good agreement between the predicted and the true traffic\nflow dynamics.\n","authors":["Zhaohui Yang","Kshitij Jerath"],"pdf_url":"https://arxiv.org/pdf/2403.18710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18705v1","updated":"2024-03-27T15:54:55Z","published":"2024-03-27T15:54:55Z","title":"Conditional Wasserstein Distances with Applications in Bayesian OT Flow\n  Matching","summary":"  In inverse problems, many conditional generative models approximate the\nposterior measure by minimizing a distance between the joint measure and its\nlearned approximation. While this approach also controls the distance between\nthe posterior measures in the case of the Kullback--Leibler divergence, this is\nin general not hold true for the Wasserstein distance. In this paper, we\nintroduce a conditional Wasserstein distance via a set of restricted couplings\nthat equals the expected Wasserstein distance of the posteriors. Interestingly,\nthe dual formulation of the conditional Wasserstein-1 flow resembles losses in\nthe conditional Wasserstein GAN literature in a quite natural way. We derive\ntheoretical properties of the conditional Wasserstein distance, characterize\nthe corresponding geodesics and velocity fields as well as the flow ODEs.\nSubsequently, we propose to approximate the velocity fields by relaxing the\nconditional Wasserstein distance. Based on this, we propose an extension of OT\nFlow Matching for solving Bayesian inverse problems and demonstrate its\nnumerical advantages on an inverse problem and class-conditional image\ngeneration.\n","authors":["Jannis Chemseddine","Paul Hagemann","Christian Wald","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2403.18705v1.pdf","comment":"This paper supersedes arXiv:2310.13433"},{"id":"http://arxiv.org/abs/2403.18703v1","updated":"2024-03-27T15:52:54Z","published":"2024-03-27T15:52:54Z","title":"Fpga-Based Neural Thrust Controller for UAVs","summary":"  The advent of unmanned aerial vehicles (UAVs) has improved a variety of\nfields by providing a versatile, cost-effective and accessible platform for\nimplementing state-of-the-art algorithms. To accomplish a broader range of\ntasks, there is a growing need for enhanced on-board computing to cope with\nincreasing complexity and dynamic environmental conditions. Recent advances\nhave seen the application of Deep Neural Networks (DNNs), particularly in\ncombination with Reinforcement Learning (RL), to improve the adaptability and\nperformance of UAVs, especially in unknown environments. However, the\ncomputational requirements of DNNs pose a challenge to the limited computing\nresources available on many UAVs. This work explores the use of Field\nProgrammable Gate Arrays (FPGAs) as a viable solution to this challenge,\noffering flexibility, high performance, energy and time efficiency. We propose\na novel hardware board equipped with an Artix-7 FPGA for a popular open-source\nmicro-UAV platform. We successfully validate its functionality by implementing\nan RL-based low-level controller using real-world experiments.\n","authors":["Sharif Azem","David Scheunert","Mengguang Li","Jonas Gehrunger","Kai Cui","Christian Hochberger","Heinz Koepp"],"pdf_url":"https://arxiv.org/pdf/2403.18703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11800v3","updated":"2024-03-27T15:48:29Z","published":"2024-02-19T03:08:02Z","title":"Stochastic Approximation with Delayed Updates: Finite-Time Rates under\n  Markovian Sampling","summary":"  Motivated by applications in large-scale and multi-agent reinforcement\nlearning, we study the non-asymptotic performance of stochastic approximation\n(SA) schemes with delayed updates under Markovian sampling. While the effect of\ndelays has been extensively studied for optimization, the manner in which they\ninteract with the underlying Markov process to shape the finite-time\nperformance of SA remains poorly understood. In this context, our first main\ncontribution is to show that under time-varying bounded delays, the delayed SA\nupdate rule guarantees exponentially fast convergence of the \\emph{last\niterate} to a ball around the SA operator's fixed point. Notably, our bound is\n\\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the\nmixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel\ninductive proof technique that, unlike various existing delayed-optimization\nanalyses, relies on establishing uniform boundedness of the iterates. As such,\nour proof may be of independent interest. Next, to mitigate the impact of the\nmaximum delay on the convergence rate, we provide the first finite-time\nanalysis of a delay-adaptive SA scheme under Markovian sampling. In particular,\nwe show that the exponent of convergence of this scheme gets scaled down by\n$\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here,\n$\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the\nadaptive scheme requires no prior knowledge of the delay sequence for step-size\ntuning. Our theoretical findings shed light on the finite-time effects of\ndelays for a broad class of algorithms, including TD learning, Q-learning, and\nstochastic gradient descent under Markovian sampling.\n","authors":["Arman Adibi","Nicolo Dal Fabbro","Luca Schenato","Sanjeev Kulkarni","H. Vincent Poor","George J. Pappas","Hamed Hassani","Aritra Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.11800v3.pdf","comment":"Accepted to the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024!"},{"id":"http://arxiv.org/abs/2403.18699v1","updated":"2024-03-27T15:48:16Z","published":"2024-03-27T15:48:16Z","title":"Contrastive Learning with Orthonormal Anchors (CLOA)","summary":"  This study focuses on addressing the instability issues prevalent in\ncontrastive learning, specifically examining the InfoNCE loss function and its\nderivatives. We reveal a critical observation that these loss functions exhibit\na restrictive behavior, leading to a convergence phenomenon where embeddings\ntend to merge into a singular point. This \"over-fusion\" effect detrimentally\naffects classification accuracy in subsequent supervised-learning tasks.\nThrough theoretical analysis, we demonstrate that embeddings, when equalized or\nconfined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In\nresponse to this challenge, our research introduces an innovative strategy that\nleverages the same or fewer labeled data than typically used in the fine-tuning\nphase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to\ndisentangle embedding clusters, significantly enhancing the distinctiveness of\neach embedding while simultaneously ensuring their aggregation into dense,\nwell-defined clusters. Our method demonstrates remarkable improvements with\njust a fraction of the conventional label requirements, as evidenced by our\nresults on CIFAR10 and CIFAR100 datasets.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18699v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18687v1","updated":"2024-03-27T15:34:27Z","published":"2024-03-27T15:34:27Z","title":"InceptionTime vs. Wavelet -- A comparison for time series classification","summary":"  Neural networks were used to classify infrasound data. Two different\napproaches were compared. One based on the direct classification of time series\ndata, using a custom implementation of the InceptionTime network. For the other\napproach, we generated 2D images of the wavelet transformation of the signals,\nwhich were subsequently classified using a ResNet implementation. Choosing\nappropriate hyperparameter settings, both achieve a classification accuracy of\nabove 90 %, with the direct approach reaching 95.2 %.\n","authors":["Daniel Klenkert","Daniel Schaeffer","Julian Stauch"],"pdf_url":"https://arxiv.org/pdf/2403.18687v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.18685v1","updated":"2024-03-27T15:29:08Z","published":"2024-03-27T15:29:08Z","title":"Representatividad Muestral en la Incertidumbre Simétrica Multivariada\n  para la Selección de Atributos","summary":"  In this work, we analyze the behavior of the multivariate symmetric\nuncertainty (MSU) measure through the use of statistical simulation techniques\nunder various mixes of informative and non-informative randomly generated\nfeatures. Experiments show how the number of attributes, their cardinalities,\nand the sample size affect the MSU. In this thesis, through observation of\nresults, it is proposed an heuristic condition that preserves good quality in\nthe MSU under different combinations of these three factors, providing a new\nuseful criterion to help drive the process of dimension reduction.\n  --\n  En el presente trabajo hemos analizado el comportamiento de una versi\\'on\nmultivariada de la incertidumbre sim\\'etrica a trav\\'es de t\\'ecnicas de\nsimulaci\\'on estad\\'isticas sobre varias combinaciones de atributos\ninformativos y no-informativos generados de forma aleatoria. Los experimentos\nmuestran como el n\\'umero de atributos, sus cardinalidades y el tama\\~no\nmuestral afectan al MSU como medida. En esta tesis, mediante la observaci\\'on\nde resultados hemos propuesto una condici\\'on que preserva una buena calidad en\nel MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual\nprovee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\\'on\nde dimensionalidad.\n","authors":["Gustavo Sosa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2403.18685v1.pdf","comment":"52 pages, in Spanish. Advisors: Miguel Garc\\'ia-Torres, Santiago\n  G\\'omez-Guerrero, Christian E. Schaerer Serra"},{"id":"http://arxiv.org/abs/2403.18681v1","updated":"2024-03-27T15:24:54Z","published":"2024-03-27T15:24:54Z","title":"TransFusion: Contrastive Learning with Transformers","summary":"  This paper proposes a novel framework, TransFusion, designed to make the\nprocess of contrastive learning more analytical and explainable. TransFusion\nconsists of attention blocks whose softmax being replaced by ReLU, and its\nfinal block's weighted-sum operation is truncated to leave the adjacency matrix\nas the output. The model is trained by minimizing the Jensen-Shannon Divergence\nbetween its output and the target affinity matrix, which indicates whether each\npair of samples belongs to the same or different classes. The main contribution\nof TransFusion lies in defining a theoretical limit for answering two\nfundamental questions in the field: the maximum level of data augmentation and\nthe minimum batch size required for effective contrastive learning.\nFurthermore, experimental results indicate that TransFusion successfully\nextracts features that isolate clusters from complex real-world data, leading\nto improved classification accuracy in downstream tasks.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18681v1.pdf","comment":"17 pages, 4 figures,"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18668v1","updated":"2024-03-27T15:11:07Z","published":"2024-03-27T15:11:07Z","title":"Aiming for Relevance","summary":"  Vital signs are crucial in intensive care units (ICUs). They are used to\ntrack the patient's state and to identify clinically significant changes.\nPredicting vital sign trajectories is valuable for early detection of adverse\nevents. However, conventional machine learning metrics like RMSE often fail to\ncapture the true clinical relevance of such predictions. We introduce novel\nvital sign prediction performance metrics that align with clinical contexts,\nfocusing on deviations from clinical norms, overall trends, and trend\ndeviations. These metrics are derived from empirical utility curves obtained in\na previous study through interviews with ICU clinicians. We validate the\nmetrics' usefulness using simulated and real clinical datasets (MIMIC and\neICU). Furthermore, we employ these metrics as loss functions for neural\nnetworks, resulting in models that excel in predicting clinically significant\nevents. This research paves the way for clinically relevant machine learning\nmodel evaluation and optimization, promising to improve ICU patient care. 10\npages, 9 figures.\n","authors":["Bar Eini Porat","Danny Eytan","Uri Shalit"],"pdf_url":"https://arxiv.org/pdf/2403.18668v1.pdf","comment":"10 pages, 9 figures, AMIA Informatics 2024"},{"id":"http://arxiv.org/abs/2403.18664v1","updated":"2024-03-27T15:08:00Z","published":"2024-03-27T15:08:00Z","title":"Neural Network-Based Piecewise Survival Models","summary":"  In this paper, a family of neural network-based survival models is presented.\nThe models are specified based on piecewise definitions of the hazard function\nand the density function on a partitioning of the time; both constant and\nlinear piecewise definitions are presented, resulting in a family of four\nmodels. The models can be seen as an extension of the commonly used\ndiscrete-time and piecewise exponential models and thereby add flexibility to\nthis set of standard models. Using a simulated dataset the models are shown to\nperform well compared to the highly expressive, state-of-the-art energy-based\nmodel, while only requiring a fraction of the computation time.\n","authors":["Olov Holmer","Erik Frisk","Mattias Krysander"],"pdf_url":"https://arxiv.org/pdf/2403.18664v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.13374v3","updated":"2024-03-27T14:57:54Z","published":"2024-03-20T08:15:08Z","title":"Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity","summary":"  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n","authors":["Shiyuan Zuo","Xingrun Yan","Rongfei Fan","Han Hu","Hangguan Shan","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.13374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17251v2","updated":"2024-03-27T14:48:48Z","published":"2023-03-30T09:29:53Z","title":"Demystifying Misconceptions in Social Bots Research","summary":"  Research on social bots aims at advancing knowledge and providing solutions\nto one of the most debated forms of online manipulation. Yet, social bot\nresearch is plagued by widespread biases, hyped results, and misconceptions\nthat set the stage for ambiguities, unrealistic expectations, and seemingly\nirreconcilable findings. Overcoming such issues is instrumental towards\nensuring reliable solutions and reaffirming the validity of the scientific\nmethod. In this contribution, we review some recent results in social bots\nresearch, highlighting and revising factual errors as well as methodological\nand conceptual biases. More importantly, we demystify common misconceptions,\naddressing fundamental points on how social bots research is discussed. Our\nanalysis surfaces the need to discuss research about online disinformation and\nmanipulation in a rigorous, unbiased, and responsible way. This article\nbolsters such effort by identifying and refuting common fallacious arguments\nused by both proponents and opponents of social bots research, as well as\nproviding directions toward sound methodologies for future research in the\nfield.\n","authors":["Stefano Cresci","Kai-Cheng Yang","Angelo Spognardi","Roberto Di Pietro","Filippo Menczer","Marinella Petrocchi"],"pdf_url":"https://arxiv.org/pdf/2303.17251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12882v2","updated":"2024-03-27T14:47:41Z","published":"2023-08-23T17:42:00Z","title":"LCANets++: Robust Audio Classification using Multi-layer Neural Networks\n  with Lateral Competition","summary":"  Audio classification aims at recognizing audio signals, including speech\ncommands or sound events. However, current audio classifiers are susceptible to\nperturbations and adversarial attacks. In addition, real-world audio\nclassification tasks often suffer from limited labeled data. To help bridge\nthese gaps, previous work developed neuro-inspired convolutional neural\nnetworks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)\nin the first layer (i.e., LCANets) for computer vision. LCANets learn in a\ncombination of supervised and unsupervised learning, reducing dependency on\nlabeled samples. Motivated by the fact that auditory cortex is also sparse, we\nextend LCANets to audio recognition tasks and introduce LCANets++, which are\nCNNs that perform sparse coding in multiple layers via LCA. We demonstrate that\nLCANets++ are more robust than standard CNNs and LCANets against perturbations,\ne.g., background noise, as well as black-box and white-box attacks, e.g.,\nevasion and fast gradient sign (FGSM) attacks.\n","authors":["Sayanton V. Dibbo","Juston S. Moore","Garrett T. Kenyon","Michael A. Teti"],"pdf_url":"https://arxiv.org/pdf/2308.12882v2.pdf","comment":"Accepted at 2024 IEEE International Conference on Acoustics, Speech\n  and Signal Processing Workshops (ICASSPW)"},{"id":"http://arxiv.org/abs/2403.18637v1","updated":"2024-03-27T14:42:08Z","published":"2024-03-27T14:42:08Z","title":"Transformers-based architectures for stroke segmentation: A review","summary":"  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n","authors":["Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.18637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18635v1","updated":"2024-03-27T14:40:25Z","published":"2024-03-27T14:40:25Z","title":"Fusion approaches for emotion recognition from speech using acoustic and\n  text-based features","summary":"  In this paper, we study different approaches for classifying emotions from\nspeech using acoustic and text-based features. We propose to obtain\ncontextualized word embeddings with BERT to represent the information contained\nin speech transcriptions and show that this results in better performance than\nusing Glove embeddings. We also propose and compare different strategies to\ncombine the audio and text modalities, evaluating them on IEMOCAP and\nMSP-PODCAST datasets. We find that fusing acoustic and text-based systems is\nbeneficial on both datasets, though only subtle differences are observed across\nthe evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect\nthat the criteria used to define the cross-validation folds have on results. In\nparticular, the standard way of creating folds for this dataset results in a\nhighly optimistic estimation of performance for the text-based system,\nsuggesting that some previous works may overestimate the advantage of\nincorporating transcriptions.\n","authors":["Leonardo Pepino","Pablo Riera","Luciana Ferrer","Agustin Gravano"],"pdf_url":"https://arxiv.org/pdf/2403.18635v1.pdf","comment":"5 pages. Accepted in ICASSP 2020"},{"id":"http://arxiv.org/abs/2403.18631v1","updated":"2024-03-27T14:38:02Z","published":"2024-03-27T14:38:02Z","title":"First Experiences with the Identification of People at Risk for Diabetes\n  in Argentina using Machine Learning Techniques","summary":"  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for\nmedicine due to the absence of pathogenic symptoms and the lack of known\nassociated risk factors. Even though some proposals for machine learning models\nenable the identification of people at risk, the nature of the condition makes\nit so that a model suitable for one population may not necessarily be suitable\nfor another. In this article, the development and assessment of predictive\nmodels to identify people at risk for T2D and PD specifically in Argentina are\ndiscussed. First, the database was thoroughly preprocessed and three specific\ndatasets were generated considering a compromise between the number of records\nand the amount of available variables. After applying 5 different\nclassification models, the results obtained show that a very good performance\nwas observed for two datasets with some of these models. In particular, RF, DT,\nand ANN demonstrated great classification power, with good values for the\nmetrics under consideration. Given the lack of this type of tool in Argentina,\nthis work represents the first step towards the development of more\nsophisticated models.\n","authors":["Enzo Rucci","Gonzalo Tittarelli","Franco Ronchetti","Jorge F. Elgart","Laura Lanzarini","Juan José Gagliardino"],"pdf_url":"https://arxiv.org/pdf/2403.18631v1.pdf","comment":"Accepted for publication in Computer Science - CACIC 2023"},{"id":"http://arxiv.org/abs/2403.16451v3","updated":"2024-03-27T14:36:21Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18613v1","updated":"2024-03-27T14:28:44Z","published":"2024-03-27T14:28:44Z","title":"Scalable Lipschitz Estimation for CNNs","summary":"  Estimating the Lipschitz constant of deep neural networks is of growing\ninterest as it is useful for informing on generalisability and adversarial\nrobustness. Convolutional neural networks (CNNs) in particular, underpin much\nof the recent success in computer vision related applications. However,\nalthough existing methods for estimating the Lipschitz constant can be tight,\nthey have limited scalability when applied to CNNs. To tackle this, we propose\na novel method to accelerate Lipschitz constant estimation for CNNs. The core\nidea is to divide a large convolutional block via a joint layer and width-wise\npartition, into a collection of smaller blocks. We prove an upper-bound on the\nLipschitz constant of the larger block in terms of the Lipschitz constants of\nthe smaller blocks. Through varying the partition factor, the resulting method\ncan be adjusted to prioritise either accuracy or scalability and permits\nparallelisation. We demonstrate an enhanced scalability and comparable accuracy\nto existing baselines through a range of experiments.\n","authors":["Yusuf Sulehman","Tingting Mu"],"pdf_url":"https://arxiv.org/pdf/2403.18613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18597v1","updated":"2024-03-27T14:20:11Z","published":"2024-03-27T14:20:11Z","title":"Heterogeneous Peridynamic Neural Operators: Discover Biotissue\n  Constitutive Law and Microstructure From Digital Image Correlation\n  Measurements","summary":"  Human tissues are highly organized structures with specific collagen fiber\narrangements varying from point to point. The effects of such heterogeneity\nplay an important role for tissue function, and hence it is of critical to\ndiscover and understand the distribution of such fiber orientations from\nexperimental measurements, such as the digital image correlation data. To this\nend, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)\napproach, for data-driven constitutive modeling of heterogeneous anisotropic\nmaterials. The goal is to learn both a nonlocal constitutive law together with\nthe material microstructure, in the form of a heterogeneous fiber orientation\nfield, from loading field-displacement field measurements. To this end, we\npropose a two-phase learning approach. Firstly, we learn a homogeneous\nconstitutive law in the form of a neural network-based kernel function and a\nnonlocal bond force, to capture complex homogeneous material responses from\ndata. Then, in the second phase we reinitialize the learnt bond force and the\nkernel function, and training them together with a fiber orientation field for\neach material point. Owing to the state-based peridynamic skeleton, our\nHeteroPNO-learned material models are objective and have the balance of linear\nand angular momentum guaranteed. Moreover, the effects from heterogeneity and\nnonlinear constitutive relationship are captured by the kernel function and the\nbond force respectively, enabling physical interpretability. As a result, our\nHeteroPNO architecture can learn a constitutive model for a biological tissue\nwith anisotropic heterogeneous response undergoing large deformation regime.\nMoreover, the framework is capable to provide displacement and stress field\npredictions for new and unseen loading instances.\n","authors":["Siavash Jafarzadeh","Stewart Silling","Lu Zhang","Colton Ross","Chung-Hao Lee","S. M. Rakibur Rahman","Shuodao Wang","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18587v1","updated":"2024-03-27T14:11:23Z","published":"2024-03-27T14:11:23Z","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency\n  Attacks in Computer Vision","summary":"  Resource efficiency plays an important role for machine learning nowadays.\nThe energy and decision latency are two critical aspects to ensure a\nsustainable and practical application. Unfortunately, the energy consumption\nand decision latency are not robust against adversaries. Researchers have\nrecently demonstrated that attackers can compute and submit so-called sponge\nexamples at inference time to increase the energy consumption and decision\nlatency of neural networks. In computer vision, the proposed strategy crafts\ninputs with less activation sparsity which could otherwise be used to\naccelerate the computation. In this paper, we analyze the mechanism how these\nenergy-latency attacks reduce activation sparsity. In particular, we find that\ninput uniformity is a key enabler. A uniform image, that is, an image with\nmostly flat, uniformly colored surfaces, triggers more activations due to a\nspecific interplay of convolution, batch normalization, and ReLU activation.\nBased on these insights, we propose two new simple, yet effective strategies\nfor crafting sponge examples: sampling images from a probability distribution\nand identifying dense, yet inconspicuous inputs in natural datasets. We\nempirically examine our findings in a comprehensive evaluation with multiple\nimage classification models and show that our attack achieves the same sparsity\neffect as prior sponge-example methods, but at a fraction of computation\neffort. We also show that our sponge examples transfer between different neural\nnetworks. Finally, we discuss applications of our findings for the good by\nimproving efficiency by increasing sparsity.\n","authors":["Andreas Müller","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2403.18587v1.pdf","comment":"Accepted at the DLSP 2024"},{"id":"http://arxiv.org/abs/2403.18582v1","updated":"2024-03-27T14:03:41Z","published":"2024-03-27T14:03:41Z","title":"One flow to correct them all: improving simulations in high-energy\n  physics with a single normalising flow and a switch","summary":"  Simulated events are key ingredients in almost all high-energy physics\nanalyses. However, imperfections in the simulation can lead to sizeable\ndifferences between the observed data and simulated events. The effects of such\nmismodelling on relevant observables must be corrected either effectively via\nscale factors, with weights or by modifying the distributions of the\nobservables and their correlations. We introduce a correction method that\ntransforms one multidimensional distribution (simulation) into another one\n(data) using a simple architecture based on a single normalising flow with a\nboolean condition. We demonstrate the effectiveness of the method on a\nphysics-inspired toy dataset with non-trivial mismodelling of several\nobservables and their correlations.\n","authors":["Caio Cesar Daumann","Mauro Donega","Johannes Erdmann","Massimiliano Galli","Jan Lukas Späh","Davide Valsecchi"],"pdf_url":"https://arxiv.org/pdf/2403.18582v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2306.09459v3","updated":"2024-03-27T14:02:58Z","published":"2023-06-15T19:29:08Z","title":"Recurrent Action Transformer with Memory","summary":"  Recently, the use of transformers in offline reinforcement learning has\nbecome a rapidly developing area. This is due to their ability to treat the\nagent's trajectory in the environment as a sequence, thereby reducing the\npolicy learning problem to sequence modeling. In environments where the agent's\ndecisions depend on past events, it is essential to capture both the event\nitself and the decision point in the context of the model. However, the\nquadratic complexity of the attention mechanism limits the potential for\ncontext expansion. One solution to this problem is to enhance transformers with\nmemory mechanisms. In this paper, we propose the Recurrent Action Transformer\nwith Memory (RATE) - a model that incorporates recurrent memory. To evaluate\nour model, we conducted extensive experiments on both memory-intensive\nenvironments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo\ncontrol environments. The results show that the use of memory can significantly\nimprove performance in memory-intensive environments while maintaining or\nimproving results in classic environments. We hope that our findings will\nstimulate research on memory mechanisms for transformers applicable to offline\nreinforcement learning.\n","authors":["Alexey Staroverov","Egor Cherepanov","Dmitry Yudin","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2306.09459v3.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2309.11427v2","updated":"2024-03-27T14:02:57Z","published":"2023-09-20T16:01:45Z","title":"Generative Pre-Training of Time-Series Data for Unsupervised Fault\n  Detection in Semiconductor Manufacturing","summary":"  This paper introduces TRACE-GPT, which stands for Time-seRies\nAnomaly-detection with Convolutional Embedding and Generative Pre-trained\nTransformers. TRACE-GPT is designed to pre-train univariate time-series sensor\ndata and detect faults on unlabeled datasets in semiconductor manufacturing. In\nsemiconductor industry, classifying abnormal time-series sensor data from\nnormal data is important because it is directly related to wafer defect.\nHowever, small, unlabeled, and even mixed training data without enough\nanomalies make classification tasks difficult. In this research, we capture\nfeatures of time-series data with temporal convolutional embedding and\nGenerative Pre-trained Transformer (GPT) to classify abnormal sequences from\nnormal sequences using cross entropy loss. We prove that our model shows better\nperformance than previous unsupervised models with both an open dataset, the\nUniversity of California Riverside (UCR) time-series classification archive,\nand the process log of our Chemical Vapor Deposition (CVD) equipment. Our model\nhas the highest F1 score at Equal Error Rate (EER) across all datasets and is\nonly 0.026 below the supervised state-of-the-art baseline on the open dataset.\n","authors":["Sewoong Lee","JinKyou Choi","Min Su Kim"],"pdf_url":"https://arxiv.org/pdf/2309.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18579v1","updated":"2024-03-27T13:59:09Z","published":"2024-03-27T13:59:09Z","title":"On Optimizing Hyperparameters for Quantum Neural Networks","summary":"  The increasing capabilities of Machine Learning (ML) models go hand in hand\nwith an immense amount of data and computational power required for training.\nTherefore, training is usually outsourced into HPC facilities, where we have\nstarted to experience limits in scaling conventional HPC hardware, as theorized\nby Moore's law. Despite heavy parallelization and optimization efforts, current\nstate-of-the-art ML models require weeks for training, which is associated with\nan enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum\nMachine Learning (QML), can offer significant theoretical speed-ups and\nenhanced expressive power. However, training QML models requires tuning various\nhyperparameters, which is a nontrivial task and suboptimal choices can highly\naffect the trainability and performance of the models. In this study, we\nidentify the most impactful hyperparameters and collect data about the\nperformance of QML models. We compare different configurations and provide\nresearchers with performance data and concrete suggestions for hyperparameter\nselection.\n","authors":["Sabrina Herbst","Vincenzo De Maio","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2403.18579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18578v1","updated":"2024-03-27T13:59:05Z","published":"2024-03-27T13:59:05Z","title":"SteinGen: Generating Fidelitous and Diverse Graph Samples","summary":"  Generating graphs that preserve characteristic structures while promoting\nsample diversity can be challenging, especially when the number of graph\nobservations is small. Here, we tackle the problem of graph generation from\nonly one observed graph. The classical approach of graph generation from\nparametric models relies on the estimation of parameters, which can be\ninconsistent or expensive to compute due to intractable normalisation\nconstants. Generative modelling based on machine learning techniques to\ngenerate high-quality graph samples avoids parameter estimation but usually\nrequires abundant training samples. Our proposed generating procedure,\nSteinGen, which is phrased in the setting of graphs as realisations of\nexponential random graph models, combines ideas from Stein's method and MCMC by\nemploying Markovian dynamics which are based on a Stein operator for the target\nmodel. SteinGen uses the Glauber dynamics associated with an estimated Stein\noperator to generate a sample, and re-estimates the Stein operator from the\nsample after every sampling step. We show that on a class of exponential random\ngraph models this novel \"estimation and re-estimation\" generation strategy\nyields high distributional similarity (high fidelity) to the original data,\ncombined with high sample diversity.\n","authors":["Gesine Reinert","Wenkai Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18570v1","updated":"2024-03-27T13:51:26Z","published":"2024-03-27T13:51:26Z","title":"Physics-Informed Graph Neural Networks for Water Distribution Systems","summary":"  Water distribution systems (WDS) are an integral part of critical\ninfrastructure which is pivotal to urban development. As 70% of the world's\npopulation will likely live in urban environments in 2050, efficient simulation\nand planning tools for WDS play a crucial role in reaching UN's sustainable\ndevelopmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this\nrealm, we propose a novel and efficient machine learning emulator, more\nprecisely, a physics-informed deep learning (DL) model, for hydraulic state\nestimation in WDS. Using a recursive approach, our model only needs a few graph\nconvolutional neural network (GCN) layers and employs an innovative algorithm\nbased on message passing. Unlike conventional machine learning tasks, the model\nuses hydraulic principles to infer two additional hydraulic state features in\nthe process of reconstructing the available ground truth feature in an\nunsupervised manner. To the best of our knowledge, this is the first DL\napproach to emulate the popular hydraulic simulator EPANET, utilizing no\nadditional information. Like most DL models and unlike the hydraulic simulator,\nour model demonstrates vastly faster emulation times that do not increase\ndrastically with the size of the WDS. Moreover, we achieve high accuracy on the\nground truth and very similar results compared to the hydraulic simulator as\ndemonstrated through experiments on five real-world WDS datasets.\n","authors":["Inaam Ashraf","Janine Strotherm","Luca Hermes","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18570v1.pdf","comment":"Extended version of the paper with the same title published at\n  Proceedings of the AAAI Conference on Artificial Intelligence 2024"},{"id":"http://arxiv.org/abs/2403.18569v1","updated":"2024-03-27T13:50:13Z","published":"2024-03-27T13:50:13Z","title":"PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop\n  Prediction","summary":"  IR drop on the power delivery network (PDN) is closely related to PDN's\nconfiguration and cell current consumption. As the integrated circuit (IC)\ndesign is growing larger, dynamic IR drop simulation becomes computationally\nunaffordable and machine learning based IR drop prediction has been explored as\na promising solution. Although CNN-based methods have been adapted to IR drop\nprediction task in several works, the shortcomings of overlooking PDN\nconfiguration is non-negligible. In this paper, we consider not only how to\nproperly represent cell-PDN relation, but also how to model IR drop following\nits physical nature in the feature aggregation procedure. Thus, we propose a\nnovel graph structure, PDNGraph, to unify the representations of the PDN\nstructure and the fine-grained cell-PDN relation. We further propose a\ndual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN\nbranches to favorably capture the above features during the learning process.\nSeveral key designs are presented to make the dynamic IR drop prediction highly\neffective and interpretable. We are the first work to apply graph structure to\ndeep-learning based dynamic IR drop prediction method. Experiments show that\nPDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%\nreduction in prediction error and achieves 545x speedup compared to the\ncommercial tool, which demonstrates the superiority of our method.\n","authors":["Yuxiang Zhao","Zhuomin Chai","Xun Jiang","Yibo Lin","Runsheng Wang","Ru Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12370v2","updated":"2024-03-27T13:44:21Z","published":"2023-10-18T22:34:32Z","title":"No-Regret Learning in Bilateral Trade via Global Budget Balance","summary":"  Bilateral trade models the problem of intermediating between two rational\nagents -- a seller and a buyer -- both characterized by a private valuation for\nan item they want to trade. We study the online learning version of the\nproblem, in which at each time step a new seller and buyer arrive and the\nlearner has to set prices for them without any knowledge about their\n(adversarially generated) valuations.\n  In this setting, known impossibility results rule out the existence of\nno-regret algorithms when budget balanced has to be enforced at each time step.\nIn this paper, we introduce the notion of \\emph{global budget balance}, which\nonly requires the learner to fulfill budget balance over the entire time\nhorizon. Under this natural relaxation, we provide the first no-regret\nalgorithms for adversarial bilateral trade under various feedback models.\nFirst, we show that in the full-feedback model, the learner can guarantee\n$\\tilde O(\\sqrt{T})$ regret against the best fixed prices in hindsight, and\nthat this bound is optimal up to poly-logarithmic terms. Second, we provide a\nlearning algorithm guaranteeing a $\\tilde O(T^{3/4})$ regret upper bound with\none-bit feedback, which we complement with a $\\Omega(T^{5/7})$ lower bound that\nholds even in the two-bit feedback model. Finally, we introduce and analyze an\nalternative benchmark that is provably stronger than the best fixed prices in\nhindsight and is inspired by the literature on bandits with knapsacks.\n","authors":["Martino Bernasconi","Matteo Castiglioni","Andrea Celli","Federico Fusco"],"pdf_url":"https://arxiv.org/pdf/2310.12370v2.pdf","comment":"Accepted at STOC 2024"},{"id":"http://arxiv.org/abs/2403.18560v1","updated":"2024-03-27T13:42:14Z","published":"2024-03-27T13:42:14Z","title":"Noise-Robust Keyword Spotting through Self-supervised Pretraining","summary":"  Voice assistants are now widely available, and to activate them a keyword\nspotting (KWS) algorithm is used. Modern KWS systems are mainly trained using\nsupervised learning methods and require a large amount of labelled data to\nachieve a good performance. Leveraging unlabelled data through self-supervised\nlearning (SSL) has been shown to increase the accuracy in clean conditions.\nThis paper explores how SSL pretraining such as Data2Vec can be used to enhance\nthe robustness of KWS models in noisy conditions, which is under-explored.\n  Models of three different sizes are pretrained using different pretraining\napproaches and then fine-tuned for KWS. These models are then tested and\ncompared to models trained using two baseline supervised learning methods, one\nbeing standard training using clean data and the other one being multi-style\ntraining (MTR). The results show that pretraining and fine-tuning on clean data\nis superior to supervised learning on clean data across all testing conditions,\nand superior to supervised MTR for testing conditions of SNR above 5 dB. This\nindicates that pretraining alone can increase the model's robustness. Finally,\nit is found that using noisy data for pretraining models, especially with the\nData2Vec-denoising approach, significantly enhances the robustness of KWS\nmodels in noisy conditions.\n","authors":["Jacob Mørk","Holger Severin Bovbjerg","Gergely Kiss","Zheng-Hua Tan"],"pdf_url":"https://arxiv.org/pdf/2403.18560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00117v4","updated":"2024-03-27T13:38:00Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v4.pdf","comment":"CHI 2024"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18540v1","updated":"2024-03-27T13:17:15Z","published":"2024-03-27T13:17:15Z","title":"skscope: Fast Sparsity-Constrained Optimization in Python","summary":"  Applying iterative solvers on sparsity-constrained optimization (SCO)\nrequires tedious mathematical deduction and careful programming/debugging that\nhinders these solvers' broad impact. In the paper, the library skscope is\nintroduced to overcome such an obstacle. With skscope, users can solve the SCO\nby just programming the objective function. The convenience of skscope is\ndemonstrated through two examples in the paper, where sparse linear regression\nand trend filtering are addressed with just four lines of code. More\nimportantly, skscope's efficient implementation allows state-of-the-art solvers\nto quickly attain the sparse solution regardless of the high dimensionality of\nparameter space. Numerical experiments reveal the available solvers in skscope\ncan achieve up to 80x speedup on the competing relaxation solutions obtained\nvia the benchmarked convex solver. skscope is published on the Python Package\nIndex (PyPI) and Conda, and its source code is available at:\nhttps://github.com/abess-team/skscope.\n","authors":["Zezhi Wang","Jin Zhu","Peng Chen","Huiyang Peng","Xiaoke Zhang","Anran Wang","Yu Zheng","Junxian Zhu","Xueqin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18540v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.18539v1","updated":"2024-03-27T13:14:29Z","published":"2024-03-27T13:14:29Z","title":"Safe and Robust Reinforcement-Learning: Principles and Practice","summary":"  Reinforcement Learning (RL) has shown remarkable success in solving\nrelatively complex tasks, yet the deployment of RL systems in real-world\nscenarios poses significant challenges related to safety and robustness. This\npaper aims to identify and further understand those challenges thorough the\nexploration of the main dimensions of the safe and robust RL landscape,\nencompassing algorithmic, ethical, and practical considerations. We conduct a\ncomprehensive review of methodologies and open problems that summarizes the\nefforts in recent years to address the inherent risks associated with RL\napplications.\n  After discussing and proposing definitions for both safe and robust RL, the\npaper categorizes existing research works into different algorithmic approaches\nthat enhance the safety and robustness of RL agents. We examine techniques such\nas uncertainty estimation, optimisation methodologies, exploration-exploitation\ntrade-offs, and adversarial training. Environmental factors, including\nsim-to-real transfer and domain adaptation, are also scrutinized to understand\nhow RL systems can adapt to diverse and dynamic surroundings. Moreover, human\ninvolvement is an integral ingredient of the analysis, acknowledging the broad\nset of roles that humans can take in this context.\n  Importantly, to aid practitioners in navigating the complexities of safe and\nrobust RL implementation, this paper introduces a practical checklist derived\nfrom the synthesized literature. The checklist encompasses critical aspects of\nalgorithm design, training environment considerations, and ethical guidelines.\nIt will serve as a resource for developers and policymakers alike to ensure the\nresponsible deployment of RL systems in many application domains.\n","authors":["Taku Yamagata","Raul Santos-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2403.18539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18535v1","updated":"2024-03-27T13:11:34Z","published":"2024-03-27T13:11:34Z","title":"Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs","summary":"  Recent studies reveal a significant theoretical link between variational\nautoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to\nestimate the theoretical upper bound of the information rate-distortion\nfunction of images. Such estimated theoretical bounds substantially exceed the\nperformance of existing neural image codecs (NICs). To narrow this gap, we\npropose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The\nproposed BG-VAE leverages the theoretical bound to guide the NIC model towards\nenhanced performance. We implement the BG-VAE using Hierarchical VAEs and\ndemonstrate its effectiveness through extensive experiments. Along with\nadvanced neural network blocks, we provide a versatile, variable-rate NIC that\noutperforms existing methods when considering both rate-distortion performance\nand computational complexity. The code is available at BG-VAE.\n","authors":["Yichi Zhang","Zhihao Duan","Yuning Huang","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18535v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo (ICME2024)"},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2303.10365v3","updated":"2024-03-27T12:53:12Z","published":"2023-03-18T08:48:16Z","title":"CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label\n  Learning","summary":"  Partial-label learning (PLL) is an important weakly supervised learning\nproblem, which allows each training example to have a candidate label set\ninstead of a single ground-truth label. Identification-based methods have been\nwidely explored to tackle label ambiguity issues in PLL, which regard the true\nlabel as a latent variable to be identified. However, identifying the true\nlabels accurately and completely remains challenging, causing noise in pseudo\nlabels during model training. In this paper, we propose a new method called\nCroSel, which leverages historical predictions from the model to identify true\nlabels for most training examples. First, we introduce a cross selection\nstrategy, which enables two deep models to select true labels of partially\nlabeled data for each other. Besides, we propose a novel consistency\nregularization term called co-mix to avoid sample waste and tiny noise caused\nby false selection. In this way, CroSel can pick out the true labels of most\nexamples with high precision. Extensive experiments demonstrate the superiority\nof CroSel, which consistently outperforms previous state-of-the-art methods on\nbenchmark datasets. Additionally, our method achieves over 90\\% accuracy and\nquantity for selecting true labels on CIFAR-type datasets under various\nsettings.\n","authors":["Shiyu Tian","Hongxin Wei","Yiqun Wang","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2303.10365v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18519v1","updated":"2024-03-27T12:50:27Z","published":"2024-03-27T12:50:27Z","title":"Improving Line Search Methods for Large Scale Neural Network Training","summary":"  In recent studies, line search methods have shown significant improvements in\nthe performance of traditional stochastic gradient descent techniques,\neliminating the need for a specific learning rate schedule. In this paper, we\nidentify existing issues in state-of-the-art line search methods, propose\nenhancements, and rigorously evaluate their effectiveness. We test these\nmethods on larger datasets and more complex data domains than before.\nSpecifically, we improve the Armijo line search by integrating the momentum\nterm from ADAM in its search direction, enabling efficient large-scale\ntraining, a task that was previously prone to failure using Armijo line search\nmethods. Our optimization approach outperforms both the previous Armijo\nimplementation and tuned learning rate schedules for Adam. Our evaluation\nfocuses on Transformers and CNNs in the domains of NLP and image data. Our work\nis publicly available as a Python package, which provides a hyperparameter free\nPytorch optimizer.\n","authors":["Philip Kenneweg","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18517v1","updated":"2024-03-27T12:49:14Z","published":"2024-03-27T12:49:14Z","title":"Efficient Algorithms for Regularized Nonnegative Scale-invariant\n  Low-rank Approximation Models","summary":"  Regularized nonnegative low-rank approximations such as sparse Nonnegative\nMatrix Factorization or sparse Nonnegative Tucker Decomposition are an\nimportant branch of dimensionality reduction models with enhanced\ninterpretability. However, from a practical perspective, the choice of\nregularizers and regularization coefficients, as well as the design of\nefficient algorithms, is challenging because of the multifactor nature of these\nmodels and the lack of theory to back these choices. This paper aims at\nimproving upon these issues. By studying a more general model called the\nHomogeneous Regularized Scale-Invariant, we prove that the scale-invariance\ninherent to low-rank approximation models causes an implicit regularization\nwith both unexpected beneficial and detrimental effects. This observation\nallows to better understand the effect of regularization functions in low-rank\napproximation models, to guide the choice of the regularization\nhyperparameters, and to design balancing strategies to enhance the convergence\nspeed of dedicated optimization algorithms. Some of these results were already\nknown but restricted to specific instances of regularized low-rank\napproximations. We also derive a generic Majorization Minimization algorithm\nthat handles many regularized nonnegative low-rank approximations, with\nconvergence guarantees. We showcase our contributions on sparse Nonnegative\nMatrix Factorization, ridge-regularized Canonical Polyadic decomposition and\nsparse Nonnegative Tucker Decomposition.\n","authors":["Jeremy E. Cohen","Valentin Leplat"],"pdf_url":"https://arxiv.org/pdf/2403.18517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18514v1","updated":"2024-03-27T12:44:57Z","published":"2024-03-27T12:44:57Z","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection\n  of Pathological Pulmonary CT scans","summary":"  Unsupervised pathology detection can be implemented by training a model on\nhealthy data only and measuring the deviation from the training set upon\ninference, for example with CNN-based feature extraction and one-class\nclassifiers, or reconstruction-score-based methods such as AEs, GANs and\nDiffusion models. Normalizing Flows (NF) have the ability to directly learn the\nprobability distribution of training examples through an invertible\narchitecture. We leverage this property in a novel 3D NF-based model named\nCT-3DFlow, specifically tailored for patient-level pulmonary pathology\ndetection in chest CT data. Our model is trained unsupervised on healthy 3D\npulmonary CT patches, and detects deviations from its log-likelihood\ndistribution as anomalies. We aggregate patches-level likelihood values from a\npatient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.\nOut-of-distribution detection performance is evaluated using expert annotations\non a separate chest CT test dataset, outperforming other state-of-the-art\nmethods.\n","authors":["Aissam Djahnine","Alexandre Popoff","Emilien Jupin-Delevaux","Vincent Cottin","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2403.18514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18509v1","updated":"2024-03-27T12:39:16Z","published":"2024-03-27T12:39:16Z","title":"Distributed Maximum Consensus over Noisy Links","summary":"  We introduce a distributed algorithm, termed noise-robust distributed maximum\nconsensus (RD-MC), for estimating the maximum value within a multi-agent\nnetwork in the presence of noisy communication links. Our approach entails\nredefining the maximum consensus problem as a distributed optimization problem,\nallowing a solution using the alternating direction method of multipliers.\nUnlike existing algorithms that rely on multiple sets of noise-corrupted\nestimates, RD-MC employs a single set, enhancing both robustness and\nefficiency. To further mitigate the effects of link noise and improve\nrobustness, we apply moving averaging to the local estimates. Through extensive\nsimulations, we demonstrate that RD-MC is significantly more robust to\ncommunication link noise compared to existing maximum-consensus algorithms.\n","authors":["Ehsan Lari","Reza Arablouei","Naveen K. D. Venkategowda","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2403.18509v1.pdf","comment":"5 pages, 7 figures, submitted to EUSIPCO 2024 conference"},{"id":"http://arxiv.org/abs/2403.18506v1","updated":"2024-03-27T12:35:23Z","published":"2024-03-27T12:35:23Z","title":"Faster Convergence for Transformer Fine-tuning with Line Search Methods","summary":"  Recent works have shown that line search methods greatly increase performance\nof traditional stochastic gradient descent methods on a variety of datasets and\narchitectures [1], [2]. In this work we succeed in extending line search\nmethods to the novel and highly popular Transformer architecture and dataset\ndomains in natural language processing. More specifically, we combine the\nArmijo line search with the Adam optimizer and extend it by subdividing the\nnetworks architecture into sensible units and perform the line search\nseparately on these local units. Our optimization method outperforms the\ntraditional Adam optimizer and achieves significant performance improvements\nfor small data sets or small training budgets, while performing equal or better\nfor other tested cases. Our work is publicly available as a python package,\nwhich provides a hyperparameter-free pytorch optimizer that is compatible with\narbitrary network architectures.\n","authors":["Philip Kenneweg","Leonardo Galli","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08579v2","updated":"2024-03-27T12:28:02Z","published":"2024-03-13T14:34:34Z","title":"Machine Learning Optimized Orthogonal Basis Piecewise Polynomial\n  Approximation","summary":"  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,\nlike trajectory planning, to approximate position profiles given in the form of\na set of points. While the approximation target along with domain-specific\nrequirements, like Ck -continuity, can be formulated as a system of equations\nand a result can be computed directly, such closed-form solutions posses\nlimited flexibility with respect to polynomial degrees, polynomial bases or\nadding further domain-specific requirements. Sufficiently complex optimization\ngoals soon call for the use of numerical methods, like gradient descent. Since\ngradient descent lies at the heart of training Artificial Neural Networks\n(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set\nof gradient-based optimizers potentially suitable for a wide range of\noptimization problems beyond the training task for ANNs. Our approach is to\nutilize the versatility of PP models and combine it with the potential of\nmodern ML optimizers for the use in function approximation in 1D trajectory\nplanning in the context of electronic cam design. We utilize available\noptimizers of the ML framework TensorFlow directly, outside of the scope of\nANNs, to optimize model parameters of our PP model. In this paper, we show how\nan orthogonal polynomial basis contributes to improving approximation and\ncontinuity optimization performance. Utilizing Chebyshev polynomials of the\nfirst kind, we develop a novel regularization approach enabling clearly\nimproved convergence behavior. We show that, using this regularization\napproach, Chebyshev basis performs better than power basis for all relevant\noptimizers in the combined approximation and continuity optimization setting\nand demonstrate usability of the presented approach within the electronic cam\ndomain.\n","authors":["Hannes Waclawek","Stefan Huber"],"pdf_url":"https://arxiv.org/pdf/2403.08579v2.pdf","comment":"Submitted to LION18"},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.18495v1","updated":"2024-03-27T12:15:22Z","published":"2024-03-27T12:15:22Z","title":"Direct mineral content prediction from drill core images via transfer\n  learning","summary":"  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n","authors":["Romana Boiger","Sergey V. Churakov","Ignacio Ballester Llagaria","Georg Kosakowski","Raphael Wüst","Nikolaos I. Prasianakis"],"pdf_url":"https://arxiv.org/pdf/2403.18495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18494v1","updated":"2024-03-27T12:10:30Z","published":"2024-03-27T12:10:30Z","title":"Learning in PINNs: Phase transition, total diffusion, and generalization","summary":"  We investigate the learning dynamics of fully-connected neural networks\nthrough the lens of gradient signal-to-noise ratio (SNR), examining the\nbehavior of first-order optimizers like Adam in non-convex objectives. By\ninterpreting the drift/diffusion phases in the information bottleneck theory,\nfocusing on gradient homogeneity, we identify a third phase termed ``total\ndiffusion\", characterized by equilibrium in the learning rates and homogeneous\ngradients. This phase is marked by an abrupt SNR increase, uniform residuals\nacross the sample space and the most rapid training convergence. We propose a\nresidual-based re-weighting scheme to accelerate this diffusion in quadratic\nloss functions, enhancing generalization. We also explore the information\ncompression phenomenon, pinpointing a significant saturation-induced\ncompression of activations at the total diffusion phase, with deeper layers\nexperiencing negligible information loss. Supported by experimental data on\nphysics-informed neural networks (PINNs), which underscore the importance of\ngradient homogeneity due to their PDE-based sample inter-dependence, our\nfindings suggest that recognizing phase transitions could refine ML\noptimization strategies for improved generalization.\n","authors":["Sokratis J. Anagnostopoulos","Juan Diego Toscano","Nikolaos Stergiopulos","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2403.18494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18489v1","updated":"2024-03-27T12:01:51Z","published":"2024-03-27T12:01:51Z","title":"Impact of Employing Weather Forecast Data as Input to the Estimation of\n  Evapotranspiration by Deep Neural Network Models","summary":"  Reference Evapotranspiration (ET0) is a key parameter for designing smart\nirrigation scheduling, since it is related by a coefficient to the water needs\nof a crop. The United Nations Food and Agriculture Organization, proposed a\nstandard method for ET0 computation (FAO56PM), based on the parameterization of\nthe Penman-Monteith equation, that is widely adopted in the literature. To\ncompute ET0 using the FAO56-PM method, four main weather parameters are needed:\ntemperature, humidity, wind, and solar radiation (SR). One way to make daily\nET0 estimations for future days is to use freely available weather forecast\nservices (WFSs), where many meteorological parameters are estimated up to the\nnext 15 days. A problem with this method is that currently, SR is not provided\nas a free forecast parameter on most of those online services or, normally,\nsuch forecasts present a financial cost penalty. For this reason, several ET0\nestimation models using machine and deep learning were developed and presented\nin the literature, that use as input features a reduced set of carefully\nselected weather parameters, that are compatible with common freely available\nWFSs. However, most studies on this topic have only evaluated model performance\nusing data from weather stations (WSs), without considering the effect of using\nweather forecast data. In this study, the performance of authors' previous\nmodels is evaluated when using weather forecast data from two online WFSs, in\nthe following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)\nestimate SR by ANN model, and then use that estimation for ET0 computation,\nusing the FAO56-PM method. Employing data collected from two WFSs and a WS\nlocated in Vale do Lobo, Portugal, the latter approach achieved the best\nresult, with a coefficient of determination (R2) ranging between 0.893 and\n0.667, when considering forecasts up to 15 days.\n","authors":["Pedro J. Vaz","Gabriela Schütz","Carlos Guerrero","Pedro J. S. Cardoso"],"pdf_url":"https://arxiv.org/pdf/2403.18489v1.pdf","comment":"A partial version of the work submitted to ESRE/INTERNATIONAL\n  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY"},{"id":"http://arxiv.org/abs/2403.18486v1","updated":"2024-03-27T11:58:45Z","published":"2024-03-27T11:58:45Z","title":"Synthesizing EEG Signals from Event-Related Potential Paradigms with\n  Conditional Diffusion Models","summary":"  Data scarcity in the brain-computer interface field can be alleviated through\nthe use of generative models, specifically diffusion models. While diffusion\nmodels have previously been successfully applied to electroencephalogram (EEG)\ndata, existing models lack flexibility w.r.t.~sampling or require alternative\nrepresentations of the EEG data. To overcome these limitations, we introduce a\nnovel approach to conditional diffusion models that utilizes classifier-free\nguidance to directly generate subject-, session-, and class-specific EEG data.\nIn addition to commonly used metrics, domain-specific metrics are employed to\nevaluate the specificity of the generated samples. The results indicate that\nthe proposed model can generate EEG data that resembles real data for each\nsubject, session, and class.\n","authors":["Guido Klein","Pierre Guetschel","Gianluigi Silvestri","Michael Tangermann"],"pdf_url":"https://arxiv.org/pdf/2403.18486v1.pdf","comment":"submitted to 9th Graz BCI conference, 6 pages, 3 figures, first\n  figure is split into two subfigures, 1 table"},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18451v1","updated":"2024-03-27T11:11:06Z","published":"2024-03-27T11:11:06Z","title":"CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in\n  Resource-Constrained CPS and IoT","summary":"  Foundation models (FMs) emerge as a promising solution to harness distributed\nand diverse environmental data by leveraging prior knowledge to understand the\ncomplicated temporal and spatial correlations within heterogeneous datasets.\nUnlike distributed learning frameworks such as federated learning, which often\nstruggle with multimodal data, FMs can transform diverse inputs into\nembeddings. This process facilitates the integration of information from\nvarious modalities and the application of prior learning to new domains.\nHowever, deploying FMs in resource-constrained edge systems poses significant\nchallenges. To this end, we introduce CoRAST, a novel learning framework that\nutilizes FMs for enhanced analysis of distributed, correlated heterogeneous\ndata. Utilizing a server-based FM, CoRAST can exploit existing environment\ninformation to extract temporal, spatial, and cross-modal correlations among\nsensor data. This enables CoRAST to offer context-aware insights for localized\nclient tasks through FM-powered global representation learning. Our evaluation\non real-world weather dataset demonstrates CoRAST's ability to exploit\ncorrelated heterogeneous data through environmental representation learning to\nreduce the forecast errors by up to 50.3% compared to the baselines.\n","authors":["Yi Hu","Jinhang Zuo","Alanis Zhao","Bob Iannucci","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2403.18451v1.pdf","comment":"accepted and to be published in 2024 IEEE International Workshop on\n  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)"},{"id":"http://arxiv.org/abs/2403.09267v3","updated":"2024-03-27T11:11:02Z","published":"2024-03-14T10:44:10Z","title":"Deep Limit Order Book Forecasting","summary":"  We exploit cutting-edge deep learning methodologies to explore the\npredictability of high-frequency Limit Order Book mid-price changes for a\nheterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we\nrelease `LOBFrame', an open-source code base to efficiently process large-scale\nLimit Order Book data and quantitatively assess state-of-the-art deep learning\nmodels' forecasting capabilities. Our results are twofold. We demonstrate that\nthe stocks' microstructural characteristics influence the efficacy of deep\nlearning methods and that their high forecasting power does not necessarily\ncorrespond to actionable trading signals. We argue that traditional machine\nlearning metrics fail to adequately assess the quality of forecasts in the\nLimit Order Book context. As an alternative, we propose an innovative\noperational framework that evaluates predictions' practicality by focusing on\nthe probability of accurately forecasting complete transactions. This work\noffers academics and practitioners an avenue to make informed and robust\ndecisions on the application of deep learning techniques, their scope and\nlimitations, effectively exploiting emergent statistical properties of the\nLimit Order Book.\n","authors":["Antonio Briola","Silvia Bartolucci","Tomaso Aste"],"pdf_url":"https://arxiv.org/pdf/2403.09267v3.pdf","comment":"43 pages, 14 figures, 12 Tables"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18444v1","updated":"2024-03-27T11:00:53Z","published":"2024-03-27T11:00:53Z","title":"FRESCO: Federated Reinforcement Energy System for Cooperative\n  Optimization","summary":"  The rise in renewable energy is creating new dynamics in the energy grid that\npromise to create a cleaner and more participative energy grid, where\ntechnology plays a crucial part in making the required flexibility to achieve\nthe vision of the next-generation grid. This work presents FRESCO, a framework\nthat aims to ease the implementation of energy markets using a hierarchical\ncontrol architecture of reinforcement learning agents trained using federated\nlearning. The core concept we are proving is that having greedy agents subject\nto changing conditions from a higher level agent creates a cooperative setup\nthat will allow for fulfilling all the individual objectives. This paper\npresents a general overview of the framework, the current progress, and some\ninsights we obtained from the recent results.\n","authors":["Nicolas Mauricio Cuadrado","Roberto Alejandro Gutierrez","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18444v1.pdf","comment":"Tiny Paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2403.18439v1","updated":"2024-03-27T10:47:06Z","published":"2024-03-27T10:47:06Z","title":"Generalized Policy Learning for Smart Grids: FL TRPO Approach","summary":"  The smart grid domain requires bolstering the capabilities of existing energy\nmanagement systems; Federated Learning (FL) aligns with this goal as it\ndemonstrates a remarkable ability to train models on heterogeneous datasets\nwhile maintaining data privacy, making it suitable for smart grid applications,\nwhich often involve disparate data distributions and interdependencies among\nfeatures that hinder the suitability of linear models. This paper introduces a\nframework that combines FL with a Trust Region Policy Optimization (FL TRPO)\naiming to reduce energy-associated emissions and costs. Our approach reveals\nlatent interconnections and employs personalized encoding methods to capture\nunique insights, understanding the relationships between features and optimal\nstrategies, allowing our model to generalize to previously unseen data.\nExperimental results validate the robustness of our approach, affirming its\nproficiency in effectively learning policy models for smart grid challenges.\n","authors":["Yunxiang Li","Nicolas Mauricio Cuadrado","Samuel Horváth","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18439v1.pdf","comment":"ICLR 2024 Workshop: Tackling Climate Change with Machine Learning"},{"id":"http://arxiv.org/abs/2403.18438v1","updated":"2024-03-27T10:45:16Z","published":"2024-03-27T10:45:16Z","title":"Global Vegetation Modeling with Pre-Trained Weather Transformers","summary":"  Accurate vegetation models can produce further insights into the complex\ninteraction between vegetation activity and ecosystem processes. Previous\nresearch has established that long-term trends and short-term variability of\ntemperature and precipitation affect vegetation activity. Motivated by the\nrecent success of Transformer-based Deep Learning models for medium-range\nweather forecasting, we adapt the publicly available pre-trained FourCastNet to\nmodel vegetation activity while accounting for the short-term dynamics of\nclimate variability. We investigate how the learned global representation of\nthe atmosphere's state can be transferred to model the normalized difference\nvegetation index (NDVI). Our model globally estimates vegetation activity at a\nresolution of \\SI{0.25}{\\degree} while relying only on meteorological data. We\ndemonstrate that leveraging pre-trained weather models improves the NDVI\nestimates compared to learning an NDVI model from scratch. Additionally, we\ncompare our results to other recent data-driven NDVI modeling approaches from\nmachine learning and ecology literature. We further provide experimental\nevidence on how much data and training time is necessary to turn FourCastNet\ninto an effective vegetation model. Code and models will be made available upon\npublication.\n","authors":["Pascal Janetzky","Florian Gallusser","Simon Hentschel","Andreas Hotho","Anna Krause"],"pdf_url":"https://arxiv.org/pdf/2403.18438v1.pdf","comment":"Tackling Climate Change with Machine Learning Workshop @ ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18436v1","updated":"2024-03-27T10:40:27Z","published":"2024-03-27T10:40:27Z","title":"Collaborative Active Learning in Conditional Trust Environment","summary":"  In this paper, we investigate collaborative active learning, a paradigm in\nwhich multiple collaborators explore a new domain by leveraging their combined\nmachine learning capabilities without disclosing their existing data and\nmodels. Instead, the collaborators share prediction results from the new domain\nand newly acquired labels. This collaboration offers several advantages: (a) it\naddresses privacy and security concerns by eliminating the need for direct\nmodel and data disclosure; (b) it enables the use of different data sources and\ninsights without direct data exchange; and (c) it promotes cost-effectiveness\nand resource efficiency through shared labeling costs. To realize these\nbenefits, we introduce a collaborative active learning framework designed to\nfulfill the aforementioned objectives. We validate the effectiveness of the\nproposed framework through simulations. The results demonstrate that\ncollaboration leads to higher AUC scores compared to independent efforts,\nhighlighting the framework's ability to overcome the limitations of individual\nmodels. These findings support the use of collaborative approaches in active\nlearning, emphasizing their potential to enhance outcomes through collective\nexpertise and shared resources. Our work provides a foundation for further\nresearch on collaborative active learning and its practical applications in\nvarious domains where data privacy, cost efficiency, and model performance are\ncritical considerations.\n","authors":["Zan-Kai Chong","Hiroyuki Ohsaki","Bryan Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18436v1.pdf","comment":"5 pages, 9 figures, conference"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01191v2","updated":"2024-03-27T10:12:31Z","published":"2023-11-02T12:36:19Z","title":"VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node\n  Classification","summary":"  Class imbalance in graph data presents significant challenges for node\nclassification. While existing methods, such as SMOTE-based approaches,\npartially mitigate this issue, they still exhibit limitations in constructing\nimbalanced graphs. Generative self-supervised learning (SSL) methods,\nexemplified by graph autoencoders (GAEs), offer a promising solution by\ndirectly generating minority nodes from the data itself, yet their potential\nremains underexplored. In this paper, we delve into the shortcomings of\nSMOTE-based approaches in the construction of imbalanced graphs. Furthermore,\nwe introduce VIGraph, a simple yet effective generative SSL approach that\nrelies on the Variational GAE as the fundamental model. VIGraph strictly\nadheres to the concept of imbalance when constructing imbalanced graphs and\ninnovatively leverages the variational inference (VI) ability of Variational\nGAE to generate nodes for minority classes. VIGraph introduces comprehensive\ntraining strategies, including cross-view contrastive learning at the decoding\nphase to capture semantic knowledge, adjacency matrix reconstruction to\npreserve graph structure, and alignment strategy to ensure stable training.\nVIGraph can generate high-quality nodes directly usable for classification,\neliminating the need to integrate the generated nodes back to the graph as well\nas additional retraining found in SMOTE-based methods. We conduct extensive\nexperiments, results from which demonstrate the superiority and generality of\nour approach.\n","authors":["Yulan Hu","Sheng Ouyang","Zhirui Yang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.01191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18415v1","updated":"2024-03-27T10:06:33Z","published":"2024-03-27T10:06:33Z","title":"The Topos of Transformer Networks","summary":"  The transformer neural network has significantly out-shined all other neural\nnetwork architectures as the engine behind large language models. We provide a\ntheoretical analysis of the expressivity of the transformer architecture\nthrough the lens of topos theory. From this viewpoint, we show that many common\nneural network architectures, such as the convolutional, recurrent and graph\nconvolutional networks, can be embedded in a pretopos of piecewise-linear\nfunctions, but that the transformer necessarily lives in its topos completion.\nIn particular, this suggests that the two network families instantiate\ndifferent fragments of logic: the former are first order, whereas transformers\nare higher-order reasoners. Furthermore, we draw parallels with architecture\nsearch and gradient descent, integrating our analysis in the framework of\ncybernetic agents.\n","authors":["Mattia Jacopo Villani","Peter McBurney"],"pdf_url":"https://arxiv.org/pdf/2403.18415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06075v2","updated":"2024-03-27T09:51:15Z","published":"2023-09-12T09:12:37Z","title":"A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation","summary":"  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n","authors":["Francesco Galati","Daniele Falcetta","Rosa Cortese","Barbara Casolla","Ferran Prados","Ninon Burgos","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2309.06075v2.pdf","comment":"Accepted at the 34th British Machine Vision Conference (BMVC)"},{"id":"http://arxiv.org/abs/2310.05723v2","updated":"2024-03-27T09:48:34Z","published":"2023-10-09T13:47:05Z","title":"Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement\n  Learning","summary":"  Offline pretraining with a static dataset followed by online fine-tuning\n(offline-to-online, or OtO) is a paradigm well matched to a real-world RL\ndeployment process. In this scenario, we aim to find the best-performing policy\nwithin a limited budget of online interactions. Previous work in the OtO\nsetting has focused on correcting for bias introduced by the policy-constraint\nmechanisms of offline RL algorithms. Such constraints keep the learned policy\nclose to the behavior policy that collected the dataset, but we show this can\nunnecessarily limit policy performance if the behavior policy is far from\noptimal. Instead, we forgo constraints and frame OtO RL as an exploration\nproblem that aims to maximize the benefit of online data-collection. We first\nstudy the major online RL exploration methods based on intrinsic rewards and\nUCB in the OtO setting, showing that intrinsic rewards add training instability\nthrough reward-function modification, and UCB methods are myopic and it is\nunclear which learned-component's ensemble to use for action selection. We then\nintroduce an algorithm for planning to go out-of-distribution (PTGOOD) that\navoids these issues. PTGOOD uses a non-myopic planning procedure that targets\nexploration in relatively high-reward regions of the state-action space\nunlikely to be visited by the behavior policy. By leveraging concepts from the\nConditional Entropy Bottleneck, PTGOOD encourages data collected online to\nprovide new information relevant to improving the final deployment policy\nwithout altering rewards. We show empirically in several continuous control\ntasks that PTGOOD significantly improves agent returns during online\nfine-tuning and avoids the suboptimal policy convergence that many of our\nbaselines exhibit in several environments.\n","authors":["Trevor McInroe","Adam Jelley","Stefano V. Albrecht","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2310.05723v2.pdf","comment":"10 pages, 17 figures, preprint"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18402v1","updated":"2024-03-27T09:44:50Z","published":"2024-03-27T09:44:50Z","title":"On Spectrogram Analysis in a Multiple Classifier Fusion Framework for\n  Power Grid Classification Using Electric Network Frequency","summary":"  The Electric Network Frequency (ENF) serves as a unique signature inherent to\npower distribution systems. Here, a novel approach for power grid\nclassification is developed, leveraging ENF. Spectrograms are generated from\naudio and power recordings across different grids, revealing distinctive ENF\npatterns that aid in grid classification through a fusion of classifiers. Four\ntraditional machine learning classifiers plus a Convolutional Neural Network\n(CNN), optimized using Neural Architecture Search, are developed for One-vs-All\nclassification. This process generates numerous predictions per sample, which\nare then compiled and used to train a shallow multi-label neural network\nspecifically designed to model the fusion process, ultimately leading to the\nconclusive class prediction for each sample. Experimental findings reveal that\nboth validation and testing accuracy outperform those of current\nstate-of-the-art classifiers, underlining the effectiveness and robustness of\nthe proposed methodology.\n","authors":["Georgios Tzolopoulos","Christos Korgialas","Constantine Kotropoulos"],"pdf_url":"https://arxiv.org/pdf/2403.18402v1.pdf","comment":"13th International Conference on Pattern Recognition Applications and\n  Methods (ICPRAM)"},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.18393v1","updated":"2024-03-27T09:30:50Z","published":"2024-03-27T09:30:50Z","title":"Tensor-based Graph Learning with Consistency and Specificity for\n  Multi-view Clustering","summary":"  Graph learning is widely recognized as a crucial technique in multi-view\nclustering. Existing graph learning methods typically involve constructing an\nadaptive neighbor graph based on probabilistic neighbors and then learning a\nconsensus graph to for clustering, however, they are confronted with two\nlimitations. Firstly, they often rely on Euclidean distance to measure\nsimilarity when constructing the adaptive neighbor graph, which proves\ninadequate in capturing the intrinsic structure among data points in many\nreal-world scenarios. Secondly, most of these methods focus solely on consensus\ngraph, ignoring view-specific graph information. In response to the\naforementioned drawbacks, we in this paper propose a novel tensor-based graph\nlearning framework that simultaneously considers consistency and specificity\nfor multi-view clustering. Specifically, we calculate the similarity distance\non the Stiefel manifold to preserve the intrinsic structure among data points.\nBy making an assumption that the learned neighbor graph of each view comprises\nboth a consistent graph and a view-specific graph, we formulate a new\ntensor-based target graph learning paradigm. Owing to the benefits of tensor\nsingular value decomposition (t-SVD) in uncovering high-order correlations,\nthis model is capable of achieving a complete understanding of the target\ngraph. Furthermore, we develop an iterative algorithm to solve the proposed\nobjective optimization problem. Experiments conducted on real-world datasets\nhave demonstrated the superior performance of the proposed method over some\nstate-of-the-art multi-view clustering methods. The source code has been\nreleased on https://github.com/lshi91/CSTGL-Code.\n","authors":["Long Shi","Lei Cao","Yunshan Ye","Yu Zhao","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18383v1","updated":"2024-03-27T09:21:07Z","published":"2024-03-27T09:21:07Z","title":"Generative Multi-modal Models are Good Class-Incremental Learners","summary":"  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic\nforgetting caused by the classifier's bias towards the current task has long\nposed a significant challenge. It is mainly caused by the characteristic of\ndiscriminative models. With the growing popularity of the generative\nmulti-modal models, we would explore replacing discriminative models with\ngenerative ones for CIL. However, transitioning from discriminative to\ngenerative models requires addressing two key challenges. The primary challenge\nlies in transferring the generated textual information into the classification\nof distinct categories. Additionally, it requires formulating the task of CIL\nwithin a generative framework. To this end, we propose a novel generative\nmulti-modal model (GMM) framework for class-incremental learning. Our approach\ndirectly generates labels for images using an adapted generative model. After\nobtaining the detailed text, we use a text encoder to extract text features and\nemploy feature matching to determine the most similar label as the\nclassification prediction. In the conventional CIL settings, we achieve\nsignificantly better results in long-sequence task scenarios. Under the\nFew-shot CIL setting, we have improved by at least 14\\% accuracy over all the\ncurrent state-of-the-art methods with significantly less forgetting. Our code\nis available at \\url{https://github.com/DoubleClass/GMM}.\n","authors":["Xusheng Cao","Haori Lu","Linlan Huang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.18383v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18379v1","updated":"2024-03-27T09:17:50Z","published":"2024-03-27T09:17:50Z","title":"IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining\n  Useful Life Prediction","summary":"  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion\nbatteries is crucial for maintaining the safe and stable operation of\nrechargeable battery management systems. However, this task is often\nchallenging due to the complex temporal dynamics involved. Recently,\nattention-based networks, such as Transformers and Informer, have been the\npopular architecture in time series forecasting. Despite their effectiveness,\nthese models with abundant parameters necessitate substantial training time to\nunravel temporal patterns. To tackle these challenges, we propose a simple\nMLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which\nis an architecture based exclusively on multi-layer perceptrons (MLPs),\nextracting information by mixing operations along both intra-patch and\ninter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer\ncomprises parallel dual-head mixer layers: the intra-patch mixing MLP,\ncapturing local temporal patterns in the short-term period, and the inter-patch\nmixing MLP, capturing global temporal patterns in the long-term period.\nNotably, to address the varying importance of features in RUL prediction, we\nintroduce a weighted loss function in the MLP-Mixer-based architecture, marking\nthe first time such an approach has been employed. Our experiments demonstrate\nthat IIP-Mixer achieves competitive performance in battery RUL prediction,\noutperforming other popular time-series frameworks\n","authors":["Guangzai Ye","Li Feng","Jianlan Guo","Yuqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18375v1","updated":"2024-03-27T09:14:36Z","published":"2024-03-27T09:14:36Z","title":"Stragglers-Aware Low-Latency Synchronous Federated Learning via\n  Layer-Wise Model Updates","summary":"  Synchronous federated learning (FL) is a popular paradigm for collaborative\nedge learning. It typically involves a set of heterogeneous devices locally\ntraining neural network (NN) models in parallel with periodic centralized\naggregations. As some of the devices may have limited computational resources\nand varying availability, FL latency is highly sensitive to stragglers.\nConventional approaches discard incomplete intra-model updates done by\nstragglers, alter the amount of local workload and architecture, or resort to\nasynchronous settings; which all affect the trained model performance under\ntight training latency constraints. In this work, we propose straggler-aware\nlayer-wise federated learning (SALF) that leverages the optimization procedure\nof NNs via backpropagation to update the global model in a layer-wise fashion.\nSALF allows stragglers to synchronously convey partial gradients, having each\nlayer of the global model be updated independently with a different\ncontributing set of users. We provide a theoretical analysis, establishing\nconvergence guarantees for the global model under mild assumptions on the\ndistribution of the participating devices, revealing that SALF converges at the\nsame asymptotic rate as FL with no timing limitations. This insight is matched\nwith empirical observations, demonstrating the performance gains of SALF\ncompared to alternative mechanisms mitigating the device heterogeneity gap in\nFL.\n","authors":["Natalie Lang","Alejandro Cohen","Nir Shlezinger"],"pdf_url":"https://arxiv.org/pdf/2403.18375v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.18821v1","updated":"2024-03-27T17:59:56Z","published":"2024-03-27T17:59:56Z","title":"Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and\n  Benchmark","summary":"  We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/\n","authors":["Ziyang Chen","Israel D. Gebru","Christian Richardt","Anurag Kumar","William Laney","Andrew Owens","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2403.18821v1.pdf","comment":"Accepted to CVPR 2024. Project site:\n  https://facebookresearch.github.io/real-acoustic-fields/"},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18714v1","updated":"2024-03-27T16:02:00Z","published":"2024-03-27T16:02:00Z","title":"Bringing Textual Prompt to AI-Generated Image Quality Assessment","summary":"  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike\ntraditional image quality assessment (IQA) on natural scenarios, AGIs quality\nassessment (AGIQA) takes the correspondence of image and its textual prompt\ninto consideration. This is coupled in the ground truth score, which confuses\nthe unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs\nQuality Assessment via Image and Prompt), a multimodal framework for AGIQA via\ncorresponding image and prompt incorporation. Specifically, we propose a novel\nincremental pretraining task named Image2Prompt for better understanding of\nAGIs and their corresponding textual prompts. An effective and efficient\nimage-prompt fusion module, along with a novel special [QA] token, are also\napplied. Both are plug-and-play and beneficial for the cooperation of image and\nits corresponding prompt. Experiments demonstrate that our IP-IQA achieves the\nstate-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.\n","authors":["Bowen Qu","Haohui Li","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18714v1.pdf","comment":"6 pages, 3 figures, accepted by ICME2024"},{"id":"http://arxiv.org/abs/2402.14326v2","updated":"2024-03-27T13:25:17Z","published":"2024-02-22T06:38:25Z","title":"Think before You Leap: Content-Aware Low-Cost Edge-Assisted Video\n  Semantic Segmentation","summary":"  Offloading computing to edge servers is a promising solution to support\ngrowing video understanding applications at resource-constrained IoT devices.\nRecent efforts have been made to enhance the scalability of such systems by\nreducing inference costs on edge servers. However, existing research is not\ndirectly applicable to pixel-level vision tasks such as video semantic\nsegmentation (VSS), partly due to the fluctuating VSS accuracy and segment\nbitrate caused by the dynamic video content. In response, we present Penance, a\nnew edge inference cost reduction framework. By exploiting softmax outputs of\nVSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes\nmodel selection and compression settings to minimize the inference cost while\nmeeting the required accuracy within the available bandwidth constraints. We\nimplement Penance in a commercial IoT device with only CPUs. Experimental\nresults show that Penance consumes a negligible 6.8% more computation resources\nthan the optimal strategy while satisfying accuracy and bandwidth constraints\nwith a low failure rate.\n","authors":["Mingxuan Yan","Yi Wang","Xuedou Xiao","Zhiqing Luo","Jianhua He","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2402.14326v2.pdf","comment":"Accepted by ACM Multimedia 2023"},{"id":"http://arxiv.org/abs/2403.18323v1","updated":"2024-03-27T07:52:51Z","published":"2024-03-27T07:52:51Z","title":"How to Cache Important Contents for Multi-modal Service in Dynamic\n  Networks: A DRL-based Caching Scheme","summary":"  With the continuous evolution of networking technologies, multi-modal\nservices that involve video, audio, and haptic contents are expected to become\nthe dominant multimedia service in the near future. Edge caching is a key\ntechnology that can significantly reduce network load and content transmission\nlatency, which is critical for the delivery of multi-modal contents. However,\nexisting caching approaches only rely on a limited number of factors, e.g.,\npopularity, to evaluate their importance for caching, which is inefficient for\ncaching multi-modal contents, especially in dynamic network environments. To\novercome this issue, we propose a content importance-based caching scheme which\nconsists of a content importance evaluation model and a caching model. By\nleveraging dueling double deep Q networks (D3QN) model, the content importance\nevaluation model can adaptively evaluate contents' importance in dynamic\nnetworks. Based on the evaluated contents' importance, the caching model can\neasily cache and evict proper contents to improve caching efficiency. The\nsimulation results show that the proposed content importance-based caching\nscheme outperforms existing caching schemes in terms of caching hit ratio (at\nleast 15% higher), reduced network load (up to 22% reduction), average number\nof hops (up to 27% lower), and unsatisfied requests ratio (more than 47%\nreduction).\n","authors":["Zhe Zhang","Marc St-Hilaire","Xin Wei","Haiwei Dong","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2403.18323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.10066v3","updated":"2024-03-27T02:25:51Z","published":"2024-03-15T07:16:07Z","title":"Contrastive Pre-Training with Multi-View Fusion for No-Reference Point\n  Cloud Quality Assessment","summary":"  No-reference point cloud quality assessment (NR-PCQA) aims to automatically\nevaluate the perceptual quality of distorted point clouds without available\nreference, which have achieved tremendous improvements due to the utilization\nof deep neural networks. However, learning-based NR-PCQA methods suffer from\nthe scarcity of labeled data and usually perform suboptimally in terms of\ngeneralization. To solve the problem, we propose a novel contrastive\npre-training framework tailored for PCQA (CoPA), which enables the pre-trained\nmodel to learn quality-aware representations from unlabeled data. To obtain\nanchors in the representation space, we project point clouds with different\ndistortions into images and randomly mix their local patches to form mixed\nimages with multiple distortions. Utilizing the generated anchors, we constrain\nthe pre-training process via a quality-aware contrastive loss following the\nphilosophy that perceptual quality is closely related to both content and\ndistortion. Furthermore, in the model fine-tuning stage, we propose a\nsemantic-guided multi-view fusion module to effectively integrate the features\nof projected images from multiple perspectives. Extensive experiments show that\nour method outperforms the state-of-the-art PCQA methods on popular benchmarks.\nFurther investigations demonstrate that CoPA can also benefit existing\nlearning-based PCQA models.\n","authors":["Ziyu Shan","Yujie Zhang","Qi Yang","Haichen Yang","Yiling Xu","Jenq-Neng Hwang","Xiaozhong Xu","Shan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10066v3.pdf","comment":null}]},"2024-03-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.09887v2","updated":"2024-03-26T23:52:35Z","published":"2024-03-14T21:44:48Z","title":"Sabiá-2: A New Generation of Portuguese Large Language Models","summary":"  We introduce Sabi\\'a-2, a family of large language models trained on\nPortuguese texts. The models are evaluated on a diverse range of exams,\nincluding entry-level tests for Brazilian universities, professional\ncertification exams, and graduate-level exams for various disciplines such as\naccounting, economics, engineering, law and medicine. Our results reveal that\nour best model so far, Sabi\\'a-2 Medium, matches or surpasses GPT-4's\nperformance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64\nexams. Notably, specialization has a significant impact on a model's\nperformance without the need to increase its size, allowing us to offer\nSabi\\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.\nFinally, we identified that math and coding are key abilities that need\nimprovement.\n","authors":["Thales Sales Almeida","Hugo Abonizio","Rodrigo Nogueira","Ramon Pires"],"pdf_url":"https://arxiv.org/pdf/2403.09887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18159v1","updated":"2024-03-26T23:51:44Z","published":"2024-03-26T23:51:44Z","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models","summary":"  Large generative models, such as large language models (LLMs) and diffusion\nmodels have as revolutionized the fields of NLP and computer vision\nrespectively. However, their slow inference, high computation and memory\nrequirement makes it challenging to deploy them on edge devices. In this study,\nwe propose a light-weight quantization aware fine tuning technique using\nknowledge distillation (KD-QAT) to improve the performance of 4-bit weight\nquantized LLMs using commonly available datasets to realize a popular language\nuse case, on device chat applications. To improve this paradigm of finetuning,\nas main contributions, we provide insights into stability of KD-QAT by\nempirically studying the gradient propagation during training to better\nunderstand the vulnerabilities of KD-QAT based approaches to low-bit\nquantization errors. Based on our insights, we propose ov-freeze, a simple\ntechnique to stabilize the KD-QAT process. Finally, we experiment with the\npopular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that\nov-freeze results in near float-point precision performance, i.e., less than\n0.7% loss of accuracy on Commonsense Reasoning benchmarks.\n","authors":["Kartikeya Bhardwaj","Nilesh Prasad Pandey","Sweta Priyadarshi","Kyunggeun Lee","Jun Ma","Harris Teague"],"pdf_url":"https://arxiv.org/pdf/2403.18159v1.pdf","comment":"Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2403.18152v1","updated":"2024-03-26T23:32:52Z","published":"2024-03-26T23:32:52Z","title":"Large Language Models as Financial Data Annotators: A Study on\n  Effectiveness and Efficiency","summary":"  Collecting labeled datasets in finance is challenging due to scarcity of\ndomain experts and higher cost of employing them. While Large Language Models\n(LLMs) have demonstrated remarkable performance in data annotation tasks on\ngeneral domain datasets, their effectiveness on domain specific datasets\nremains underexplored. To address this gap, we investigate the potential of\nLLMs as efficient data annotators for extracting relations in financial\ndocuments. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,\nand MPT Instruct) against expert annotators and crowdworkers. We demonstrate\nthat the current state-of-the-art LLMs can be sufficient alternatives to\nnon-expert crowdworkers. We analyze models using various prompts and parameter\nsettings and find that customizing the prompts for each relation group by\nproviding specific examples belonging to those groups is paramount.\nFurthermore, we introduce a reliability index (LLM-RelIndex) used to identify\noutputs that may require expert attention. Finally, we perform an extensive\ntime, cost and error analysis and provide recommendations for the collection\nand usage of automated annotations in domain-specific settings.\n","authors":["Toyin Aguda","Suchetha Siddagangappa","Elena Kochkina","Simerjot Kaur","Dongsheng Wang","Charese Smiley","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2403.18152v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18148v1","updated":"2024-03-26T23:14:34Z","published":"2024-03-26T23:14:34Z","title":"Large Language Models Produce Responses Perceived to be Empathic","summary":"  Large Language Models (LLMs) have demonstrated surprising performance on many\ntasks, including writing supportive messages that display empathy. Here, we had\nthese models generate empathic messages in response to posts describing common\nlife experiences, such as workplace situations, parenting, relationships, and\nother anxiety- and anger-eliciting situations. Across two studies (N=192, 202),\nwe showed human raters a variety of responses written by several models (GPT4\nTurbo, Llama2, and Mistral), and had people rate these responses on how\nempathic they seemed to be. We found that LLM-generated responses were\nconsistently rated as more empathic than human-written responses. Linguistic\nanalyses also show that these models write in distinct, predictable ``styles\",\nin terms of their use of punctuation, emojis, and certain words. These results\nhighlight the potential of using LLMs to enhance human peer support in contexts\nwhere empathy is important.\n","authors":["Yoon Kyung Lee","Jina Suh","Hongli Zhan","Junyi Jessy Li","Desmond C. Ong"],"pdf_url":"https://arxiv.org/pdf/2403.18148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09618v2","updated":"2024-03-26T22:59:52Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v2.pdf","comment":"In CVPR, 2024"},{"id":"http://arxiv.org/abs/2310.19055v2","updated":"2024-03-26T22:59:36Z","published":"2023-10-29T16:02:46Z","title":"A Few-Shot Learning Focused Survey on Recent Named Entity Recognition\n  and Relation Classification Methods","summary":"  Named Entity Recognition (NER) and Relation Classification (RC) are important\nsteps in extracting information from unstructured text and formatting it into a\nmachine-readable format. We present a survey of recent deep learning models\nthat address named entity recognition and relation classification, with focus\non few-shot learning performance. Our survey is helpful for researchers in\nknowing the recent techniques in text mining and extracting structured\ninformation from raw text.\n","authors":["Sakher Khalil Alqaaidi","Elika Bozorgi","Afsaneh Shams","Krzysztof Kochut"],"pdf_url":"https://arxiv.org/pdf/2310.19055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05882v2","updated":"2024-03-26T22:54:48Z","published":"2023-06-09T13:24:27Z","title":"Good, but not always Fair: An Evaluation of Gender Bias for three\n  commercial Machine Translation Systems","summary":"  Machine Translation (MT) continues to make significant strides in quality and\nis increasingly adopted on a larger scale. Consequently, analyses have been\nredirected to more nuanced aspects, intricate phenomena, as well as potential\nrisks that may arise from the widespread use of MT tools. Along this line, this\npaper offers a meticulous assessment of three commercial MT systems - Google\nTranslate, DeepL, and Modern MT - with a specific focus on gender translation\nand bias. For three language pairs (English/Spanish, English/Italian, and\nEnglish/French), we scrutinize the behavior of such systems at several levels\nof granularity and on a variety of naturally occurring gender phenomena in\ntranslation. Our study takes stock of the current state of online MT tools, by\nrevealing significant discrepancies in the gender translation of the three\nsystems, with each system displaying varying degrees of bias despite their\noverall translation quality.\n","authors":["Silvia Alma Piazzolla","Beatrice Savoldi","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2306.05882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18140v1","updated":"2024-03-26T22:54:12Z","published":"2024-03-26T22:54:12Z","title":"Juru: Legal Brazilian Large Language Model from Reputable Sources","summary":"  The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.\n","authors":["Roseval Malaquias Junior","Ramon Pires","Roseli Romero","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2403.18140v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.18025v1","updated":"2024-03-26T18:23:16Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v1.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2403.17901v1","updated":"2024-03-26T17:43:08Z","published":"2024-03-26T17:43:08Z","title":"Search and Society: Reimagining Information Access for Radical Futures","summary":"  Information retrieval (IR) technologies and research are undergoing\ntransformative changes. It is our perspective that the community should accept\nthis opportunity to re-center our research agendas on societal needs while\ndismantling the artificial separation between the work on fairness,\naccountability, transparency, and ethics in IR and the rest of IR research.\nInstead of adopting a reactionary strategy of trying to mitigate potential\nsocial harms from emerging technologies, the community should aim to\nproactively set the research agenda for the kinds of systems we should build\ninspired by diverse explicitly stated sociotechnical imaginaries. The\nsociotechnical imaginaries that underpin the design and development of\ninformation access technologies needs to be explicitly articulated, and we need\nto develop theories of change in context of these diverse perspectives. Our\nguiding future imaginaries must be informed by other academic fields, such as\ndemocratic theory and critical theory, and should be co-developed with social\nscience scholars, legal scholars, civil rights and social justice activists,\nand artists, among others. In this perspective paper, we motivate why the\ncommunity must consider this radical shift in how we do research and what we\nwork on, and sketch a path forward towards this transformation.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2403.17901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17876v1","updated":"2024-03-26T17:06:56Z","published":"2024-03-26T17:06:56Z","title":"MIND Your Language: A Multilingual Dataset for Cross-lingual News\n  Recommendation","summary":"  Digital news platforms use news recommenders as the main instrument to cater\nto the individual information needs of readers. Despite an increasingly\nlanguage-diverse online community, in which many Internet users consume news in\nmultiple languages, the majority of news recommendation focuses on major,\nresource-rich languages, and English in particular. Moreover, nearly all news\nrecommendation efforts assume monolingual news consumption, whereas more and\nmore users tend to consume information in at least two languages. Accordingly,\nthe existing body of work on news recommendation suffers from a lack of\npublicly available multilingual benchmarks that would catalyze development of\nnews recommenders effective in multilingual settings and for low-resource\nlanguages. Aiming to fill this gap, we introduce xMIND, an open, multilingual\nnews recommendation dataset derived from the English MIND dataset using machine\ntranslation, covering a set of 14 linguistically and geographically diverse\nlanguages, with digital footprints of varying sizes. Using xMIND, we\nsystematically benchmark several state-of-the-art content-based neural news\nrecommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT)\ncross-lingual transfer scenarios, considering both monolingual and bilingual\nnews consumption patterns. Our findings reveal that (i) current NNRs, even when\nbased on a multilingual language model, suffer from substantial performance\nlosses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT\ntraining has limited benefits, particularly when combined with a bilingual news\nconsumption. Our findings thus warrant a broader research effort in\nmultilingual and cross-lingual news recommendation. The xMIND dataset is\navailable at https://github.com/andreeaiana/xMIND.\n","authors":["Andreea Iana","Goran Glavaš","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2403.17876v1.pdf","comment":"Accepted at the 47th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval (SIGIR 2024)"},{"id":"http://arxiv.org/abs/2403.17848v1","updated":"2024-03-26T16:37:54Z","published":"2024-03-26T16:37:54Z","title":"ArabicaQA: A Comprehensive Dataset for Arabic Question Answering","summary":"  In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.\n","authors":["Abdelrahman Abdallah","Mahmoud Kasem","Mahmoud Abdalla","Mohamed Mahmoud","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17848v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2306.13186v3","updated":"2024-03-26T16:29:17Z","published":"2023-06-22T20:03:09Z","title":"A Decade of Scholarly Research on Open Knowledge Graphs","summary":"  The proliferation of open knowledge graphs has led to a surge in scholarly\nresearch on the topic over the past decade. This paper presents a bibliometric\nanalysis of the scholarly literature on open knowledge graphs published between\n2013 and 2023. The study aims to identify the trends, patterns, and impact of\nresearch in this field, as well as the key topics and research questions that\nhave emerged. The work uses bibliometric techniques to analyze a sample of 4445\nscholarly articles retrieved from Scopus. The findings reveal an\never-increasing number of publications on open knowledge graphs published every\nyear, particularly in developed countries (+50 per year). These outputs are\npublished in highly-referred scholarly journals and conferences. The study\nidentifies three main research themes: (1) knowledge graph construction and\nenrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into\nNLP systems. Within these themes, the study identifies specific tasks that have\nreceived considerable attention, including entity linking, knowledge graph\nembedding, and graph neural networks.\n","authors":["Houcemeddine Turki","Abraham Toluwase Owodunni","Mohamed Ali Hadj Taieb","René Fabrice Bile","Mohamed Ben Aouicha"],"pdf_url":"https://arxiv.org/pdf/2306.13186v3.pdf","comment":"Camera-ready edition for LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17780v1","updated":"2024-03-26T15:13:16Z","published":"2024-03-26T15:13:16Z","title":"CaseLink: Inductive Graph Learning for Legal Case Retrieval","summary":"  In case law, the precedents are the relevant cases that are used to support\nthe decisions made by the judges and the opinions of lawyers towards a given\ncase. This relevance is referred to as the case-to-case reference relation. To\nefficiently find relevant cases from a large case pool, retrieval tools are\nwidely used by legal practitioners. Existing legal case retrieval models mainly\nwork by comparing the text representations of individual cases. Although they\nobtain a decent retrieval accuracy, the intrinsic case connectivity\nrelationships among cases have not been well exploited for case encoding,\ntherefore limiting the further improvement of retrieval performance. In a case\npool, there are three types of case connectivity relationships: the case\nreference relationship, the case semantic relationship, and the case legal\ncharge relationship. Due to the inductive manner in the task of legal case\nretrieval, using case reference as input is not applicable for testing. Thus,\nin this paper, a CaseLink model based on inductive graph learning is proposed\nto utilise the intrinsic case connectivity for legal case retrieval, a novel\nGlobal Case Graph is incorporated to represent both the case semantic\nrelationship and the case legal charge relationship. A novel contrastive\nobjective with a regularisation on the degree of case nodes is proposed to\nleverage the information carried by the case reference relationship to optimise\nthe model. Extensive experiments have been conducted on two benchmark datasets,\nwhich demonstrate the state-of-the-art performance of CaseLink. The code has\nbeen released on https://github.com/yanran-tang/CaseLink.\n","authors":["Yanran Tang","Ruihong Qiu","Hongzhi Yin","Xue Li","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17759v1","updated":"2024-03-26T14:51:03Z","published":"2024-03-26T14:51:03Z","title":"TWOLAR: a TWO-step LLM-Augmented distillation method for passage\n  Reranking","summary":"  In this paper, we present TWOLAR: a two-stage pipeline for passage reranking\nbased on the distillation of knowledge from Large Language Models (LLM). TWOLAR\nintroduces a new scoring strategy and a distillation process consisting in the\ncreation of a novel and diverse training dataset. The dataset consists of 20K\nqueries, each associated with a set of documents retrieved via four distinct\nretrieval methods to ensure diversity, and then reranked by exploiting the\nzero-shot reranking capabilities of an LLM. Our ablation studies demonstrate\nthe contribution of each new component we introduced. Our experimental results\nshow that TWOLAR significantly enhances the document reranking ability of the\nunderlying model, matching and in some cases even outperforming\nstate-of-the-art models with three orders of magnitude more parameters on the\nTREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate\nfuture work we release our data set, finetuned models, and code.\n","authors":["Davide Baldelli","Junfeng Jiang","Akiko Aizawa","Paolo Torroni"],"pdf_url":"https://arxiv.org/pdf/2403.17759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17740v1","updated":"2024-03-26T14:29:34Z","published":"2024-03-26T14:29:34Z","title":"All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating\n  Prediction","summary":"  Cold-start rating prediction is a fundamental problem in recommender systems\nthat has been extensively studied. Many methods have been proposed that exploit\nexplicit relations among existing data, such as collaborative filtering, social\nrecommendations and heterogeneous information network, to alleviate the data\ninsufficiency issue for cold-start users and items. However, the explicit\nrelations constructed based on data between different roles may be unreliable\nand irrelevant, which limits the performance ceiling of the specific\nrecommendation task. Motivated by this, in this paper, we propose a flexible\nframework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not\nsolely rely on the pre-defined interaction pattern or the manually constructed\nheterogeneous information network. Instead, we devise a Heterogeneous\nInteraction Module (HIM) to jointly model the heterogeneous interactions and\ndirectly infer the important interactions via the observed data. In the\nexperiments, we evaluate our model under three cold-start settings on three\nreal-world datasets. The experimental results show that HIRE outperforms other\nbaselines by a large margin. Furthermore, we visualize the inferred\ninteractions of HIRE to confirm the contribution of our model.\n","authors":["Shuheng Fang","Kangfei Zhao","Yu Rong","Zhixun Li","Jeffrey Xu Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17740v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2309.17078v2","updated":"2024-03-26T14:27:59Z","published":"2023-09-29T09:14:53Z","title":"Unsupervised Large Language Model Alignment for Information Retrieval\n  via Contrastive Feedback","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious research domains, including the field of Information Retrieval (IR).\nHowever, the responses generated by off-the-shelf LLMs tend to be generic,\ni.e., cannot capture the distinctiveness of each document with similar content.\nThis limits the performance of LLMs in IR because finding and distinguishing\nrelevant documents from substantial similar documents is a typical problem in\nmany IR tasks. To address this issue, we propose an unsupervised alignment\nmethod, namely Reinforcement Learning from Contrastive Feedback (RLCF),\nempowering LLMs to generate both high-quality and context-specific responses.\nOur approach constructs unsupervised contrastive feedback signals based on\nsimilar document groups, and adopts a reward function, named group-wise\nreciprocal rank, to optimize LLMs within a standard Proximal Policy\nOptimization. We conduct extensive experiments to evaluate the effectiveness of\nRLCF on LLMs built with different languages and parameter sizes on multiple\ndownstream IR applications. RLCF significantly outperforms existing alignment\nmethods, and RLCF-optimized LLMs demonstrate considerable improvement in\ngenerating responses with distinctiveness.\n","authors":["Qian Dong","Yiding Liu","Qingyao Ai","Zhijing Wu","Haitao Li","Yiqun Liu","Shuaiqiang Wang","Dawei Yin","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2309.17078v2.pdf","comment":"Accepted by SIGIR24"},{"id":"http://arxiv.org/abs/2403.17729v1","updated":"2024-03-26T14:18:43Z","published":"2024-03-26T14:18:43Z","title":"EulerFormer: Sequential User Behavior Modeling with Complex Vector\n  Attention","summary":"  To capture user preference, transformer models have been widely applied to\nmodel sequential user behavior data. The core of transformer architecture lies\nin the self-attention mechanism, which computes the pairwise attention scores\nin a sequence. Due to the permutation-equivariant nature, positional encoding\nis used to enhance the attention between token representations. In this\nsetting, the pairwise attention scores can be derived by both semantic\ndifference and positional difference. However, prior studies often model the\ntwo kinds of difference measurements in different ways, which potentially\nlimits the expressive capacity of sequence modeling. To address this issue,\nthis paper proposes a novel transformer variant with complex vector attention,\nnamed EulerFormer, which provides a unified theoretical framework to formulate\nboth semantic difference and positional difference. The EulerFormer involves\ntwo key technical improvements. First, it employs a new transformation function\nfor efficiently transforming the sequence tokens into polar-form complex\nvectors using Euler's formula, enabling the unified modeling of both semantic\nand positional information in a complex rotation form.Secondly, it develops a\ndifferential rotation mechanism, where the semantic rotation angles can be\ncontrolled by an adaptation function, enabling the adaptive integration of the\nsemantic and positional information according to the semantic\ncontexts.Furthermore, a phase contrastive learning task is proposed to improve\nthe anisotropy of contextual representations in EulerFormer. Our theoretical\nframework possesses a high degree of completeness and generality. It is more\nrobust to semantic variations and possesses moresuperior theoretical properties\nin principle. Extensive experiments conducted on four public datasets\ndemonstrate the effectiveness and efficiency of our approach.\n","authors":["Zhen Tian","Wayne Xin Zhao","Changwang Zhang","Xin Zhao","Zhongrui Ma","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.17729v1.pdf","comment":"Accepted for publication in SIGIR'24"},{"id":"http://arxiv.org/abs/2403.17688v1","updated":"2024-03-26T13:31:33Z","published":"2024-03-26T13:31:33Z","title":"Large Language Models Enhanced Collaborative Filtering","summary":"  Recent advancements in Large Language Models (LLMs) have attracted\nconsiderable interest among researchers to leverage these models to enhance\nRecommender Systems (RSs). Existing work predominantly utilizes LLMs to\ngenerate knowledge-rich texts or utilizes LLM-derived embeddings as features to\nimprove RSs. Although the extensive world knowledge embedded in LLMs generally\nbenefits RSs, the application can only take limited number of users and items\nas inputs, without adequately exploiting collaborative filtering information.\nConsidering its crucial role in RSs, one key challenge in enhancing RSs with\nLLMs lies in providing better collaborative filtering information through LLMs.\nIn this paper, drawing inspiration from the in-context learning and chain of\nthought reasoning in LLMs, we propose the Large Language Models enhanced\nCollaborative Filtering (LLM-CF) framework, which distils the world knowledge\nand reasoning capabilities of LLMs into collaborative filtering. We also\nexplored a concise and efficient instruction-tuning method, which improves the\nrecommendation capabilities of LLMs while preserving their general\nfunctionalities (e.g., not decreasing on the LLM benchmark). Comprehensive\nexperiments on three real-world datasets demonstrate that LLM-CF significantly\nenhances several backbone recommendation models and consistently outperforms\ncompetitive baselines, showcasing its effectiveness in distilling the world\nknowledge and reasoning capabilities of LLM into collaborative filtering.\n","authors":["Zhongxiang Sun","Zihua Si","Xiaoxue Zang","Kai Zheng","Yang Song","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17688v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.17643v1","updated":"2024-03-26T12:23:34Z","published":"2024-03-26T12:23:34Z","title":"S+t-SNE -- Bringing dimensionality reduction to data streams","summary":"  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. Employing a blind method for drift management\nadjusts the embedding space, facilitating continuous visualisation of evolving\ndata dynamics. Our experimental evaluations demonstrate the effectiveness and\nefficiency of S+t-SNE. The results highlight its ability to capture patterns in\na streaming scenario. We hope our approach offers researchers and practitioners\na real-time tool for understanding and interpreting high-dimensional data.\n","authors":["Pedro C. Vieira","João P. Montrezol","João T. Vieira","João Gama"],"pdf_url":"https://arxiv.org/pdf/2403.17643v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. We will soon add a link to the final version of\n  this contribution that underwent peer-review and post-acceptance improvements\n  and was presented at IDA2024 (https://ida2024.org/)"},{"id":"http://arxiv.org/abs/2403.17634v1","updated":"2024-03-26T12:08:58Z","published":"2024-03-26T12:08:58Z","title":"Retentive Decision Transformer with Adaptive Masking for Reinforcement\n  Learning based Recommendation Systems","summary":"  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise\nacross a spectrum of applications, from e-commerce platforms to streaming\nservices. Yet, they grapple with challenges, notably in crafting reward\nfunctions and harnessing large pre-existing datasets within the RL framework.\nRecent advancements in offline RLRS provide a solution for how to address these\ntwo challenges. However, existing methods mainly rely on the transformer\narchitecture, which, as sequence lengths increase, can introduce challenges\nassociated with computational resources and training costs. Additionally, the\nprevalent methods employ fixed-length input trajectories, restricting their\ncapacity to capture evolving user preferences. In this study, we introduce a\nnew offline RLRS method to deal with the above problems. We reinterpret the\nRLRS challenge by modeling sequential decision-making as an inference task,\nleveraging adaptive masking configurations. This adaptive approach selectively\nmasks input tokens, transforming the recommendation task into an inference\nchallenge based on varying token subsets, thereby enhancing the agent's ability\nto infer across diverse trajectory lengths. Furthermore, we incorporate a\nmulti-scale segmented retention mechanism that facilitates efficient modeling\nof long sequences, significantly enhancing computational efficiency. Our\nexperimental analysis, conducted on both online simulator and offline datasets,\nclearly demonstrates the advantages of our proposed method.\n","authors":["Siyu Wang","Xiaocong Chen","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17603v1","updated":"2024-03-26T11:28:31Z","published":"2024-03-26T11:28:31Z","title":"END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential\n  Recommendation","summary":"  In recommendation systems, users frequently engage in multiple types of\nbehaviors, such as clicking, adding to a cart, and purchasing. However, with\ndiversified behavior data, user behavior sequences will become very long in the\nshort term, which brings challenges to the efficiency of the sequence\nrecommendation model. Meanwhile, some behavior data will also bring inevitable\nnoise to the modeling of user interests. To address the aforementioned issues,\nfirstly, we develop the Efficient Behavior Sequence Miner (EBM) that\nefficiently captures intricate patterns in user behavior while maintaining low\ntime complexity and parameter count. Secondly, we design hard and soft\ndenoising modules for different noise types and fully explore the relationship\nbetween behaviors and noise. Finally, we introduce a contrastive loss function\nalong with a guided training strategy to compare the valid information in the\ndata with the noisy signal, and seamlessly integrate the two denoising\nprocesses to achieve a high degree of decoupling of the noisy signal.\nSufficient experiments on real-world datasets demonstrate the effectiveness and\nefficiency of our approach in dealing with multi-behavior sequential\nrecommendation.\n","authors":["Yongqiang Han","Hao Wang","Kefan Wang","Likang Wu","Zhi Li","Wei Guo","Yong Liu","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10909v2","updated":"2024-03-26T09:26:32Z","published":"2023-01-26T02:38:59Z","title":"Optimizing Feature Set for Click-Through Rate Prediction","summary":"  Click-through prediction (CTR) models transform features into latent vectors\nand enumerate possible feature interactions to improve performance based on the\ninput feature set. Therefore, when selecting an optimal feature set, we should\nconsider the influence of both feature and its interaction. However, most\nprevious works focus on either feature field selection or only select feature\ninteraction based on the fixed feature set to produce the feature set. The\nformer restricts search space to the feature field, which is too coarse to\ndetermine subtle features. They also do not filter useless feature\ninteractions, leading to higher computation costs and degraded model\nperformance. The latter identifies useful feature interaction from all\navailable features, resulting in many redundant features in the feature set. In\nthis paper, we propose a novel method named OptFS to address these problems. To\nunify the selection of feature and its interaction, we decompose the selection\nof each feature interaction into the selection of two correlated features. Such\na decomposition makes the model end-to-end trainable given various feature\ninteraction operations. By adopting feature-level search space, we set a\nlearnable gate to determine whether each feature should be within the feature\nset. Because of the large-scale search space, we develop a\nlearning-by-continuation training scheme to learn such gates. Hence, OptFS\ngenerates the feature set only containing features which improve the final\nprediction results. Experimentally, we evaluate OptFS on three public datasets,\ndemonstrating OptFS can optimize feature sets which enhance the model\nperformance and further reduce both the storage and computational cost.\n","authors":["Fuyuan Lyu","Xing Tang","Dugang Liu","Liang Chen","Xiuqiang He","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2301.10909v2.pdf","comment":"Accepted by WWW 2023 Research Tracks"},{"id":"http://arxiv.org/abs/2311.08744v2","updated":"2024-03-26T08:14:22Z","published":"2023-11-15T07:25:14Z","title":"Graph Signal Diffusion Model for Collaborative Filtering","summary":"  Collaborative filtering is a critical technique in recommender systems. Among\nvarious methods, an increasingly popular paradigm is to reconstruct user-item\ninteractions based on the historical observations. This can be viewed as a\nconditional generative task, where recently developed diffusion model\ndemonstrates great potential. However, existing studies on diffusion models\nlack effective solutions for modeling implicit feedback data. Particularly, the\nisotropic nature of the standard diffusion process fails to account for the\nheterogeneous dependencies among items, leading to a misalignment with the\ngraphical structure of the interaction space. Meanwhile, random noise\ndestroying personalized information in interaction vectors, causing difficulty\nin reverse reconstruction. In this paper, we make novel adaptions of diffusion\nmodel and propose Graph Signal Diffusion Model for Collaborative Filtering\n(named GiffCF). To better represent the high-dimensional and sparse\ndistribution of implicit feedback, we define a generalized form of denoising\ndiffusion using heat equation on the item-item similarity graph. Our forward\nprocess smooths interaction signals with an advanced family of graph filters.\nHence, instead of losing information, it involves item-item similarities as\nbeneficial prior knowledge for recommendation. To reconstruct high-quality\ninteractions, our reverse process iteratively refines and sharpens preference\nsignals in a deterministic manner, where the update direction is conditioned on\nthe user history and computed from a carefully designed two-stage denoiser.\nFinally, through extensive experiments, we show that GiffCF effectively\nleverages the advantages of both diffusion model and graph signal processing,\nand achieves state-of-the-art performance on three benchmark datasets.\n","authors":["Yunqin Zhu","Chao Wang","Qi Zhang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2311.08744v2.pdf","comment":"11 pages, 8 figures, Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17473v1","updated":"2024-03-26T08:07:33Z","published":"2024-03-26T08:07:33Z","title":"Document Set Expansion with Positive-Unlabelled Learning Using\n  Intractable Density Estimation","summary":"  The Document Set Expansion (DSE) task involves identifying relevant documents\nfrom large collections based on a limited set of example documents. Previous\nresearch has highlighted Positive and Unlabeled (PU) learning as a promising\napproach for this task. However, most PU methods rely on the unrealistic\nassumption of knowing the class prior for positive samples in the collection.\nTo address this limitation, this paper introduces a novel PU learning framework\nthat utilizes intractable density estimation models. Experiments conducted on\nPubMed and Covid datasets in a transductive setting showcase the effectiveness\nof the proposed method for DSE. Code is available from\nhttps://github.com/Beautifuldog01/Document-set-expansion-puDE.\n","authors":["Haiyang Zhang","Qiuyi Chen","Yuanjie Zou","Yushan Pan","Jia Wang","Mark Stevenson"],"pdf_url":"https://arxiv.org/pdf/2403.17473v1.pdf","comment":"Accepted at LREC-COLING 2024. arXiv admin note: text overlap with\n  arXiv:2401.11145"},{"id":"http://arxiv.org/abs/2403.17442v1","updated":"2024-03-26T07:19:26Z","published":"2024-03-26T07:19:26Z","title":"Touch the Core: Exploring Task Dependence Among Hybrid Targets for\n  Recommendation","summary":"  As user behaviors become complicated on business platforms, online\nrecommendations focus more on how to touch the core conversions, which are\nhighly related to the interests of platforms. These core conversions are\nusually continuous targets, such as \\textit{watch time}, \\textit{revenue}, and\nso on, whose predictions can be enhanced by previous discrete conversion\nactions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to\nlearn these hybrid targets. However, existing works mainly emphasize\ninvestigating the sequential dependence among discrete conversion actions,\nwhich neglects the complexity of dependence between discrete conversions and\nthe final continuous conversion. Moreover, simultaneously optimizing hybrid\ntasks with stronger task dependence will suffer from volatile issues where the\ncore regression task might have a larger influence on other tasks. In this\npaper, we study the MTL problem with hybrid targets for the first time and\npropose the model named Hybrid Targets Learning Network (HTLNet) to explore\ntask dependence and enhance optimization. Specifically, we introduce label\nembedding for each task to explicitly transfer the label information among\nthese tasks, which can effectively explore logical task dependence. We also\nfurther design the gradient adjustment regime between the final regression task\nand other classification tasks to enhance the optimization. Extensive\nexperiments on two offline public datasets and one real-world industrial\ndataset are conducted to validate the effectiveness of HTLNet. Moreover, online\nA/B tests on the financial recommender system also show our model has superior\nimprovement.\n","authors":["Xing Tang","Yang Qiao","Fuyuan Lyu","Dugang Liu","Xiuqiang He"],"pdf_url":"https://arxiv.org/pdf/2403.17442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.01262v2","updated":"2024-03-26T06:54:43Z","published":"2022-07-04T08:54:43Z","title":"Understanding Performance of Long-Document Ranking Models through\n  Comprehensive Evaluation and Leaderboarding","summary":"  We evaluated 20+ Transformer models for ranking of long documents (including\nrecent LongP models trained with FlashAttention) and compared them with simple\nFirstP baselines (applying the same model to input truncated to the first 512\ntokens). We used MS MARCO Documents v1 as a primary training set and evaluated\nmodels in the zero-shot scenario as well as after fine-tuning on other\ncollections.\n  In our initial experiments with standard collections we found that\nlong-document models underperformed FirstP or outperformed it by at most 5% on\naverage in terms of MRR or NDCG. We then conjectured that this was not due to\nmodels inability to process long context but rather due to a positional bias of\nrelevant passages, which tended to be among the first 512 document tokens. We\nfound evidence that this bias was, indeed, present in at least two test sets,\nwhich motivated us to create a new collection MS MARCO FarRelevant where the\nrelevant passages were not present among the first 512 tokens.\n  Unlike standard collections where we observed both little benefit from\nincorporating longer contexts and limited variability in model performance\n(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic\ndifferences among models. FirstP models performed roughly at the\nrandom-baseline level in both zero-shot and fine-tuning scenarios. Simple\naggregation models (e.g., MaxP) had good zero-shot accuracy but benefited\nlittle from fine-tuning. Most other models had poor zero-shot performance\n(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\\%\nafter finetuning. Thus, positional bias not only diminishes benefits of\nprocessing longer document contexts but also leads to model overfitting to this\nbias and performing poorly in a zero-shot setting when a distribution of\nrelevant passages changes substantially.\n  We make our software and MS MARCO FarRelevant available.\n","authors":["Leonid Boytsov","David Akinpelu","Tianyi Lin","Fangwei Gao","Yutian Zhao","Jeffrey Huang","Eric Nyberg"],"pdf_url":"https://arxiv.org/pdf/2207.01262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17425v1","updated":"2024-03-26T06:42:23Z","published":"2024-03-26T06:42:23Z","title":"Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion\n  Rate Prediction with a Single Model","summary":"  In real-world advertising systems, conversions have different types in nature\nand ads can be shown in different display scenarios, both of which highly\nimpact the actual conversion rate (CVR). This results in the multi-type and\nmulti-scenario CVR prediction problem. A desired model for this problem should\nsatisfy the following requirements: 1) Accuracy: the model should achieve\nfine-grained accuracy with respect to any conversion type in any display\nscenario. 2) Scalability: the model parameter size should be affordable. 3)\nConvenience: the model should not require a large amount of effort in data\npartitioning, subset processing and separate storage. Existing approaches\ncannot simultaneously satisfy these requirements. For example, building a\nseparate model for each (conversion type, display scenario) pair is neither\nscalable nor convenient. Building a unified model trained on all the data with\nconversion type and display scenario included as two features is not accurate\nenough. In this paper, we propose the Masked Multi-domain Network (MMN) to\nsolve this problem. To achieve the accuracy requirement, we model\ndomain-specific parameters and propose a dynamically weighted loss to account\nfor the loss scale imbalance issue within each mini-batch. To achieve the\nscalability requirement, we propose a parameter sharing and composition\nstrategy to reduce model parameters from a product space to a sum space. To\nachieve the convenience requirement, we propose an auto-masking strategy which\ncan take mixed data from all the domains as input. It avoids the overhead\ncaused by data partitioning, individual processing and separate storage. Both\noffline and online experimental results validate the superiority of MMN for\nmulti-type and multi-scenario CVR prediction. MMN is now the serving model for\nreal-time CVR prediction in UC Toutiao.\n","authors":["Wentao Ouyang","Xiuwu Zhang","Chaofeng Guo","Shukui Ren","Yupei Sui","Kun Zhang","Jinmei Luo","Yunfeng Chen","Dongbo Xu","Xiangzheng Liu","Yanlong Du"],"pdf_url":"https://arxiv.org/pdf/2403.17425v1.pdf","comment":"CIKM 2023 (larger figures)"},{"id":"http://arxiv.org/abs/2403.17416v1","updated":"2024-03-26T06:14:19Z","published":"2024-03-26T06:14:19Z","title":"AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering\n  for Recommendations","summary":"  Collaborative filtering methods based on graph neural networks (GNNs) have\nwitnessed significant success in recommender systems (RS), capitalizing on\ntheir ability to capture collaborative signals within intricate user-item\nrelationships via message-passing mechanisms. However, these GNN-based RS\ninadvertently introduce excess linear correlation between user and item\nembeddings, contradicting the goal of providing personalized recommendations.\nWhile existing research predominantly ascribes this flaw to the over-smoothing\nproblem, this paper underscores the critical, often overlooked role of the\nover-correlation issue in diminishing the effectiveness of GNN representations\nand subsequent recommendation performance. Up to now, the over-correlation\nissue remains unexplored in RS. Meanwhile, how to mitigate the impact of\nover-correlation while preserving collaborative filtering signals is a\nsignificant challenge. To this end, this paper aims to address the\naforementioned gap by undertaking a comprehensive study of the over-correlation\nissue in graph collaborative filtering models. Firstly, we present empirical\nevidence to demonstrate the widespread prevalence of over-correlation in these\nmodels. Subsequently, we dive into a theoretical analysis which establishes a\npivotal connection between the over-correlation and over-smoothing issues.\nLeveraging these insights, we introduce the Adaptive Feature De-correlation\nGraph Collaborative Filtering (AFDGCF) framework, which dynamically applies\ncorrelation penalties to the feature dimensions of the representation matrix,\neffectively alleviating both over-correlation and over-smoothing issues. The\nefficacy of the proposed framework is corroborated through extensive\nexperiments conducted with four representative graph collaborative filtering\nmodels across four publicly available datasets.\n","authors":["Wei Wu","Chao Wang","Dazhong Shen","Chuan Qin","Liyi Chen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.17416v1.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2403.17374v1","updated":"2024-03-26T04:30:40Z","published":"2024-03-26T04:30:40Z","title":"Multi-Domain Recommendation to Attract Users via Domain Preference\n  Modeling","summary":"  Recently, web platforms have been operating various service domains\nsimultaneously. Targeting a platform that operates multiple service domains, we\nintroduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU),\nwhich recommends items from multiple ``unseen'' domains with which each user\nhas not interacted yet, by using knowledge from the user's ``seen'' domains. In\nthis paper, we point out two challenges of MDRAU task. First, there are\nnumerous possible combinations of mappings from seen to unseen domains because\nusers have usually interacted with a different subset of service domains.\nSecond, a user might have different preferences for each of the target unseen\ndomains, which requires that recommendations reflect the user's preferences on\ndomains as well as items. To tackle these challenges, we propose DRIP framework\nthat models users' preferences at two levels (i.e., domain and item) and learns\nvarious seen-unseen domain mappings in a unified way with masked domain\nmodeling. Our extensive experiments demonstrate the effectiveness of DRIP in\nMDRAU task and its ability to capture users' domain-level preferences.\n","authors":["Hyuunjun Ju","SeongKu Kang","Dongha Lee","Junyoung Hwang","Sanghwan Jang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17374v1.pdf","comment":"Accepted to AAAI'24"},{"id":"http://arxiv.org/abs/2403.17372v1","updated":"2024-03-26T04:16:57Z","published":"2024-03-26T04:16:57Z","title":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders","summary":"  Sequential Recommendation (SR) aims to predict future user-item interactions\nbased on historical interactions. While many SR approaches concentrate on user\nIDs and item IDs, the human perception of the world through multi-modal\nsignals, like text and images, has inspired researchers to delve into\nconstructing SR from multi-modal information without using IDs. However, the\ncomplexity of multi-modal learning manifests in diverse feature extractors,\nfusion methods, and pre-trained models. Consequently, designing a simple and\nuniversal \\textbf{M}ulti-\\textbf{M}odal \\textbf{S}equential\n\\textbf{R}ecommendation (\\textbf{MMSR}) framework remains a formidable\nchallenge. We systematically summarize the existing multi-modal related SR\nmethods and distill the essence into four core components: visual encoder, text\nencoder, multimodal fusion module, and sequential architecture. Along these\ndimensions, we dissect the model designs, and answer the following\nsub-questions: First, we explore how to construct MMSR from scratch, ensuring\nits performance either on par with or exceeds existing SR methods without\ncomplex techniques. Second, we examine if MMSR can benefit from existing\nmulti-modal pre-training paradigms. Third, we assess MMSR's capability in\ntackling common challenges like cold start and domain transferring. Our\nexperiment results across four real-world recommendation scenarios demonstrate\nthe great potential ID-agnostic multi-modal sequential recommendation. Our\nframework can be found at: https://github.com/MMSR23/MMSR.\n","authors":["Youhua Li","Hanwen Du","Yongxin Ni","Yuanqi He","Junchen Fu","Xiangyan Liu","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.17372v1.pdf","comment":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders"},{"id":"http://arxiv.org/abs/2403.09963v2","updated":"2024-03-26T04:08:47Z","published":"2024-03-15T02:04:35Z","title":"Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias\n  in Factual Knowledge Extraction","summary":"  Recent research shows that pre-trained language models (PLMs) suffer from\n\"prompt bias\" in factual knowledge extraction, i.e., prompts tend to introduce\nbiases toward specific labels. Prompt bias presents a significant challenge in\nassessing the factual knowledge within PLMs. Therefore, this paper aims to\nimprove the reliability of existing benchmarks by thoroughly investigating and\nmitigating prompt bias. We show that: 1) all prompts in the experiments exhibit\nnon-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt\ndisplaying significantly higher levels of bias; 2) prompt bias can amplify\nbenchmark accuracy unreasonably by overfitting the test datasets, especially on\nimbalanced datasets like LAMA. Based on these findings, we propose a\nrepresentation-based approach to mitigate the prompt bias during inference\ntime. Specifically, we first estimate the biased representation using\nprompt-only querying, and then remove it from the model's internal\nrepresentations to generate the debiased representations, which are used to\nproduce the final debiased outputs. Experiments across various prompts, PLMs,\nand benchmarks show that our approach can not only correct the overfitted\nperformance caused by prompt bias, but also significantly improve the prompt\nretrieval capability (up to 10% absolute performance gain). These results\nindicate that our approach effectively alleviates prompt bias in knowledge\nevaluation, thereby enhancing the reliability of benchmark assessments.\nHopefully, our plug-and-play approach can be a golden standard to strengthen\nPLMs toward reliable knowledge bases. Code and data are released in\nhttps://github.com/FelliYang/PromptBias.\n","authors":["Ziyang Xu","Keqin Peng","Liang Ding","Dacheng Tao","Xiliang Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09963v2.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2112.06460v5","updated":"2024-03-26T03:44:29Z","published":"2021-12-13T07:33:28Z","title":"Improving Sequential Recommendations via Bidirectional Temporal Data\n  Augmentation with Pre-training","summary":"  Sequential recommendation systems are integral to discerning temporal user\npreferences. Yet, the task of learning from abbreviated user interaction\nsequences poses a notable challenge. Data augmentation has been identified as a\npotent strategy to enhance the informational richness of these sequences.\nTraditional augmentation techniques, such as item randomization, may disrupt\nthe inherent temporal dynamics. Although recent advancements in reverse\nchronological pseudo-item generation have shown promise, they can introduce\ntemporal discrepancies when assessed in a natural chronological context. In\nresponse, we introduce a sophisticated approach, Bidirectional temporal data\nAugmentation with pre-training (BARec). Our approach leverages bidirectional\ntemporal augmentation and knowledge-enhanced fine-tuning to synthesize\nauthentic pseudo-prior items that \\emph{retain user preferences and capture\ndeeper item semantic correlations}, thus boosting the model's expressive power.\nOur comprehensive experimental analysis confirms the superiority of BARec\nacross both short and elongated sequence contexts. Moreover, theoretical\nexamination and visual representation of item embeddings offer further insight\ninto the model's logical processes and interpretability. The source code for\nour study is available at\n\\textcolor{blue}{\\href{https://github.com/juyongjiang/BARec}{https://github.com/juyongjiang/BARec}}.\n","authors":["Juyong Jiang","Peiyan Zhang","Yingtao Luo","Chaozhuo Li","Jaeboum Kim","Kai Zhang","Senzhang Wang","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2112.06460v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17286v1","updated":"2024-03-26T00:29:52Z","published":"2024-03-26T00:29:52Z","title":"Cognitively Biased Users Interacting with Algorithmically Biased Results\n  in Whole-Session Search on Controversial Topics","summary":"  When interacting with information retrieval (IR) systems, users, affected by\nconfirmation biases, tend to select search results that confirm their existing\nbeliefs on socially significant contentious issues. To understand the judgments\nand attitude changes of users searching online, our study examined how\ncognitively biased users interact with algorithmically biased search engine\nresult pages (SERPs). We designed three-query search sessions on debated topics\nunder various bias conditions. We recruited 1,321 crowdsourcing participants\nand explored their attitude changes, search interactions, and the effects of\nconfirmation bias. Three key findings emerged: 1) most attitude changes occur\nin the initial query of a search session; 2) confirmation bias and result\npresentation on SERPs affect search behaviors in the current query and\nperceived familiarity with clicked results in subsequent queries. The bias\nposition also affect attitude changes of users with lower perceived openness to\nconflicting opinions; 3) Interactions in the first query and and dwell time\nthroughout the session are associated with users' attitude changes in different\nforms. Our study goes beyond traditional simulation-based evaluation settings\nand simulated rational users, sheds light on the mixed effects of human biases\nand algorithmic biases in controversial information retrieval tasks, and can\ninform the design of bias-aware user models, human-centered bias mitigation\ntechniques, and socially responsible intelligent IR systems.\n","authors":["Ben Wang","Jiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17286v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.18063v1","updated":"2024-03-26T19:29:21Z","published":"2024-03-26T19:29:21Z","title":"Spectral Convolutional Transformer: Harmonizing Real vs. Complex\n  Multi-View Spectral Operators for Vision Transformer","summary":"  Transformers used in vision have been investigated through diverse\narchitectures - ViT, PVT, and Swin. These have worked to improve the attention\nmechanism and make it more efficient. Differently, the need for including local\ninformation was felt, leading to incorporating convolutions in transformers\nsuch as CPVT and CvT. Global information is captured using a complex Fourier\nbasis to achieve global token mixing through various methods, such as AFNO,\nGFNet, and Spectformer. We advocate combining three diverse views of data -\nlocal, global, and long-range dependence. We also investigate the simplest\nglobal representation using only the real domain spectral representation -\nobtained through the Hartley transform. We use a convolutional operator in the\ninitial layers to capture local information. Through these two contributions,\nwe are able to optimize and obtain a spectral convolution transformer (SCT)\nthat provides improved performance over the state-of-the-art methods while\nreducing the number of parameters. Through extensive experiments, we show that\nSCT-C-small gives state-of-the-art performance on the ImageNet dataset and\nreaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge\nreaches 86.4\\%. We evaluate SCT on transfer learning on datasets such as\nCIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on\ndownstream tasks i.e. instance segmentation on the MSCOCO dataset. The project\npage is available on this webpage.\\url{https://github.com/badripatro/sct}\n","authors":["Badri N. Patro","Vinay P. Namboodiri","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.18063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17870v1","updated":"2024-03-26T16:57:55Z","published":"2024-03-26T16:57:55Z","title":"Boosting Diffusion Models with Moving Average Sampling in Frequency\n  Domain","summary":"  Diffusion models have recently brought a powerful revolution in image\ngeneration. Despite showing impressive generative capabilities, most of these\nmodels rely on the current sample to denoise the next one, possibly resulting\nin denoising instability. In this paper, we reinterpret the iterative denoising\nprocess as model optimization and leverage a moving average mechanism to\nensemble all the prior samples. Instead of simply applying moving average to\nthe denoised samples at different timesteps, we first map the denoised samples\nto data space and then perform moving average to avoid distribution shift\nacross timesteps. In view that diffusion models evolve the recovery from\nlow-frequency components to high-frequency details, we further decompose the\nsamples into different frequency components and execute moving average\nseparately on each component. We name the complete approach \"Moving Average\nSampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into\nmainstream pre-trained diffusion models and sampling schedules. Extensive\nexperiments on both unconditional and conditional diffusion models demonstrate\nthat our MASF leads to superior performances compared to the baselines, with\nalmost negligible additional complexity cost.\n","authors":["Yurui Qian","Qi Cai","Yingwei Pan","Yehao Li","Ting Yao","Qibin Sun","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17870v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time-consuming. Therefore, the challenging task of reconstructing\nvisually accurate HDR images from their Low Dynamic Range (LDR) counterparts is\ngaining attention in the vision research community. A major challenge in this\nresearch problem is the lack of datasets, which capture diverse scene\nconditions (e.g., lighting, shadows, weather, locations, landscapes, objects,\nhumans, buildings) and various image features (e.g., color, contrast,\nsaturation, hue, luminance, brightness, radiance). To address this gap, in this\npaper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic\nHDR images sampled from the GTA-V video game. We perform thorough evaluation of\nthe proposed dataset, which demonstrates significant qualitative and\nquantitative improvements of the state-of-the-art HDR image reconstruction\nmethods. Furthermore, we demonstrate the effectiveness of the proposed dataset\nand its impact on additional computer vision tasks including 3D human pose\nestimation, human body part segmentation, and holistic scene segmentation. The\ndataset, data collection pipeline, and evaluation code are available at:\nhttps://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2403.17727v1","updated":"2024-03-26T14:16:56Z","published":"2024-03-26T14:16:56Z","title":"FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts","summary":"  Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.\n","authors":["Kazuki Kawamura","Jun Rekimoto"],"pdf_url":"https://arxiv.org/pdf/2403.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17708v1","updated":"2024-03-26T13:54:52Z","published":"2024-03-26T13:54:52Z","title":"Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","summary":"  With the rapid development and widespread application of VR/AR technology,\nmaximizing the quality of immersive panoramic video services that match users'\npersonal preferences and habits has become a long-standing challenge.\nUnderstanding the saliency region where users focus, based on data collected\nwith HMDs, can promote multimedia encoding, transmission, and quality\nassessment. At the same time, large-scale datasets are essential for\nresearchers and developers to explore short/long-term user behavior patterns\nand train AI models related to panoramic videos. However, existing panoramic\nvideo datasets often include low-frequency user head or eye movement data\nthrough short-term videos only, lacking sufficient data for analyzing users'\nField of View (FoV) and generating video saliency regions.\n  Driven by these practical factors, in this paper, we present a head and eye\ntracking dataset involving 50 users (25 males and 25 females) watching 15\npanoramic videos. The dataset provides details on the viewport and gaze\nattention locations of users. Besides, we present some statistics samples\nextracted from the dataset. For example, the deviation between head and eye\nmovements challenges the widely held assumption that gaze attention decreases\nfrom the center of the FoV following a Gaussian distribution. Our analysis\nreveals a consistent downward offset in gaze fixations relative to the FoV in\nexperimental settings involving multiple users and videos. That's why we name\nthe dataset Panonut, a saliency weighting shaped like a donut. Finally, we also\nprovide a script that generates saliency distributions based on given head or\neye coordinates and pre-generated saliency distribution map sets of each video\nfrom the collected eye tracking data.\n  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.\n","authors":["Yutong Xu","Junhao Du","Jiahe Wang","Yuwei Ning","Sihan Zhou Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17708v1.pdf","comment":"7 pages,ACM MMSys'24 accepted"},{"id":"http://arxiv.org/abs/2312.02512v2","updated":"2024-03-26T13:21:28Z","published":"2023-12-05T05:36:44Z","title":"AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation\n  with Unified Audio-Visual Speech Representation","summary":"  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.\n","authors":["Jeongsoo Choi","Se Jin Park","Minsu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2312.02512v2.pdf","comment":"CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av"},{"id":"http://arxiv.org/abs/2403.17589v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Dual Memory Networks: A Versatile Adaptation Approach for\n  Vision-Language Models","summary":"  With the emergence of pre-trained vision-language models like CLIP, how to\nadapt them to various downstream classification tasks has garnered significant\nattention in recent research. The adaptation strategies can be typically\ncategorized into three paradigms: zero-shot adaptation, few-shot adaptation,\nand the recently-proposed training-free few-shot adaptation. Most existing\napproaches are tailored for a specific setting and can only cater to one or two\nof these paradigms. In this paper, we introduce a versatile adaptation approach\nthat can effectively work under all three settings. Specifically, we propose\nthe dual memory networks that comprise dynamic and static memory components.\nThe static memory caches training data knowledge, enabling training-free\nfew-shot adaptation, while the dynamic memory preserves historical test\nfeatures online during the testing process, allowing for the exploration of\nadditional data insights beyond the training set. This novel capability\nenhances model performance in the few-shot setting and enables model usability\nin the absence of training data. The two memory networks employ the same\nflexible memory interactive strategy, which can operate in a training-free mode\nand can be further enhanced by incorporating learnable projection layers. Our\napproach is tested across 11 datasets under the three task settings.\nRemarkably, in the zero-shot scenario, it outperforms existing methods by over\n3\\% and even shows superior results against methods utilizing external training\ndata. Additionally, our method exhibits robust performance against natural\ndistribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.\n","authors":["Yabin Zhang","Wenjie Zhu","Hui Tang","Zhiyuan Ma","Kaiyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17589v1.pdf","comment":"CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}"},{"id":"http://arxiv.org/abs/2403.17420v1","updated":"2024-03-26T06:27:50Z","published":"2024-03-26T06:27:50Z","title":"Learning to Visually Localize Sound Sources from Mixtures without Prior\n  Source Knowledge","summary":"  The goal of the multi-sound source localization task is to localize sound\nsources from the mixture individually. While recent multi-sound source\nlocalization methods have shown improved performance, they face challenges due\nto their reliance on prior information about the number of objects to be\nseparated. In this paper, to overcome this limitation, we present a novel\nmulti-sound source localization method that can perform localization without\nprior knowledge of the number of sound sources. To achieve this goal, we\npropose an iterative object identification (IOI) module, which can recognize\nsound-making objects in an iterative manner. After finding the regions of\nsound-making objects, we devise object similarity-aware clustering (OSC) loss\nto guide the IOI module to effectively combine regions of the same object but\nalso distinguish between different objects and backgrounds. It enables our\nmethod to perform accurate localization of sound-making objects without any\nprior knowledge. Extensive experimental results on the MUSIC and VGGSound\nbenchmarks show the significant performance improvements of the proposed method\nover the existing methods for both single and multi-source. Our code is\navailable at: https://github.com/VisualAIKHU/NoPrior_MultiSSL\n","authors":["Dongjin Kim","Sung Jin Um","Sangmin Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17420v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2402.19330v2","updated":"2024-03-26T04:15:53Z","published":"2024-02-29T16:33:12Z","title":"A Novel Approach to Industrial Defect Generation through Blended Latent\n  Diffusion Model with Online Adaptation","summary":"  Effectively addressing the challenge of industrial Anomaly Detection (AD)\nnecessitates an ample supply of defective samples, a constraint often hindered\nby their scarcity in industrial contexts. This paper introduces a novel\nalgorithm designed to augment defective samples, thereby enhancing AD\nperformance. The proposed method tailors the blended latent diffusion model for\ndefect sample generation, employing a diffusion model to generate defective\nsamples in the latent space. A feature editing process, controlled by a\n``trimap\" mask and text prompts, refines the generated samples. The image\ngeneration inference process is structured into three stages: a free diffusion\nstage, an editing diffusion stage, and an online decoder adaptation stage. This\nsophisticated inference strategy yields high-quality synthetic defective\nsamples with diverse pattern variations, leading to significantly improved AD\naccuracies based on the augmented training set. Specifically, on the widely\nrecognized MVTec AD dataset, the proposed method elevates the state-of-the-art\n(SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD\nmetrics AP, IAP, and IAP90, respectively. The implementation code of this work\ncan be found at the GitHub repository\nhttps://github.com/GrandpaXun242/AdaBLDM.git\n","authors":["Hanxi Li","Zhengxun Zhang","Hao Chen","Lin Wu","Bo Li","Deyin Liu","Mingwen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.19330v2.pdf","comment":"13 pages,7 figures"}]},"2024-03-25T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.09738v4","updated":"2024-03-25T23:53:01Z","published":"2024-03-13T18:16:21Z","title":"Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation","summary":"  Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.\n","authors":["Se-eun Yoon","Zhankui He","Jessica Maria Echterhoff","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.09738v4.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17264v1","updated":"2024-03-25T23:15:13Z","published":"2024-03-25T23:15:13Z","title":"EXPLORA: A teacher-apprentice methodology for eliciting natural\n  child-computer interactions","summary":"  Investigating child-computer interactions within their contexts is vital for\ndesigning technology that caters to children's needs. However, determining what\naspects of context are relevant for designing child-centric technology remains\na challenge. We introduce EXPLORA, a multimodal, multistage online methodology\ncomprising three pivotal stages: (1) building a teacher-apprentice\nrelationship,(2) learning from child-teachers, and (3) assessing and\nreinforcing researcher-apprentice learning. Central to EXPLORA is the\ncollection of attitudinal data through pre-observation interviews, offering\nresearchers a deeper understanding of children's characteristics and contexts.\nThis informs subsequent online observations, allowing researchers to focus on\nfrequent interactions. Furthermore, researchers can validate preliminary\nassumptions with children. A means-ends analysis framework aids in the\nsystematic analysis of data, shedding light on context, agency and\nhomework-information searching processes children employ in their activities.\nTo illustrate EXPLORA's capabilities, we present nine single case studies\ninvestigating Brazilian child-caregiver dyads' (children ages 9-11) use of\ntechnology in homework information-searching.\n","authors":["Vanessa Figueiredo","Catherine Ann Cameron"],"pdf_url":"https://arxiv.org/pdf/2403.17264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17210v1","updated":"2024-03-25T21:37:31Z","published":"2024-03-25T21:37:31Z","title":"CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug\n  Interactions","summary":"  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process\nof drug development. DDIs occur when one drug's properties are affected by the\ninclusion of other drugs. Detecting favorable DDIs has the potential to pave\nthe way for creating and advancing innovative medications applicable in\npractical settings. However, existing DDI prediction models continue to face\nchallenges related to generalization in extreme cases, robust feature\nextraction, and real-life application possibilities. We aim to address these\nchallenges by leveraging the effectiveness of context-aware deep graph learning\nby introducing a novel framework named CADGL. Based on a customized variational\ngraph autoencoder (VGAE), we capture critical structural and physio-chemical\ninformation using two context preprocessors for feature extraction from two\ndifferent perspectives: local neighborhood and molecular context, in a\nheterogeneous graphical structure. Our customized VGAE consists of a graph\nencoder, a latent information encoder, and an MLP decoder. CADGL surpasses\nother state-of-the-art DDI prediction models, excelling in predicting\nclinically valuable novel DDIs, supported by rigorous case studies.\n","authors":["Azmine Toushik Wasi","Taki Hasan Rafi","Raima Islam","Serbetar Karlo","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2403.17210v1.pdf","comment":"8 Pages, 4 Figures; In review in IEEE/ACM Transactions on\n  Computational Biology and Bioinformatics"},{"id":"http://arxiv.org/abs/2403.17209v1","updated":"2024-03-25T21:37:30Z","published":"2024-03-25T21:37:30Z","title":"Generation of Asset Administration Shell with Large Language Model\n  Agents: Interoperability in Digital Twins with Semantic Node","summary":"  This research introduces a novel approach for assisting the creation of Asset\nAdministration Shell (AAS) instances for digital twin modeling within the\ncontext of Industry 4.0, aiming to enhance interoperability in smart\nmanufacturing and reduce manual effort. We construct a \"semantic node\" data\nstructure to capture the semantic essence of textual data. Then, a system\npowered by large language models is designed and implemented to process\n\"semantic node\" and generate AAS instance models from textual technical data.\nOur evaluation demonstrates a 62-79% effective generation rate, indicating a\nsubstantial proportion of manual creation effort can be converted into easier\nvalidation effort, thereby reducing the time and cost in creating AAS instance\nmodels. In our evaluation, a comparative analysis of different LLMs and an\nin-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms\nprovide insights into the effectiveness of LLM systems for interpreting\ntechnical concepts. Our findings emphasize LLMs' capability in automating AAS\ninstance creation, enhancing semantic interoperability, and contributing to the\nbroader field of semantic interoperability for digital twins in industrial\napplications. The prototype implementation and evaluation results are released\non our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM\n","authors":["Yuchen Xia","Zhewen Xiao","Nasser Jazdi","Michael Weyrich"],"pdf_url":"https://arxiv.org/pdf/2403.17209v1.pdf","comment":"Pre-print, submitted to IEEE ACCESS, under peer-review"},{"id":"http://arxiv.org/abs/2403.17089v1","updated":"2024-03-25T18:25:10Z","published":"2024-03-25T18:25:10Z","title":"GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI\n  collaboration","summary":"  The advent of ChatGPT and similar large language models (LLMs) has\nrevolutionized the human-AI interaction and information-seeking process.\nLeveraging LLMs as an alternative to search engines, users can now access\nsummarized information tailored to their queries, significantly reducing the\ncognitive load associated with navigating vast information resources. This\nshift underscores the potential of LLMs in redefining information access\nparadigms. Drawing on the foundation of task-focused information retrieval and\nLLMs' task planning ability, this research extends the scope of LLM\ncapabilities beyond routine task automation to support users in navigating\nlong-term and significant life tasks. It introduces the GOLF framework\n(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability\nto assist in significant life decisions through goal orientation and long-term\nplanning. The methodology encompasses a comprehensive simulation study to test\nthe framework's efficacy, followed by model and human evaluations to develop a\ndataset benchmark for long-term life tasks, and experiments across different\nmodels and settings. By shifting the focus from short-term tasks to the broader\nspectrum of long-term life goals, this research underscores the transformative\npotential of LLMs in enhancing human decision-making processes and task\nmanagement, marking a significant step forward in the evolution of human-AI\ncollaboration.\n","authors":["Ben Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16948v1","updated":"2024-03-25T17:10:39Z","published":"2024-03-25T17:10:39Z","title":"Reinforcement Learning-based Recommender Systems with Large Language\n  Models for State Reward and Action Modeling","summary":"  Reinforcement Learning (RL)-based recommender systems have demonstrated\npromising performance in meeting user expectations by learning to make accurate\nnext-item recommendations from historical user-item interactions. However,\nexisting offline RL-based sequential recommendation methods face the challenge\nof obtaining effective user feedback from the environment. Effectively modeling\nthe user state and shaping an appropriate reward for recommendation remains a\nchallenge. In this paper, we leverage language understanding capabilities and\nadapt large language models (LLMs) as an environment (LE) to enhance RL-based\nrecommenders. The LE is learned from a subset of user-item interaction data,\nthus reducing the need for large training data, and can synthesise user\nfeedback for offline data by: (i) acting as a state model that produces high\nquality states that enrich the user representation, and (ii) functioning as a\nreward model to accurately capture nuanced user preferences on actions.\nMoreover, the LE allows to generate positive actions that augment the limited\noffline training data. We propose a LE Augmentation (LEA) method to further\nimprove recommendation performance by optimising jointly the supervised\ncomponent and the RL policy, using the augmented actions and historical user\nsignals. We use LEA, the state and reward models in conjunction with\nstate-of-the-art RL recommenders and report experimental results on two\npublicly available datasets.\n","authors":["Jie Wang","Alexandros Karatzoglou","Ioannis Arapakis","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2403.16948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16778v1","updated":"2024-03-25T13:54:44Z","published":"2024-03-25T13:54:44Z","title":"GloSIS: The Global Soil Information System Web Ontology","summary":"  Established in 2012 by members of the Food and Agriculture Organisation\n(FAO), the Global Soil Partnership (GSP) is a global network of stakeholders\npromoting sound land and soil management practices towards a sustainable world\nfood system. However, soil survey largely remains a local or regional activity,\nbound to heterogeneous methods and conventions. Recognising the relevance of\nglobal and trans-national policies towards sustainable land management\npractices, the GSP elected data harmonisation and exchange as one of its key\nlines of action. Building upon international standards and previous work\ntowards a global soil data ontology, an improved domain model was eventually\ndeveloped within the GSP [54], the basis for a Global Soil Information System\n(GloSIS). This work also identified the Semantic Web as a possible avenue to\noperationalise the domain model. This article presents the GloSIS web ontology,\nan implementation of the GloSIS domain model with the Web Ontology Language\n(OWL). Thoroughly employing a host of Semantic Web standards (SOSA, SKOS,\nGeoSPARQL, QUDT), GloSIS lays out not only a soil data ontology but also an\nextensive set of ready-to-use code-lists for soil description and\nphysio-chemical analysis. Various examples are provided on the provision and\nuse of GloSIS-compliant linked data, showcasing the contribution of this\nontology to the discovery, exploration, integration and access of soil data.\n","authors":["Raul Palma","Bogusz Janiak","Luís Moreira de Sousa","Kathi Schleidt","Tomáš Řezník","Fenny van Egmond","Johan Leenaars","Dimitrios Moshou","Abdul Mouazen","Peter Wilson","David Medyckyj-Scott","Alistair Ritchie","Yusuf Yigini","Ronald Vargas"],"pdf_url":"https://arxiv.org/pdf/2403.16778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01744v2","updated":"2024-03-25T13:16:43Z","published":"2024-03-04T05:41:29Z","title":"NoteLLM: A Retrievable Large Language Model for Note Recommendation","summary":"  People enjoy sharing \"notes\" including their experiences within online\ncommunities. Therefore, recommending notes aligned with user interests has\nbecome a crucial task. Existing online methods only input notes into BERT-based\nmodels to generate note embeddings for assessing similarity. However, they may\nunderutilize some important cues, e.g., hashtags or categories, which represent\nthe key concepts of notes. Indeed, learning to generate hashtags/categories can\npotentially enhance note embeddings, both of which compress key note\ninformation into limited content. Besides, Large Language Models (LLMs) have\nsignificantly outperformed BERT in understanding natural languages. It is\npromising to introduce LLMs into note recommendation. In this paper, we propose\na novel unified framework called NoteLLM, which leverages LLMs to address the\nitem-to-item (I2I) note recommendation. Specifically, we utilize Note\nCompression Prompt to compress a note into a single special token, and further\nlearn the potentially related notes' embeddings via a contrastive learning\napproach. Moreover, we use NoteLLM to summarize the note and generate the\nhashtag/category automatically through instruction tuning. Extensive\nvalidations on real scenarios demonstrate the effectiveness of our proposed\nmethod compared with the online baseline and show major improvements in the\nrecommendation system of Xiaohongshu.\n","authors":["Chao Zhang","Shiwei Wu","Haoxin Zhang","Tong Xu","Yan Gao","Yao Hu","Di Wu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.01744v2.pdf","comment":"Published as a WWW'24 full paper"},{"id":"http://arxiv.org/abs/2403.16702v1","updated":"2024-03-25T12:34:33Z","published":"2024-03-25T12:34:33Z","title":"ProCQA: A Large-scale Community-based Programming Question Answering\n  Dataset for Code Search","summary":"  Retrieval-based code question answering seeks to match user queries in\nnatural language to relevant code snippets. Previous approaches typically rely\non pretraining models using crafted bi-modal and uni-modal datasets to align\ntext and code representations. In this paper, we introduce ProCQA, a\nlarge-scale programming question answering dataset extracted from the\nStackOverflow community, offering naturally structured mixed-modal QA pairs. To\nvalidate its effectiveness, we propose a modality-agnostic contrastive\npre-training approach to improve the alignment of text and code representations\nof current code language models. Compared to previous models that primarily\nemploy bimodal and unimodal pairs extracted from CodeSearchNet for\npre-training, our model exhibits significant performance improvements across a\nwide range of code retrieval benchmarks.\n","authors":["Zehan Li","Jianfei Zhang","Chuantao Yin","Yuanxin Ouyang","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2403.16702v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.16515v2","updated":"2024-03-25T12:01:59Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16656v1","updated":"2024-03-25T11:47:53Z","published":"2024-03-25T11:47:53Z","title":"Graph Augmentation for Recommendation","summary":"  Graph augmentation with contrastive learning has gained significant attention\nin the field of recommendation systems due to its ability to learn expressive\nuser representations, even when labeled data is limited. However, directly\napplying existing GCL models to real-world recommendation environments poses\nchallenges. There are two primary issues to address. Firstly, the lack of\nconsideration for data noise in contrastive learning can result in noisy\nself-supervised signals, leading to degraded performance. Secondly, many\nexisting GCL approaches rely on graph neural network (GNN) architectures, which\ncan suffer from over-smoothing problems due to non-adaptive message passing. To\naddress these challenges, we propose a principled framework called GraphAug.\nThis framework introduces a robust data augmentor that generates denoised\nself-supervised signals, enhancing recommender systems. The GraphAug framework\nincorporates a graph information bottleneck (GIB)-regularized augmentation\nparadigm, which automatically distills informative self-supervision information\nand adaptively adjusts contrastive view generation. Through rigorous\nexperimentation on real-world datasets, we thoroughly assessed the performance\nof our novel GraphAug model. The outcomes consistently unveil its superiority\nover existing baseline methods. The source code for our model is publicly\navailable at: https://github.com/HKUDS/GraphAug.\n","authors":["Qianru Zhang","Lianghao Xia","Xuheng Cai","Siuming Yiu","Chao Huang","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2403.16656v1.pdf","comment":"13 pages and accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06182v4","updated":"2024-03-25T09:43:30Z","published":"2023-04-12T22:46:52Z","title":"GNNUERS: Fairness Explanation in GNNs for Recommendation via\n  Counterfactual Reasoning","summary":"  Nowadays, research into personalization has been focusing on explainability\nand fairness. Several approaches proposed in recent works are able to explain\nindividual recommendations in a post-hoc manner or by explanation paths.\nHowever, explainability techniques applied to unfairness in recommendation have\nbeen limited to finding user/item features mostly related to biased\nrecommendations. In this paper, we devised a novel algorithm that leverages\ncounterfactuality methods to discover user unfairness explanations in the form\nof user-item interactions. In our counterfactual framework, interactions are\nrepresented as edges in a bipartite graph, with users and items as nodes. Our\nbipartite graph explainer perturbs the topological structure to find an altered\nversion that minimizes the disparity in utility between the protected and\nunprotected demographic groups. Experiments on four real-world graphs coming\nfrom various domains showed that our method can systematically explain user\nunfairness on three state-of-the-art GNN-based recommendation models. Moreover,\nan empirical evaluation of the perturbed network uncovered relevant patterns\nthat justify the nature of the unfairness discovered by the generated\nexplanations. The source code and the preprocessed data sets are available at\nhttps://github.com/jackmedda/RS-BGExplainer.\n","authors":["Giacomo Medda","Francesco Fabbri","Mirko Marras","Ludovico Boratto","Gianni Fenu"],"pdf_url":"https://arxiv.org/pdf/2304.06182v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08929v2","updated":"2024-03-25T08:00:38Z","published":"2023-06-15T08:02:07Z","title":"On the resilience of Collaborative Learning-based Recommender Systems\n  Against Community Detection Attack","summary":"  Collaborative-learning-based recommender systems emerged following the\nsuccess of collaborative learning techniques such as Federated Learning (FL)\nand Gossip Learning (GL). In these systems, users participate in the training\nof a recommender system while maintaining their history of consumed items on\ntheir devices. While these solutions seemed appealing for preserving the\nprivacy of the participants at first glance, recent studies have revealed that\ncollaborative learning can be vulnerable to various privacy attacks. In this\npaper, we study the resilience of collaborative learning-based recommender\nsystems against a novel privacy attack called Community Detection Attack (CDA).\nThis attack enables an adversary to identify community members based on a\nchosen set of items (eg., identifying users interested in specific\npoints-of-interest). Through experiments on three real recommendation datasets\nusing two state-of-the-art recommendation models, we evaluate the sensitivity\nof an FL-based recommender system as well as two flavors of Gossip\nLearning-based recommender systems to CDA. The results show that across all\nmodels and datasets, the FL setting is more vulnerable to CDA compared to\nGossip settings. Furthermore, we assess two off-the-shelf mitigation\nstrategies, namely differential privacy (DP) and a \\emph{Share less} policy,\nwhich consists of sharing a subset of less sensitive model parameters. The\nfindings indicate a more favorable privacy-utility trade-off for the\n\\emph{Share less} strategy, particularly in FedRecs.\n","authors":["Yacine Belal","Sonia Ben Mokhtar","Mohamed Maouche","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2306.08929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16504v1","updated":"2024-03-25T07:38:40Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent\n  Classification","summary":"  Following the significant achievements of large language models (LLMs),\nresearchers have employed in-context learning for text classification tasks.\nHowever, these studies focused on monolingual, single-turn classification\ntasks. In this paper, we introduce LARA (Linguistic-Adaptive\nRetrieval-Augmented Language Models), designed to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating numerous\nintents in chatbot interactions. Multi-turn intent classification is notably\nchallenging due to the complexity and evolving nature of conversational\ncontexts. LARA tackles these issues by combining a fine-tuned smaller model\nwith a retrieval-augmented mechanism, integrated within the architecture of\nLLMs. This integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tune. Comprehensive\nexperiments demonstrate that LARA achieves state-of-the-art performance on\nmulti-turn intent classification tasks, enhancing the average accuracy by 3.67%\ncompared to existing methods.\n","authors":["Liu Junhua","Tan Yong Keat","Fu Bin"],"pdf_url":"https://arxiv.org/pdf/2403.16504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16435v1","updated":"2024-03-25T05:31:22Z","published":"2024-03-25T05:31:22Z","title":"InstUPR : Instruction-based Unsupervised Passage Reranking with Large\n  Language Models","summary":"  This paper introduces InstUPR, an unsupervised passage reranking method based\non large language models (LLMs). Different from existing approaches that rely\non extensive training with query-document pairs or retrieval-specific\ninstructions, our method leverages the instruction-following capabilities of\ninstruction-tuned LLMs for passage reranking without any additional\nfine-tuning. To achieve this, we introduce a soft score aggregation technique\nand employ pairwise reranking for unsupervised passage reranking. Experiments\non the BEIR benchmark demonstrate that InstUPR outperforms unsupervised\nbaselines as well as an instruction-tuned reranker, highlighting its\neffectiveness and superiority. Source code to reproduce all experiments is\nopen-sourced at https://github.com/MiuLab/InstUPR\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16435v1.pdf","comment":"Preprint. This manuscript was originally written and submitted in\n  June 2023"},{"id":"http://arxiv.org/abs/2403.16424v1","updated":"2024-03-25T05:04:52Z","published":"2024-03-25T05:04:52Z","title":"An Experiment with the Use of ChatGPT for LCSH Subject Assignment on\n  Electronic Theses and Dissertations","summary":"  This study delves into the potential use of Large Language Models (LLMs) for\ngenerating Library of Congress Subject Headings (LCSH). The authors employed\nChatGPT to generate subject headings for electronic theses and dissertations\n(ETDs) based on their titles and summaries. The results revealed that although\nsome generated subject headings were valid, there were issues regarding\nspecificity and exhaustiveness. The study showcases that LLMs can serve as a\nstrategic response to the backlog of items awaiting cataloging in academic\nlibraries, while also offering a cost-effective approach for promptly\ngenerating LCSH. Nonetheless, human catalogers remain essential for verifying\nand enhancing the validity, exhaustiveness, and specificity of LCSH generated\nby LLMs.\n","authors":["Eric H. C. Chow","TJ Kao","Xiaoli Li"],"pdf_url":"https://arxiv.org/pdf/2403.16424v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2403.16378v1","updated":"2024-03-25T02:52:42Z","published":"2024-03-25T02:52:42Z","title":"Play to Your Strengths: Collaborative Intelligence of Conventional\n  Recommender Models and Large Language Models","summary":"  The rise of large language models (LLMs) has opened new opportunities in\nRecommender Systems (RSs) by enhancing user behavior modeling and content\nunderstanding. However, current approaches that integrate LLMs into RSs solely\nutilize either LLM or conventional recommender model (CRM) to generate final\nrecommendations, without considering which data segments LLM or CRM excel in.\nTo fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books\ndatasets, and compare the performance of a representative CRM (DCNv2) and an\nLLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that\nLLMs excel in data segments where CRMs exhibit lower confidence and precision,\nwhile samples where CRM excels are relatively challenging for LLM, requiring\nsubstantial training data and a long training time for comparable performance.\nThis suggests potential synergies in the combination between LLM and CRM.\nMotivated by these insights, we propose Collaborative Recommendation with\nconventional Recommender and Large Language Model (dubbed \\textit{CoReLLa}). In\nthis framework, we first jointly train LLM and CRM and address the issue of\ndecision boundary shifts through alignment loss. Then, the resource-efficient\nCRM, with a shorter inference time, handles simple and moderate samples, while\nLLM processes the small subset of challenging samples for CRM. Our experimental\nresults demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM\nmethods significantly, underscoring its effectiveness in recommendation tasks.\n","authors":["Yunjia Xi","Weiwen Liu","Jianghao Lin","Chuhan Wu","Bo Chen","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2403.16378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16371v1","updated":"2024-03-25T02:31:57Z","published":"2024-03-25T02:31:57Z","title":"Uncovering Selective State Space Model's Capabilities in Lifelong\n  Sequential Recommendation","summary":"  Sequential Recommenders have been widely applied in various online services,\naiming to model users' dynamic interests from their sequential interactions.\nWith users increasingly engaging with online platforms, vast amounts of\nlifelong user behavioral sequences have been generated. However, existing\nsequential recommender models often struggle to handle such lifelong sequences.\nThe primary challenges stem from computational complexity and the ability to\ncapture long-range dependencies within the sequence. Recently, a state space\nmodel featuring a selective mechanism (i.e., Mamba) has emerged. In this work,\nwe investigate the performance of Mamba for lifelong sequential recommendation\n(i.e., length>=2k). More specifically, we leverage the Mamba block to model\nlifelong user sequences selectively. We conduct extensive experiments to\nevaluate the performance of representative sequential recommendation models in\nthe setting of lifelong sequences. Experiments on two real-world datasets\ndemonstrate the superiority of Mamba. We found that RecMamba achieves\nperformance comparable to the representative model while significantly reducing\ntraining duration by approximately 70% and memory costs by 80%. Codes and data\nare available at \\url{https://github.com/nancheng58/RecMamba}.\n","authors":["Jiyuan Yang","Yuanzi Li","Jingyu Zhao","Hanbing Wang","Muyang Ma","Jun Ma","Zhaochun Ren","Mengqi Zhang","Xin Xin","Zhumin Chen","Pengjie Ren"],"pdf_url":"https://arxiv.org/pdf/2403.16371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16345v1","updated":"2024-03-25T00:43:44Z","published":"2024-03-25T00:43:44Z","title":"Enhanced Facet Generation with LLM Editing","summary":"  In information retrieval, facet identification of a user query is an\nimportant task. If a search service can recognize the facets of a user's query,\nit has the potential to offer users a much broader range of search results.\nPrevious studies can enhance facet prediction by leveraging retrieved documents\nand related queries obtained through a search engine. However, there are\nchallenges in extending it to other applications when a search engine operates\nas part of the model. First, search engines are constantly updated. Therefore,\nadditional information may change during training and test, which may reduce\nperformance. The second challenge is that public search engines cannot search\nfor internal documents. Therefore, a separate search system needs to be built\nto incorporate documents from private domains within the company. We propose\ntwo strategies that focus on a framework that can predict facets by taking only\nqueries as input without a search engine. The first strategy is multi-task\nlearning to predict SERP. By leveraging SERP as a target instead of a source,\nthe proposed model deeply understands queries without relying on external\nmodules. The second strategy is to enhance the facets by combining Large\nLanguage Model (LLM) and the small model. Overall performance improves when\nsmall model and LLM are combined rather than facet generation individually.\n","authors":["Joosung Lee","Jinhong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.16345v1.pdf","comment":"Accepted at LREC-COLING 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.17005v1","updated":"2024-03-25T17:59:40Z","published":"2024-03-25T17:59:40Z","title":"TRIP: Temporal Residual Learning with Image Noise Prior for\n  Image-to-Video Diffusion Models","summary":"  Recent advances in text-to-video generation have demonstrated the utility of\npowerful diffusion models. Nevertheless, the problem is not trivial when\nshaping diffusion models to animate static image (i.e., image-to-video\ngeneration). The difficulty originates from the aspect that the diffusion\nprocess of subsequent animated frames should not only preserve the faithful\nalignment with the given image but also pursue temporal coherence among\nadjacent frames. To alleviate this, we present TRIP, a new recipe of\nimage-to-video diffusion paradigm that pivots on image noise prior derived from\nstatic image to jointly trigger inter-frame relational reasoning and ease the\ncoherent temporal modeling via temporal residual learning. Technically, the\nimage noise prior is first attained through one-step backward diffusion process\nbased on both static image and noised video latent codes. Next, TRIP executes a\nresidual-like dual-path scheme for noise prediction: 1) a shortcut path that\ndirectly takes image noise prior as the reference noise of each frame to\namplify the alignment between the first frame and subsequent frames; 2) a\nresidual path that employs 3D-UNet over noised video and static image latent\ncodes to enable inter-frame relational reasoning, thereby easing the learning\nof the residual noise for each frame. Furthermore, both reference and residual\nnoise of each frame are dynamically merged via attention mechanism for final\nvideo generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT\ndatasets demonstrate the effectiveness of our TRIP for image-to-video\ngeneration. Please see our project page at https://trip-i2v.github.io/TRIP/.\n","authors":["Zhongwei Zhang","Fuchen Long","Yingwei Pan","Zhaofan Qiu","Ting Yao","Yang Cao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17005v1.pdf","comment":"CVPR 2024; Project page: https://trip-i2v.github.io/TRIP/"},{"id":"http://arxiv.org/abs/2403.17004v1","updated":"2024-03-25T17:59:35Z","published":"2024-03-25T17:59:35Z","title":"SD-DiT: Unleashing the Power of Self-supervised Discrimination in\n  Diffusion Transformer","summary":"  Diffusion Transformer (DiT) has emerged as the new trend of generative\ndiffusion models on image generation. In view of extremely slow convergence in\ntypical DiT, recent breakthroughs have been driven by mask strategy that\nsignificantly improves the training efficiency of DiT with additional\nintra-image contextual learning. Despite this progress, mask strategy still\nsuffers from two inherent limitations: (a) training-inference discrepancy and\n(b) fuzzy relations between mask reconstruction & generative diffusion process,\nresulting in sub-optimal training of DiT. In this work, we address these\nlimitations by novelly unleashing the self-supervised discrimination knowledge\nto boost DiT training. Technically, we frame our DiT in a teacher-student\nmanner. The teacher-student discriminative pairs are built on the diffusion\nnoises along the same Probability Flow Ordinary Differential Equation (PF-ODE).\nInstead of applying mask reconstruction loss over both DiT encoder and decoder,\nwe decouple DiT encoder and decoder to separately tackle discriminative and\ngenerative objectives. In particular, by encoding discriminative pairs with\nstudent and teacher DiT encoders, a new discriminative loss is designed to\nencourage the inter-image alignment in the self-supervised embedding space.\nAfter that, student samples are fed into student DiT decoder to perform the\ntypical generative diffusion task. Extensive experiments are conducted on\nImageNet dataset, and our method achieves a competitive balance between\ntraining cost and generative capacity.\n","authors":["Rui Zhu","Yingwei Pan","Yehao Li","Ting Yao","Zhenglong Sun","Tao Mei","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17004v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17001v1","updated":"2024-03-25T17:59:31Z","published":"2024-03-25T17:59:31Z","title":"VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation","summary":"  Recent innovations on text-to-3D generation have featured Score Distillation\nSampling (SDS), which enables the zero-shot learning of implicit 3D models\n(NeRF) by directly distilling prior knowledge from 2D diffusion models.\nHowever, current SDS-based models still struggle with intricate text prompts\nand commonly result in distorted 3D models with unrealistic textures or\ncross-view inconsistency issues. In this work, we introduce a novel Visual\nPrompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the\nvisual appearance knowledge in 2D visual prompt to boost text-to-3D generation.\nInstead of solely supervising SDS with text prompt, VP3D first capitalizes on\n2D diffusion model to generate a high-quality image from input text, which\nsubsequently acts as visual prompt to strengthen SDS optimization with explicit\nvisual appearance. Meanwhile, we couple the SDS optimization with additional\ndifferentiable reward function that encourages rendering images of 3D models to\nbetter visually align with 2D visual prompt and semantically match with text\nprompt. Through extensive experiments, we show that the 2D Visual Prompt in our\nVP3D significantly eases the learning of visual appearance of 3D models and\nthus leads to higher visual fidelity with more detailed textures. It is also\nappealing in view that when replacing the self-generating visual prompt with a\ngiven reference image, VP3D is able to trigger a new task of stylized\ntext-to-3D generation. Our project page is available at\nhttps://vp3d-cvpr24.github.io.\n","authors":["Yang Chen","Yingwei Pan","Haibo Yang","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17001v1.pdf","comment":"CVPR 2024; Project page: https://vp3d-cvpr24.github.io"},{"id":"http://arxiv.org/abs/2403.17000v1","updated":"2024-03-25T17:59:26Z","published":"2024-03-25T17:59:26Z","title":"Learning Spatial Adaptation and Temporal Coherence in Diffusion Models\n  for Video Super-Resolution","summary":"  Diffusion models are just at a tipping point for image super-resolution task.\nNevertheless, it is not trivial to capitalize on diffusion models for video\nsuper-resolution which necessitates not only the preservation of visual\nappearance from low-resolution to high-resolution videos, but also the temporal\nconsistency across video frames. In this paper, we propose a novel approach,\npursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video\nsuper-resolution. SATeCo pivots on learning spatial-temporal guidance from\nlow-resolution videos to calibrate both latent-space high-resolution video\ndenoising and pixel-space video reconstruction. Technically, SATeCo freezes all\nthe parameters of the pre-trained UNet and VAE, and only optimizes two\ndeliberately-designed spatial feature adaptation (SFA) and temporal feature\nalignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame\nfeatures via adaptively estimating affine parameters for each pixel,\nguaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA\ndelves into feature interaction within a 3D local window (tubelet) through\nself-attention, and executes cross-attention between tubelet and its\nlow-resolution counterpart to guide temporal feature alignment. Extensive\nexperiments conducted on the REDS4 and Vid4 datasets demonstrate the\neffectiveness of our approach.\n","authors":["Zhikai Chen","Fuchen Long","Zhaofan Qiu","Ting Yao","Wengang Zhou","Jiebo Luo","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17000v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16985v1","updated":"2024-03-25T17:46:51Z","published":"2024-03-25T17:46:51Z","title":"Towards Low-Latency and Energy-Efficient Hybrid P2P-CDN Live Video\n  Streaming","summary":"  Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an\nincreasingly popular approach in both live and video-on-demand (VoD)\napplications. However, designing a scalable and adaptable framework that\nreduces servers energy consumption and supports low latency and high quality\nservices, particularly for live video streaming scenarios, is still challenging\nfor Over-The-Top (OTT) service providers. To address such challenges, this\npaper introduces a new hybrid P2P-CDN framework that leverages new networking\nand computing paradigms, i.e., Network Function Virtualization (NFV) and edge\ncomputing for live video streaming. The proposed framework introduces a\nmulti-layer architecture and a tree of possible actions therein (an action\ntree), taking into account all available resources from peers, edge, and CDN\nservers to efficiently distribute video fetching and transcoding tasks across a\nhybrid P2P-CDN network, consequently enhancing the users latency and video\nquality. We also discuss our testbed designed to validate the framework and\ncompare it with baseline methods. The experimental results indicate that the\nproposed framework improves user Quality of Experience (QoE), reduces client\nserving latency, and improves edge server energy consumption compared to\nbaseline approaches.\n","authors":["Reza Farahani","Christian Timmerer","Hermann Hellwagner"],"pdf_url":"https://arxiv.org/pdf/2403.16985v1.pdf","comment":"6 pages, 3 figures, Special Issue on Sustainable Multimedia\n  Communications and Services, IEEE MMTC Communications"},{"id":"http://arxiv.org/abs/2403.16951v1","updated":"2024-03-25T17:12:43Z","published":"2024-03-25T17:12:43Z","title":"Network-Assisted Delivery of Adaptive Video Streaming Services through\n  CDN, SDN, and MEC","summary":"  Multimedia applications, mainly video streaming services, are currently the\ndominant source of network load worldwide. In recent Video-on-Demand (VoD) and\nlive video streaming services, traditional streaming delivery techniques have\nbeen replaced by adaptive solutions based on the HTTP protocol. Current trends\ntoward high-resolution (e.g., 8K) and/or low-latency VoD and live video\nstreaming pose new challenges to end-to-end (E2E) bandwidth demand and have\nstringent delay requirements. To do this, video providers typically rely on\nContent Delivery Networks (CDNs) to ensure that they provide scalable video\nstreaming services. To support future streaming scenarios involving millions of\nusers, it is necessary to increase the CDNs' efficiency. It is widely agreed\nthat these requirements may be satisfied by adopting emerging networking\ntechniques to present Network-Assisted Video Streaming (NAVS) methods.\nMotivated by this, this thesis goes one step beyond traditional pure\nclient-based HAS algorithms by incorporating (an) in-network component(s) with\na broader view of the network to present completely transparent NAVS solutions\nfor HAS clients.\n","authors":["Reza Farahani"],"pdf_url":"https://arxiv.org/pdf/2403.16951v1.pdf","comment":"PhD thesis defended in 22.08.2023\n  (https://netlibrary.aau.at/obvuklhs/content/titleinfo/9173622)"},{"id":"http://arxiv.org/abs/2304.02970v5","updated":"2024-03-25T08:50:42Z","published":"2023-04-06T09:54:06Z","title":"Unraveling Instance Associations: A Closer Look for Audio-Visual\n  Segmentation","summary":"  Audio-visual segmentation (AVS) is a challenging task that involves\naccurately segmenting sounding objects based on audio-visual cues. The\neffectiveness of audio-visual learning critically depends on achieving accurate\ncross-modal alignment between sound and visual objects. Successful audio-visual\nlearning requires two essential components: 1) a challenging dataset with\nhigh-quality pixel-level multi-class annotated images associated with audio\nfiles, and 2) a model that can establish strong links between audio information\nand its corresponding visual object. However, these requirements are only\npartially addressed by current methods, with training sets containing biased\naudio-visual data, and models that generalise poorly beyond this biased\ntraining set. In this work, we propose a new cost-effective strategy to build\nchallenging and relatively unbiased high-quality audio-visual segmentation\nbenchmarks. We also propose a new informative sample mining method for\naudio-visual supervised contrastive learning to leverage discriminative\ncontrastive samples to enforce cross-modal understanding. We show empirical\nresults that demonstrate the effectiveness of our benchmark. Furthermore,\nexperiments conducted on existing AVS datasets and on our new benchmark show\nthat our method achieves state-of-the-art (SOTA) segmentation accuracy.\n","authors":["Yuanhong Chen","Yuyuan Liu","Hu Wang","Fengbei Liu","Chong Wang","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2304.02970v5.pdf","comment":"Code is available at https://github.com/cyh-0/CAVP"},{"id":"http://arxiv.org/abs/2402.18107v2","updated":"2024-03-25T05:28:20Z","published":"2024-02-28T06:54:35Z","title":"Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning\n  for Review Helpfulness Prediction","summary":"  In line with the latest research, the task of identifying helpful reviews\nfrom a vast pool of user-generated textual and visual data has become a\nprominent area of study. Effective modal representations are expected to\npossess two key attributes: consistency and differentiation. Current methods\ndesigned for Multimodal Review Helpfulness Prediction (MRHP) face limitations\nin capturing distinctive information due to their reliance on uniform\nmultimodal annotation. The process of adding varied multimodal annotations is\nnot only time-consuming but also labor-intensive. To tackle these challenges,\nwe propose an auto-generated scheme based on multi-task learning to generate\npseudo labels. This approach allows us to simultaneously train for the global\nmultimodal interaction task and the separate cross-modal interaction subtasks,\nenabling us to learn and leverage both consistency and differentiation\neffectively. Subsequently, experimental results validate the effectiveness of\npseudo labels, and our approach surpasses previous textual and multimodal\nbaseline models on two widely accessible benchmark datasets, providing a\nsolution to the MRHP problem.\n","authors":["HongLin Gong","Mengzhao Jia","Liqiang Jing"],"pdf_url":"https://arxiv.org/pdf/2402.18107v2.pdf","comment":"10 pages,4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.15048v2","updated":"2024-03-25T02:08:01Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v2.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2308.09911v2","updated":"2024-03-25T01:54:41Z","published":"2023-08-19T05:34:13Z","title":"Noisy-Correspondence Learning for Text-to-Image Person Re-identification","summary":"  Text-to-image person re-identification (TIReID) is a compelling topic in the\ncross-modal community, which aims to retrieve the target person based on a\ntextual query. Although numerous TIReID methods have been proposed and achieved\npromising performance, they implicitly assume the training image-text pairs are\ncorrectly aligned, which is not always the case in real-world scenarios. In\npractice, the image-text pairs inevitably exist under-correlated or even\nfalse-correlated, a.k.a noisy correspondence (NC), due to the low quality of\nthe images and annotation errors. To address this problem, we propose a novel\nRobust Dual Embedding method (RDE) that can learn robust visual-semantic\nassociations even with NC. Specifically, RDE consists of two main components:\n1) A Confident Consensus Division (CCD) module that leverages the dual-grained\ndecisions of dual embedding modules to obtain a consensus set of clean training\ndata, which enables the model to learn correct and reliable visual-semantic\nassociations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional\nTriplet Ranking loss with the hardest negative samples to a log-exponential\nupper bound over all negative ones, thus preventing the model collapse under NC\nand can also focus on hard-negative samples for promising performance. We\nconduct extensive experiments on three public benchmarks, namely CUHK-PEDES,\nICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our\nRDE. Our method achieves state-of-the-art results both with and without\nsynthetic noisy correspondences on all three datasets. Code is available at\nhttps://github.com/QinYang79/RDE.\n","authors":["Yang Qin","Yingke Chen","Dezhong Peng","Xi Peng","Joey Tianyi Zhou","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2308.09911v2.pdf","comment":null}]},"2024-03-24T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2307.09985v3","updated":"2024-03-24T15:53:57Z","published":"2023-07-19T13:44:32Z","title":"Our Model Achieves Excellent Performance on MovieLens: What Does it\n  Mean?","summary":"  A typical benchmark dataset for recommender system (RecSys) evaluation\nconsists of user-item interactions generated on a platform within a time\nperiod. The interaction generation mechanism partially explains why a user\ninteracts with (e.g., like, purchase, rate) an item, and the context of when a\nparticular interaction happened. In this study, we conduct a meticulous\nanalysis of the MovieLens dataset and explain the potential impact of using the\ndataset for evaluating recommendation algorithms. We make a few main findings\nfrom our analysis. First, there are significant differences in user\ninteractions at the different stages when a user interacts with the MovieLens\nplatform. The early interactions largely define the user portrait which affects\nthe subsequent interactions. Second, user interactions are highly affected by\nthe candidate movies that are recommended by the platform's internal\nrecommendation algorithm(s). Third, changing the order of user interactions\nmakes it more difficult for sequential algorithms to capture the progressive\ninteraction process. We further discuss the discrepancy between the interaction\ngeneration mechanism that is employed by the MovieLens system and that of\ntypical real-world recommendation scenarios. In summary, the MovieLens platform\ndemonstrates an efficient and effective way of collecting user preferences to\naddress cold-starts. However, models that achieve excellent recommendation\naccuracy on the MovieLens dataset may not demonstrate superior performance in\npractice, for at least two kinds of differences: (i) the differences in the\ncontexts of user-item interaction generation, and (ii) the differences in user\nknowledge about the item collections. While results on MovieLens can be useful\nas a reference, they should not be solely relied upon as the primary\njustification for the effectiveness of a recommendation system model.\n","authors":["Yu-chen Fan","Yitong Ji","Jie Zhang","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2307.09985v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14534v2","updated":"2024-03-24T14:34:53Z","published":"2023-02-28T12:44:10Z","title":"Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face","summary":"  We present Spacerini, a tool that integrates the Pyserini toolkit for\nreproducible information retrieval research with Hugging Face to enable the\nseamless construction and deployment of interactive search engines. Spacerini\nmakes state-of-the-art sparse and dense retrieval models more accessible to\nnon-IR practitioners while minimizing deployment effort. This is useful for NLP\nresearchers who want to better understand and validate their research by\nperforming qualitative analyses of training corpora, for IR researchers who\nwant to demonstrate new retrieval models integrated into the growing Pyserini\necosystem, and for third parties reproducing the work of other researchers.\nSpacerini is open source and includes utilities for loading, preprocessing,\nindexing, and deploying search engines locally and remotely. We demonstrate a\nportfolio of 13 search engines created with Spacerini for different use cases.\n","authors":["Christopher Akiki","Odunayo Ogundepo","Aleksandra Piktus","Xinyu Zhang","Akintunde Oladipo","Jimmy Lin","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2302.14534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16151v1","updated":"2024-03-24T13:44:32Z","published":"2024-03-24T13:44:32Z","title":"Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior\n  Detection","summary":"  The online community has increasingly been inundated by a toxic wave of\nharmful comments. In response to this growing challenge, we introduce a\ntwo-stage ultra-low-cost multimodal harmful behavior detection method designed\nto identify harmful comments and images with high precision and recall rates.\nWe first utilize the CLIP-ViT model to transform tweets and images into\nembeddings, effectively capturing the intricate interplay of semantic meaning\nand subtle contextual clues within texts and images. Then in the second stage,\nthe system feeds these embeddings into a conventional machine learning\nclassifier like SVM or logistic regression, enabling the system to be trained\nrapidly and to perform inference at an ultra-low cost. By converting tweets\ninto rich multimodal embeddings through the CLIP-ViT model and utilizing them\nto train conventional machine learning classifiers, our system is not only\ncapable of detecting harmful textual information with near-perfect performance,\nachieving precision and recall rates above 99\\% but also demonstrates the\nability to zero-shot harmful images without additional training, thanks to its\nmultimodal embedding input. This capability empowers our system to identify\nunseen harmful images without requiring extensive and costly image datasets.\nAdditionally, our system quickly adapts to new harmful content; if a new\nharmful content pattern is identified, we can fine-tune the classifier with the\ncorresponding tweets' embeddings to promptly update the system. This makes it\nwell suited to addressing the ever-evolving nature of online harmfulness,\nproviding online communities with a robust, generalizable, and cost-effective\ntool to safeguard their communities.\n","authors":["Albert Lu","Stephen Cranefield"],"pdf_url":"https://arxiv.org/pdf/2403.16151v1.pdf","comment":"to be appear in International Workshop on Coordination,\n  Organizations, Institutions, Norms and Ethics for Governance of Multi-Agent\n  Systems"},{"id":"http://arxiv.org/abs/2403.16135v1","updated":"2024-03-24T13:06:05Z","published":"2024-03-24T13:06:05Z","title":"Complementary Recommendation in E-commerce: Definition, Approaches, and\n  Future Directions","summary":"  In recent years, complementary recommendation has received extensive\nattention in the e-commerce domain. In this paper, we comprehensively summarize\nand compare 34 representative studies conducted between 2009 and 2024. Firstly,\nwe compare the data and methods used for modeling complementary relationships\nbetween products, including simple complementarity and more complex scenarios\nsuch as asymmetric complementarity, the coexistence of substitution and\ncomplementarity relationships between products, and varying degrees of\ncomplementarity between different pairs of products. Next, we classify and\ncompare the models based on the research problems of complementary\nrecommendation, such as diversity, personalization, and cold-start.\nFurthermore, we provide a comparative analysis of experimental results from\ndifferent studies conducted on the same dataset, which helps identify the\nstrengths and weaknesses of the research. Compared to previous surveys, this\npaper provides a more updated and comprehensive summary of the research,\ndiscusses future research directions, and contributes to the advancement of\nthis field.\n","authors":["Linyue Li","Zhijuan Du"],"pdf_url":"https://arxiv.org/pdf/2403.16135v1.pdf","comment":"20 pages,9 figures"},{"id":"http://arxiv.org/abs/2403.16085v1","updated":"2024-03-24T10:45:55Z","published":"2024-03-24T10:45:55Z","title":"RankingSHAP -- Listwise Feature Attribution Explanations for Ranking\n  Models","summary":"  Feature attributions are a commonly used explanation type, when we want to\nposthoc explain the prediction of a trained model. Yet, they are not very well\nexplored in IR. Importantly, feature attribution has rarely been rigorously\ndefined, beyond attributing the most important feature the highest value. What\nit means for a feature to be more important than others is often left vague.\nConsequently, most approaches focus on just selecting the most important\nfeatures and under utilize or even ignore the relative importance within\nfeatures. In this work, we rigorously define the notion of feature attribution\nfor ranking models, and list essential properties that a valid attribution\nshould have. We then propose RankingSHAP as a concrete instantiation of a\nlist-wise ranking attribution method. Contrary to current explanation\nevaluation schemes that focus on selections, we propose two novel evaluation\nparadigms for evaluating attributions over learning-to-rank models. We evaluate\nRankingSHAP for commonly used learning-to-rank datasets to showcase the more\nnuanced use of an attribution method while highlighting the limitations of\nselection-based explanations. In a simulated experiment we design an\ninterpretable model to demonstrate how list-wise ranking attributes can be used\nto investigate model decisions and evaluate the explanations qualitatively.\nBecause of the contrastive nature of the ranking task, our understanding of\nranking model decisions can substantially benefit from feature attribution\nexplanations like RankingSHAP.\n","authors":["Maria Heuss","Maarten de Rijke","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2403.16085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16037v1","updated":"2024-03-24T06:41:33Z","published":"2024-03-24T06:41:33Z","title":"Knowledge-aware Dual-side Attribute-enhanced Recommendation","summary":"  \\textit{Knowledge-aware} recommendation methods (KGR) based on \\textit{graph\nneural networks} (GNNs) and \\textit{contrastive learning} (CL) have achieved\npromising performance. However, they fall short in modeling fine-grained user\npreferences and further fail to leverage the \\textit{preference-attribute\nconnection} to make predictions, leading to sub-optimal performance. To address\nthe issue, we propose a method named \\textit{\\textbf{K}nowledge-aware\n\\textbf{D}ual-side \\textbf{A}ttribute-enhanced \\textbf{R}ecommendation} (KDAR).\nSpecifically, we build \\textit{user preference representations} and\n\\textit{attribute fusion representations} upon the attribute information in\nknowledge graphs, which are utilized to enhance \\textit{collaborative\nfiltering} (CF) based user and item representations, respectively. To\ndiscriminate the contribution of each attribute in these two types of\nattribute-based representations, a \\textit{multi-level collaborative alignment\ncontrasting} mechanism is proposed to align the importance of attributes with\nCF signals. Experimental results on four benchmark datasets demonstrate the\nsuperiority of KDAR over several state-of-the-art baselines. Further analyses\nverify the effectiveness of our method. The code of KDAR is released at:\n\\href{https://github.com/TJTP/KDAR}{https://github.com/TJTP/KDAR}.\n","authors":["Taotian Pang","Xingyu Lou","Fei Zhao","Zhen Wu","Kuiyao Dong","Qiuying Peng","Yue Qi","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2403.16037v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.16143v1","updated":"2024-03-24T13:31:31Z","published":"2024-03-24T13:31:31Z","title":"CFAT: Unleashing TriangularWindows for Image Super-resolution","summary":"  Transformer-based models have revolutionized the field of image\nsuper-resolution (SR) by harnessing their inherent ability to capture complex\ncontextual features. The overlapping rectangular shifted window technique used\nin transformer architecture nowadays is a common practice in super-resolution\nmodels to improve the quality and robustness of image upscaling. However, it\nsuffers from distortion at the boundaries and has limited unique shifting\nmodes. To overcome these weaknesses, we propose a non-overlapping triangular\nwindow technique that synchronously works with the rectangular one to mitigate\nboundary-level distortion and allows the model to access more unique sifting\nmodes. In this paper, we propose a Composite Fusion Attention Transformer\n(CFAT) that incorporates triangular-rectangular window-based local attention\nwith a channel-based global attention technique in image super-resolution. As a\nresult, CFAT enables attention mechanisms to be activated on more image pixels\nand captures long-range, multi-scale features to improve SR performance. The\nextensive experimental results and ablation study demonstrate the effectiveness\nof CFAT in the SR domain. Our proposed model shows a significant 0.7 dB\nperformance improvement over other state-of-the-art SR architectures.\n","authors":["Abhisek Ray","Gaurav Kumar","Maheshkumar H. Kolekar"],"pdf_url":"https://arxiv.org/pdf/2403.16143v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16071v1","updated":"2024-03-24T09:18:21Z","published":"2024-03-24T09:18:21Z","title":"Landmark-Guided Cross-Speaker Lip Reading with Mutual Information\n  Regularization","summary":"  Lip reading, the process of interpreting silent speech from visual lip\nmovements, has gained rising attention for its wide range of realistic\napplications. Deep learning approaches greatly improve current lip reading\nsystems. However, lip reading in cross-speaker scenarios where the speaker\nidentity changes, poses a challenging problem due to inter-speaker variability.\nA well-trained lip reading system may perform poorly when handling a brand new\nspeaker. To learn a speaker-robust lip reading model, a key insight is to\nreduce visual variations across speakers, avoiding the model overfitting to\nspecific speakers. In this work, in view of both input visual clues and latent\nrepresentations based on a hybrid CTC/attention architecture, we propose to\nexploit the lip landmark-guided fine-grained visual clues instead of\nfrequently-used mouth-cropped images as input features, diminishing\nspeaker-specific appearance characteristics. Furthermore, a max-min mutual\ninformation regularization approach is proposed to capture speaker-insensitive\nlatent representations. Experimental evaluations on public lip reading datasets\ndemonstrate the effectiveness of the proposed approach under the intra-speaker\nand inter-speaker conditions.\n","authors":["Linzhi Wu","Xingyu Zhang","Yakun Zhang","Changyan Zheng","Tiejun Liu","Liang Xie","Ye Yan","Erwei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.16071v1.pdf","comment":"To appear in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.11311v2","updated":"2024-03-24T06:21:27Z","published":"2024-03-17T19:12:26Z","title":"Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding","summary":"  Deep multimodal semantic understanding that goes beyond the mere superficial\ncontent relation mining has received increasing attention in the realm of\nartificial intelligence. The challenges of collecting and annotating\nhigh-quality multi-modal data have underscored the significance of few-shot\nlearning. In this paper, we focus on two critical tasks under this context:\nfew-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis\n(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware\nPrompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on\nthe unified vision-language model (VLM). Specifically, we design three experts\nof soft prompts: a text prompt and an image prompt that extract\nmodality-specific features to enrich the single-modal representation, and a\nunified prompt to assist multi-modal interaction. Additionally, we reorganize\nTransformer layers into several blocks and introduce cross-modal prompt\nattention between adjacent blocks, which smoothens the transition from\nsingle-modal representation to multi-modal fusion. On both MSD and MSA datasets\nin few-shot setting, our proposed model not only surpasses the 8.2B model\nInstructBLIP with merely 2% parameters (150M), but also significantly\noutperforms other widely-used prompt methods on VLMs or task-specific methods.\n","authors":["Zichen Wu","Hsiu-Yuan Huang","Fanyi Qu","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11311v2.pdf","comment":"LREC-COLING 2024, Long Paper"}]},"2024-03-23T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.15937v1","updated":"2024-03-23T21:54:18Z","published":"2024-03-23T21:54:18Z","title":"Model, Analyze, and Comprehend User Interactions and Various Attributes\n  within a Social Media Platform","summary":"  How can we effectively model, analyze, and comprehend user interactions and\nvarious attributes within a social media platform based on post-comment\nrelationship? In this study, we propose a novel graph-based approach to model\nand analyze user interactions within a social media platform based on\npost-comment relationship. We construct a user interaction graph from social\nmedia data and analyze it to gain insights into community dynamics, user\nbehavior, and content preferences. Our investigation reveals that while 56.05%\nof the active users are strongly connected within the community, only 0.8% of\nthem significantly contribute to its dynamics. Moreover, we observe temporal\nvariations in community activity, with certain periods experiencing heightened\nengagement. Additionally, our findings highlight a correlation between user\nactivity and popularity showing that more active users are generally more\npopular. Alongside these, a preference for positive and informative content is\nalso observed where 82.41% users preferred positive and informative content.\nOverall, our study provides a comprehensive framework for understanding and\nmanaging online communities, leveraging graph-based techniques to gain valuable\ninsights into user behavior and community dynamics.\n","authors":["Md Kaykobad Reza","S M Maksudul Alam","Yiran Luo","Youzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2403.15937v1.pdf","comment":"9 Pages, 8 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2309.01157v2","updated":"2024-03-23T17:05:42Z","published":"2023-09-03T12:33:47Z","title":"Large Language Models for Generative Recommendation: A Survey and\n  Visionary Discussions","summary":"  Large language models (LLM) not only have revolutionized the field of natural\nlanguage processing (NLP) but also have the potential to reshape many other\nfields, e.g., recommender systems (RS). However, most of the related work\ntreats an LLM as a component of the conventional recommendation pipeline (e.g.,\nas a feature extractor), which may not be able to fully leverage the generative\npower of LLM. Instead of separating the recommendation process into multiple\nstages, such as score computation and re-ranking, this process can be\nsimplified to one stage with LLM: directly generating recommendations from the\ncomplete pool of items. This survey reviews the progress, methods, and future\ndirections of LLM-based generative recommendation by examining three questions:\n1) What generative recommendation is, 2) Why RS should advance to generative\nrecommendation, and 3) How to implement LLM-based generative recommendation for\nvarious RS tasks. We hope that this survey can provide the context and guidance\nneeded to explore this interesting and emerging topic.\n","authors":["Lei Li","Yongfeng Zhang","Dugang Liu","Li Chen"],"pdf_url":"https://arxiv.org/pdf/2309.01157v2.pdf","comment":"Published as a conference paper at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.13597v2","updated":"2024-03-23T17:05:15Z","published":"2024-03-20T13:44:30Z","title":"No more optimization rules: LLM-enabled policy-based multi-modal query\n  optimizer","summary":"  Large language model (LLM) has marked a pivotal moment in the field of\nmachine learning and deep learning. Recently its capability for query planning\nhas been investigated, including both single-modal and multi-modal queries.\nHowever, there is no work on the query optimization capability of LLM. As a\ncritical (or could even be the most important) step that significantly impacts\nthe execution performance of the query plan, such analysis and attempts should\nnot be missed. From another aspect, existing query optimizers are usually\nrule-based or rule-based + cost-based, i.e., they are dependent on manually\ncreated rules to complete the query plan rewrite/transformation. Given the fact\nthat modern optimizers include hundreds to thousands of rules, designing a\nmulti-modal query optimizer following a similar way is significantly\ntime-consuming since we will have to enumerate as many multi-modal optimization\nrules as possible, which has not been well addressed today. In this paper, we\ninvestigate the query optimization ability of LLM and use LLM to design LaPuda,\na novel LLM and Policy based multi-modal query optimizer. Instead of\nenumerating specific and detailed rules, LaPuda only needs a few abstract\npolicies to guide LLM in the optimization, by which much time and human effort\nare saved. Furthermore, to prevent LLM from making mistakes or negative\noptimization, we borrow the idea of gradient descent and propose a guided cost\ndescent (GCD) algorithm to perform the optimization, such that the optimization\ncan be kept in the correct direction. In our evaluation, our methods\nconsistently outperform the baselines in most cases. For example, the optimized\nplans generated by our methods result in 1~3x higher execution speed than those\nby the baselines.\n","authors":["Yifan Wang","Haodi Ma","Daisy Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13597v2.pdf","comment":"Yifan and Haodi contribute equally to the work"},{"id":"http://arxiv.org/abs/2401.11624v5","updated":"2024-03-23T16:35:45Z","published":"2024-01-21T23:34:42Z","title":"In-context Learning with Retrieved Demonstrations for Language Models: A\n  Survey","summary":"  Language models, especially pre-trained large language models, have showcased\nremarkable abilities as few-shot in-context learners (ICL), adept at adapting\nto new tasks with just a few demonstrations in the input context. However, the\nmodel's ability to perform ICL is sensitive to the choice of the few-shot\ndemonstrations. Instead of using a fixed set of demonstrations, one recent\ndevelopment is to retrieve demonstrations tailored to each input query. The\nimplementation of demonstration retrieval is relatively straightforward,\nleveraging existing databases and retrieval systems. This not only improves the\nefficiency and scalability of the learning process but also has been shown to\nreduce biases inherent in manual example selection. In light of the encouraging\nresults and growing research in ICL with retrieved demonstrations, we conduct\nan extensive review of studies in this area. In this survey, we discuss and\ncompare different design choices for retrieval models, retrieval training\nprocedures, and inference algorithms.\n","authors":["Man Luo","Xin Xu","Yue Liu","Panupong Pasupat","Mehran Kazemi"],"pdf_url":"https://arxiv.org/pdf/2401.11624v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15765v1","updated":"2024-03-23T08:40:35Z","published":"2024-03-23T08:40:35Z","title":"Towards Human-Like Machine Comprehension: Few-Shot Relational Learning\n  in Visually-Rich Documents","summary":"  Key-value relations are prevalent in Visually-Rich Documents (VRDs), often\ndepicted in distinct spatial regions accompanied by specific color and font\nstyles. These non-textual cues serve as important indicators that greatly\nenhance human comprehension and acquisition of such relation triplets. However,\ncurrent document AI approaches often fail to consider this valuable prior\ninformation related to visual and spatial features, resulting in suboptimal\nperformance, particularly when dealing with limited examples. To address this\nlimitation, our research focuses on few-shot relational learning, specifically\ntargeting the extraction of key-value relation triplets in VRDs. Given the\nabsence of a suitable dataset for this task, we introduce two new few-shot\nbenchmarks built upon existing supervised benchmark datasets. Furthermore, we\npropose a variational approach that incorporates relational 2D-spatial priors\nand prototypical rectification techniques. This approach aims to generate\nrelation representations that are more aware of the spatial context and unseen\nrelation in a manner similar to human perception. Experimental results\ndemonstrate the effectiveness of our proposed method by showcasing its ability\nto outperform existing methods. This study also opens up new possibilities for\npractical applications.\n","authors":["Hao Wang","Tang Li","Chenhui Chu","Nengjun Zhu","Rui Wang","Pinpin Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.15765v1.pdf","comment":"13 pages, 7 figures, accepted by LERC-COLING2024"},{"id":"http://arxiv.org/abs/2403.15757v1","updated":"2024-03-23T08:03:50Z","published":"2024-03-23T08:03:50Z","title":"User-Side Realization","summary":"  Users are dissatisfied with services. Since the service is not tailor-made\nfor a user, it is natural for dissatisfaction to arise. The problem is, that\neven if users are dissatisfied, they often do not have the means to resolve\ntheir dissatisfaction. The user cannot alter the source code of the service,\nnor can they force the service provider to change. The user has no choice but\nto remain dissatisfied or quit the service. User-side realization offers\nproactive solutions to this problem by providing general algorithms to deal\nwith common problems on the user's side. These algorithms run on the user's\nside and solve the problems without having the service provider change the\nservice itself.\n","authors":["Ryoma Sato"],"pdf_url":"https://arxiv.org/pdf/2403.15757v1.pdf","comment":"Doctoral Thesis"},{"id":"http://arxiv.org/abs/2403.15740v1","updated":"2024-03-23T06:36:32Z","published":"2024-03-23T06:36:32Z","title":"Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large\n  Language Models","summary":"  Web user data plays a central role in the ecosystem of pre-trained large\nlanguage models (LLMs) and their fine-tuned variants. Billions of data are\ncrawled from the web and fed to LLMs. How can \\textit{\\textbf{everyday web\nusers}} confirm if LLMs misuse their data without permission? In this work, we\nsuggest that users repeatedly insert personal passphrases into their documents,\nenabling LLMs to memorize them. These concealed passphrases in user documents,\nreferred to as \\textit{ghost sentences}, once they are identified in the\ngenerated content of LLMs, users can be sure that their data is used for\ntraining. To explore the effectiveness and usage of this copyrighting tool, we\ndefine the \\textit{user training data identification} task with ghost\nsentences. Multiple datasets from various sources at different scales are\ncreated and tested with LLMs of different sizes. For evaluation, we introduce a\nlast $k$ words verification manner along with two metrics: document and user\nidentification accuracy. In the specific case of instruction tuning of a 3B\nLLaMA model, 11 out of 16 users with ghost sentences identify their data within\nthe generation content. These 16 users contribute 383 examples to $\\sim$1.8M\ntraining documents. For continuing pre-training of a 1.1B TinyLlama model, 61\nout of 64 users with ghost sentences identify their data within the LLM output.\nThese 64 users contribute 1156 examples to $\\sim$10M training documents.\n","authors":["Shuai Zhao","Linchao Zhu","Ruijie Quan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15740v1.pdf","comment":"Preprint, work in progress"},{"id":"http://arxiv.org/abs/2403.15667v1","updated":"2024-03-23T01:30:46Z","published":"2024-03-23T01:30:46Z","title":"QueryExplorer: An Interactive Query Generation Assistant for Search and\n  Exploration","summary":"  Formulating effective search queries remains a challenging task, particularly\nwhen users lack expertise in a specific domain or are not proficient in the\nlanguage of the content. Providing example documents of interest might be\neasier for a user. However, such query-by-example scenarios are prone to\nconcept drift, and the retrieval effectiveness is highly sensitive to the query\ngeneration method, without a clear way to incorporate user feedback. To enable\nexploration and to support Human-In-The-Loop experiments we propose\nQueryExplorer -- an interactive query generation, reformulation, and retrieval\ninterface with support for HuggingFace generation models and PyTerrier's\nretrieval pipelines and datasets, and extensive logging of human feedback. To\nallow users to create and modify effective queries, our demo supports\ncomplementary approaches of using LLMs interactively, assisting the user with\nedits and feedback at multiple stages of the query formulation process. With\nsupport for recording fine-grained interactions and user annotations,\nQueryExplorer can serve as a valuable experimental and research platform for\nannotation, qualitative evaluation, and conducting Human-in-the-Loop (HITL)\nexperiments for complex search tasks where users struggle to formulate queries.\n","authors":["Kaustubh D. Dhole","Shivam Bajaj","Ramraj Chandradevan","Eugene Agichtein"],"pdf_url":"https://arxiv.org/pdf/2403.15667v1.pdf","comment":"NAACL 2024 Demonstration Track"}],"Multimedia":[{"id":"http://arxiv.org/abs/2309.11093v3","updated":"2024-03-23T13:33:06Z","published":"2023-09-20T06:54:55Z","title":"K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling","summary":"  Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.\n","authors":["Haven Kim","Jongmin Jung","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2309.11093v3.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15694v1","updated":"2024-03-23T03:06:19Z","published":"2024-03-23T03:06:19Z","title":"Group Benefits Instances Selection for Data Purification","summary":"  Manually annotating datasets for training deep models is very labor-intensive\nand time-consuming. To overcome such inferiority, directly leveraging web\nimages to conduct training data becomes a natural choice. Nevertheless, the\npresence of label noise in web data usually degrades the model performance.\nExisting methods for combating label noise are typically designed and tested on\nsynthetic noisy datasets. However, they tend to fail to achieve satisfying\nresults on real-world noisy datasets. To this end, we propose a method named\nGRIP to alleviate the noisy label problem for both synthetic and real-world\ndatasets. Specifically, GRIP utilizes a group regularization strategy that\nestimates class soft labels to improve noise robustness. Soft label supervision\nreduces overfitting on noisy labels and learns inter-class similarities to\nbenefit classification. Furthermore, an instance purification operation\nglobally identifies noisy labels by measuring the difference between each\ntraining sample and its class soft label. Through operations at both group and\ninstance levels, our approach integrates the advantages of noise-robust and\nnoise-cleaning methods and remarkably alleviates the performance degradation\ncaused by noisy labels. Comprehensive experimental results on synthetic and\nreal-world datasets demonstrate the superiority of GRIP over the existing\nstate-of-the-art methods.\n","authors":["Zhenhuang Cai","Chuanyi Zhang","Dan Huang","Yuanbo Chen","Xiuyun Guan","Yazhou Yao"],"pdf_url":"https://arxiv.org/pdf/2403.15694v1.pdf","comment":"accepted by IEEE Intelligent Systems"},{"id":"http://arxiv.org/abs/2403.15679v1","updated":"2024-03-23T02:09:23Z","published":"2024-03-23T02:09:23Z","title":"DS-NeRV: Implicit Neural Video Representation with Decomposed Static and\n  Dynamic Codes","summary":"  Implicit neural representations for video (NeRV) have recently become a novel\nway for high-quality video representation. However, existing works employ a\nsingle network to represent the entire video, which implicitly confuse static\nand dynamic information. This leads to an inability to effectively compress the\nredundant static information and lack the explicitly modeling of global\ntemporal-coherent dynamic details. To solve above problems, we propose DS-NeRV,\nwhich decomposes videos into sparse learnable static codes and dynamic codes\nwithout the need for explicit optical flow or residual supervision. By setting\ndifferent sampling rates for two codes and applying weighted sum and\ninterpolation sampling methods, DS-NeRV efficiently utilizes redundant static\ninformation while maintaining high-frequency details. Additionally, we design a\ncross-channel attention-based (CCA) fusion module to efficiently fuse these two\ncodes for frame decoding. Our approach achieves a high quality reconstruction\nof 31.2 PSNR with only 0.35M parameters thanks to separate static and dynamic\ncodes representation and outperforms existing NeRV methods in many downstream\ntasks. Our project website is at https://haoyan14.github.io/DS-NeRV.\n","authors":["Hao Yan","Zhihui Ke","Xiaobo Zhou","Tie Qiu","Xidong Shi","Dadong Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.15679v1.pdf","comment":"CVPR 2024. Project page at https://haoyan14.github.io/DS-NeRV"}]},"2024-03-22T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.15548v1","updated":"2024-03-22T18:03:27Z","published":"2024-03-22T18:03:27Z","title":"Spectral Initialization for High-Dimensional Phase Retrieval with Biased\n  Spatial Directions","summary":"  We explore a spectral initialization method that plays a central role in\ncontemporary research on signal estimation in nonconvex scenarios. In a\nnoiseless phase retrieval framework, we precisely analyze the method's\nperformance in the high-dimensional limit when sensing vectors follow a\nmultivariate Gaussian distribution for two rotationally invariant models of the\ncovariance matrix C. In the first model C is a projector on a lower dimensional\nspace while in the second it is a Wishart matrix. Our analytical results extend\nthe well-established case when C is the identity matrix. Our examination shows\nthat the introduction of biased spatial directions leads to a substantial\nimprovement in the spectral method's effectiveness, particularly when the\nnumber of measurements is less than the signal's dimension. This extension also\nconsistently reveals a phase transition phenomenon dependent on the ratio\nbetween sample size and signal dimension. Surprisingly, both of these models\nshare the same threshold value.\n","authors":["Pierre Bousseyroux","Marc Potters"],"pdf_url":"https://arxiv.org/pdf/2403.15548v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.15529v1","updated":"2024-03-22T17:31:43Z","published":"2024-03-22T17:31:43Z","title":"LimGen: Probing the LLMs for Generating Suggestive Limitations of\n  Research Papers","summary":"  Examining limitations is a crucial step in the scholarly research reviewing\nprocess, revealing aspects where a study might lack decisiveness or require\nenhancement. This aids readers in considering broader implications for further\nresearch. In this article, we present a novel and challenging task of\nSuggestive Limitation Generation (SLG) for research papers. We compile a\ndataset called LimGen, encompassing 4068 research papers and their associated\nlimitations from the ACL anthology. We investigate several approaches to\nharness large language models (LLMs) for producing suggestive limitations, by\nthoroughly examining the related challenges, practical insights, and potential\nopportunities. Our LimGen dataset and code can be accessed at\nhttps://github.com/armbf/LimGen.\n","authors":["Abdur Rahman Bin Md Faizullah","Ashok Urlana","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.15529v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.15279v1","updated":"2024-03-22T15:22:06Z","published":"2024-03-22T15:22:06Z","title":"Fundus: A Simple-to-Use News Scraper Optimized for High Quality\n  Extractions","summary":"  This paper introduces Fundus, a user-friendly news scraper that enables users\nto obtain millions of high-quality news articles with just a few lines of code.\nUnlike existing news scrapers, we use manually crafted, bespoke content\nextractors that are specifically tailored to the formatting guidelines of each\nsupported online newspaper. This allows us to optimize our scraping for quality\nsuch that retrieved news articles are textually complete and without HTML\nartifacts. Further, our framework combines both crawling (retrieving HTML from\nthe web or large web archives) and content extraction into a single pipeline.\nBy providing a unified interface for a predefined collection of newspapers, we\naim to make Fundus broadly usable even for non-technical users. This paper\ngives an overview of the framework, discusses our design choices, and presents\na comparative evaluation against other popular news scrapers. Our evaluation\nshows that Fundus yields significantly higher quality extractions (complete and\nartifact-free news articles) than prior work. The framework is available on\nGitHub under https://github.com/flairNLP/fundus and can be simply installed\nusing pip.\n","authors":["Max Dallabetta","Conrad Dobberstein","Adrian Breiding","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2403.15279v1.pdf","comment":"10 pages, 4 figures, submitted to ACL 2024, for a screencast see\n  https://www.youtube.com/watch?v=9GJExMelhdI"},{"id":"http://arxiv.org/abs/2403.15246v1","updated":"2024-03-22T14:42:29Z","published":"2024-03-22T14:42:29Z","title":"FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions","summary":"  Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.\n","authors":["Orion Weller","Benjamin Chang","Sean MacAvaney","Kyle Lo","Arman Cohan","Benjamin Van Durme","Dawn Lawrie","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2403.15246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15520v1","updated":"2024-03-22T12:22:44Z","published":"2024-03-22T12:22:44Z","title":"GTC: GNN-Transformer Co-contrastive Learning for Self-supervised\n  Heterogeneous Graph Representation","summary":"  Graph Neural Networks (GNNs) have emerged as the most powerful weapon for\nvarious graph tasks due to the message-passing mechanism's great local\ninformation aggregation ability. However, over-smoothing has always hindered\nGNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs,\nTransformers can model global information and multi-hop interactions via\nmulti-head self-attention and a proper Transformer structure can show more\nimmunity to the over-smoothing problem. So, can we propose a novel framework to\ncombine GNN and Transformer, integrating both GNN's local information\naggregation and Transformer's global information modeling ability to eliminate\nthe over-smoothing problem? To realize this, this paper proposes a\ncollaborative learning scheme for GNN-Transformer and constructs GTC\narchitecture. GTC leverages the GNN and Transformer branch to encode node\ninformation from different views respectively, and establishes contrastive\nlearning tasks based on the encoded cross-view information to realize\nself-supervised heterogeneous graph representation. For the Transformer branch,\nwe propose Metapath-aware Hop2Token and CG-Hetphormer, which can cooperate with\nGNN to attentively encode neighborhood information from different levels. As\nfar as we know, this is the first attempt in the field of graph representation\nlearning to utilize both GNN and Transformer to collaboratively capture\ndifferent view information and conduct cross-view contrastive learning. The\nexperiments on real datasets show that GTC exhibits superior performance\ncompared with state-of-the-art methods. Codes can be available at\nhttps://github.com/PHD-lanyu/GTC.\n","authors":["Yundong Sun","Dongjie Zhu","Yansong Wang","Zhaoshuo Tian"],"pdf_url":"https://arxiv.org/pdf/2403.15520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15075v1","updated":"2024-03-22T09:58:33Z","published":"2024-03-22T09:58:33Z","title":"Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation","summary":"  Recent methods utilize graph contrastive Learning within graph-structured\nuser-item interaction data for collaborative filtering and have demonstrated\ntheir efficacy in recommendation tasks. However, they ignore that the\ndifference relation density of nodes between the user- and item-side causes the\nadaptability of graphs on bilateral nodes to be different after multi-hop graph\ninteraction calculation, which limits existing models to achieve ideal results.\nTo solve this issue, we propose a novel framework for recommendation tasks\ncalled Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that\nconsider the bilateral unsymmetry on user-item node relation density for sliced\nuser and item graph reasoning better with bilateral slicing contrastive\ntraining. Especially, taking into account the aggregation ability of\nhypergraph-based graph convolutional network (GCN) in digging implicit\nsimilarities is more suitable for user nodes, embeddings generated from three\ndifferent modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into\ntwo subviews by the user- and item-side respectively, and selectively combined\ninto subview pairs bilaterally based on the characteristics of inter-node\nrelation structure. Furthermore, to align the distribution of user and item\nembeddings after aggregation, a dispersing loss is leveraged to adjust the\nmutual distance between all embeddings for maintaining learning ability.\nComprehensive experiments on two public datasets have proved the superiority of\nBusGCL in comparison to various recommendation methods. Other models can simply\nutilize our bilateral slicing contrastive learning to enhance recommending\nperformance without incurring extra expenses.\n","authors":["Jiaheng Yu","Jing Li","Yue He","Kai Zhu","Shuyi Zhang","Wen Hu"],"pdf_url":"https://arxiv.org/pdf/2403.15075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10435v3","updated":"2024-03-22T05:57:48Z","published":"2023-09-19T08:54:47Z","title":"Language Modeling for Content-enriched Recommendation","summary":"  Recommender systems are indispensable in the realm of online applications,\nand sequential recommendation has enjoyed considerable prevalence due to its\ncapacity to encapsulate the dynamic shifts in user interests. However, previous\nsequential modeling methods still have limitations in capturing contextual\ninformation. The primary reason is the lack of understanding of domain-specific\nknowledge and item-related textual content by language models. Fortunately, the\nemergence of powerful language models has unlocked the potential to incorporate\nextensive world knowledge into recommendation algorithms, enabling them to go\nbeyond simple item attributes and truly understand the world surrounding user\npreferences. To achieve this, we propose LANCER, which leverages the semantic\nunderstanding capabilities of pre-trained language models to generate\npersonalized recommendations. Our approach bridges the gap between language\nmodels and recommender systems, resulting in more human-like recommendations.\nWe demonstrate the effectiveness of our approach through a series of\nexperiments conducted on multiple benchmark datasets, showing promising results\nand providing valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable.\n","authors":["Junzhe Jiang","Shang Qu","Mingyue Cheng","Qi Liu","Zhiding Liu","Hao Zhang","Rujiao Zhang","Kai Zhang","Rui Li","Jiatong Li","Min Gao"],"pdf_url":"https://arxiv.org/pdf/2309.10435v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08699v3","updated":"2024-03-22T03:31:22Z","published":"2024-01-14T12:38:49Z","title":"On Image Search in Histopathology","summary":"  Pathology images of histopathology can be acquired from camera-mounted\nmicroscopes or whole slide scanners. Utilizing similarity calculations to match\npatients based on these images holds significant potential in research and\nclinical contexts. Recent advancements in search technologies allow for\nimplicit quantification of tissue morphology across diverse primary sites,\nfacilitating comparisons and enabling inferences about diagnosis, and\npotentially prognosis, and predictions for new patients when compared against a\ncurated database of diagnosed and treated cases. In this paper, we\ncomprehensively review the latest developments in image search technologies for\nhistopathology, offering a concise overview tailored for computational\npathology researchers seeking effective, fast and efficient image search\nmethods in their work.\n","authors":["H. R. Tizhoosh","Liron Pantanowitz"],"pdf_url":"https://arxiv.org/pdf/2401.08699v3.pdf","comment":"A chapter in the Book \"Artificial INtelligence in Digital Pathology\"\n  by Cohen and Chauhan, 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.15336v1","updated":"2024-03-22T16:41:45Z","published":"2024-03-22T16:41:45Z","title":"Dialogue Understandability: Why are we streaming movies with subtitles?","summary":"  Watching movies and TV shows with subtitles enabled is not simply down to\naudibility or speech intelligibility. A variety of evolving factors related to\ntechnological advances, cinema production and social behaviour challenge our\nperception and understanding. This study seeks to formalise and give context to\nthese influential factors under a wider and novel term referred to as Dialogue\nUnderstandability. We propose a working definition for Dialogue\nUnderstandability being a listener's capacity to follow the story without undue\ncognitive effort or concentration being required that impacts their Quality of\nExperience (QoE). The paper identifies, describes and categorises the factors\nthat influence Dialogue Understandability mapping them over the QoE framework,\na media streaming lifecycle, and the stakeholders involved. We then explore\navailable measurement tools in the literature and link them to the factors they\ncould potentially be used for. The maturity and suitability of these tools is\nevaluated over a set of pilot experiments. Finally, we reflect on the gaps that\nstill need to be filled, what we can measure and what not, future subjective\nexperiments, and new research trends that could help us to fully characterise\nDialogue Understandability.\n","authors":["Helard Becerra Martinez","Alessandro Ragano","Diptasree Debnath","Asad Ullah","Crisron Rudolf Lucas","Martin Walsh","Andrew Hines"],"pdf_url":"https://arxiv.org/pdf/2403.15336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15256v1","updated":"2024-03-22T14:57:12Z","published":"2024-03-22T14:57:12Z","title":"Experimental Studies of Metaverse Streaming","summary":"  Metaverse aims to construct a large, unified, immersive, and shared digital\nrealm by combining various technologies, namely XR (extended reality),\nblockchain, and digital twin, among others. This article explores the Metaverse\nfrom the perspective of multimedia communication by conducting and analyzing\nreal-world experiments on four different Metaverse platforms: VR (virtual\nreality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual\nCity. We first investigate the traffic patterns and network performance in the\nthree VR platforms. After raising the challenges of the Metaverse streaming and\ninvestigating the potential methods to enhance Metaverse performance, we\npropose a remote rendering architecture and verify its advantages through a\nprototype involving the campus network and MR multimodal interaction by\ncomparison with local rendering.\n","authors":["Haopeng Wang","Roberto Martinez-Velazquez","Haiwei Dong","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2403.15256v1.pdf","comment":"Accepted by IEEE Consumer Electronics Magazine"},{"id":"http://arxiv.org/abs/2308.04025v3","updated":"2024-03-22T14:49:31Z","published":"2023-08-08T03:43:24Z","title":"MSAC: Multiple Speech Attribute Control Method for Reliable Speech\n  Emotion Recognition","summary":"  Despite notable progress, speech emotion recognition (SER) remains\nchallenging due to the intricate and ambiguous nature of speech emotion,\nparticularly in wild world. While current studies primarily focus on\nrecognition and generalization abilities, our research pioneers an\ninvestigation into the reliability of SER methods in the presence of semantic\ndata shifts and explores how to exert fine-grained control over various\nattributes inherent in speech signals to enhance speech emotion modeling. In\nthis paper, we first introduce MSAC-SERNet, a novel unified SER framework\ncapable of simultaneously handling both single-corpus and cross-corpus SER.\nSpecifically, concentrating exclusively on the speech emotion attribute, a\nnovel CNN-based SER model is presented to extract discriminative emotional\nrepresentations, guided by additive margin softmax loss. Considering\ninformation overlap between various speech attributes, we propose a novel\nlearning paradigm based on correlations of different speech attributes, termed\nMultiple Speech Attribute Control (MSAC), which empowers the proposed SER model\nto simultaneously capture fine-grained emotion-related features while\nmitigating the negative impact of emotion-agnostic representations.\nFurthermore, we make a first attempt to examine the reliability of the\nMSAC-SERNet framework using out-of-distribution detection methods. Experiments\non both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet\nnot only consistently outperforms the baseline in all aspects, but achieves\nsuperior performance compared to state-of-the-art SER approaches.\n","authors":["Yu Pan","Yuguang Yang","Yuheng Huang","Jixun Yao","Jingjing Yin","Yanni Hu","Heng Lu","Lei Ma","Jianjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.04025v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.15226v1","updated":"2024-03-22T14:20:34Z","published":"2024-03-22T14:20:34Z","title":"Not All Attention is Needed: Parameter and Computation Efficient\n  Transfer Learning for Multi-modal Large Language Models","summary":"  In this paper, we propose a novel parameter and computation efficient tuning\nmethod for Multi-modal Large Language Models (MLLMs), termed Efficient\nAttention Skipping (EAS). Concretely, we first reveal that multi-head\nattentions (MHAs), the main computational overhead of MLLMs, are often\nredundant to downstream tasks. Based on this observation, EAS evaluates the\nattention redundancy and skips the less important MHAs to speed up inference.\nBesides, we also propose a novel propagation-of-information adapter (PIA) to\nserve the attention skipping of EAS and keep parameter efficiency, which can be\nfurther re-parameterized into feed-forward networks (FFNs) for zero-extra\nlatency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN\nand a classic VL pre-trained model called METER, and conduct extensive\nexperiments on a set of benchmarks. The experiments show that EAS not only\nretains high performance and parameter efficiency, but also greatly speeds up\ninference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on\nScineceQA while speeding up inference by 2.2 times to LaVIN\n","authors":["Qiong Wu","Weihao Ye","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.15226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14899v2","updated":"2024-03-22T13:24:35Z","published":"2023-06-26T17:59:55Z","title":"FunQA: Towards Surprising Video Comprehension","summary":"  Surprising videos, such as funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question-answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Moreover, we propose\nFunMentor, an agent designed for Vision-Language Models (VLMs) that uses\nmulti-turn dialogues to enhance models' understanding of counter-intuitiveness.\nExtensive experiments with existing VLMs demonstrate the effectiveness of\nFunMentor and reveal significant performance gaps for the FunQA videos across\nspatial-temporal reasoning, visual-centered reasoning, and free-text\ngeneration.\n","authors":["Binzhu Xie","Sicheng Zhang","Zitang Zhou","Bo Li","Yuanhan Zhang","Jack Hessel","Jingkang Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2306.14899v2.pdf","comment":"Project Page: https://funqa-benchmark.github.io/ Codebase:\n  https://github.com/Jingkang50/FunQA"},{"id":"http://arxiv.org/abs/2402.02733v3","updated":"2024-03-22T09:17:24Z","published":"2024-02-05T05:25:33Z","title":"ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer","summary":"  Face re-aging is a prominent field in computer vision and graphics, with\nsignificant applications in photorealistic domains such as movies, advertising,\nand live streaming. Recently, the need to apply face re-aging to\nnon-photorealistic images, like comics, illustrations, and animations, has\nemerged as an extension in various entertainment sectors. However, the lack of\na network that can seamlessly edit the apparent age in NPR images has limited\nthese tasks to a naive, sequential approach. This often results in unpleasant\nartifacts and a loss of facial attributes due to domain discrepancies. In this\npaper, we introduce a novel one-stage method for face re-aging combined with\nportrait style transfer, executed in a single generative step. We leverage\nexisting face re-aging and style transfer networks, both trained within the\nsame PR domain. Our method uniquely fuses distinct latent vectors, each\nresponsible for managing aging-related attributes and NPR appearance. By\nadopting an exemplar-based approach, our method offers greater flexibility\ncompared to domain-level fine-tuning approaches, which typically require\nseparate training or fine-tuning for each domain. This effectively addresses\nthe limitation of requiring paired datasets for re-aging and domain-level,\ndata-driven approaches for stylization. Our experiments show that our model can\neffortlessly generate re-aged images while simultaneously transferring the\nstyle of examples, maintaining both natural appearance and controllability.\n","authors":["Bumsoo Kim","Abdul Muqeet","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2402.02733v3.pdf","comment":"14 pages, 15 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.11700v2","updated":"2024-03-22T08:13:11Z","published":"2024-03-18T11:56:35Z","title":"Virbo: Multimodal Multilingual Avatar Video Generation in Digital\n  Marketing","summary":"  With the widespread popularity of internet celebrity marketing all over the\nworld, short video production has gradually become a popular way of presenting\nproducts information. However, the traditional video production industry\nusually includes series of procedures as script writing, video filming in a\nprofessional studio, video clipping, special effects rendering, customized\npost-processing, and so forth. Not to mention that multilingual videos is not\naccessible for those who could not speak multilingual languages. These\ncomplicated procedures usually needs a professional team to complete, and this\nmade short video production costly in both time and money. This paper presents\nan intelligent system that supports the automatic generation of talking avatar\nvideos, namely Virbo. With simply a user-specified script, Virbo could use a\ndeep generative model to generate a target talking videos. Meanwhile, the\nsystem also supports multimodal inputs to customize the video with specified\nface, specified voice and special effects. This system also integrated a\nmultilingual customization module that supports generate multilingual talking\navatar videos in a batch with hundreds of delicate templates and creative\nspecial effects. Through a series of user studies and demo tests, we found that\nVirbo can generate talking avatar videos that maintained a high quality of\nvideos as those from a professional team while reducing the entire production\ncosts significantly. This intelligent system will effectively promote the video\nproduction industry and facilitate the internet marketing neglecting of\nlanguage barriers and cost challenges.\n","authors":["Juan Zhang","Jiahao Chen","Cheng Wang","Zhiwang Yu","Tangquan Qi","Can Liu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14972v1","updated":"2024-03-22T06:03:07Z","published":"2024-03-22T06:03:07Z","title":"A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal\n  Reasoning","summary":"  This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate BDoG, achieving state-of-the-art results in\nScience QA and MMBench with significant improvements over previous methods.\n","authors":["Changmeng Zheng","Dayong Liang","Wengyu Zhang","Xiao-Yong Wei","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.14972v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.14468v2","updated":"2024-03-22T02:16:40Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks","summary":"  Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Harry Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v2.pdf","comment":"preprint"}]},"2024-03-21T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.15503v1","updated":"2024-03-21T23:40:42Z","published":"2024-03-21T23:40:42Z","title":"Evaluating the Performance of LLMs on Technical Language Processing\n  tasks","summary":"  In this paper we present the results of an evaluation study of the\nperfor-mance of LLMs on Technical Language Processing tasks. Humans are often\nconfronted with tasks in which they have to gather information from dispar-ate\nsources and require making sense of large bodies of text. These tasks can be\nsignificantly complex for humans and often require deep study including\nrereading portions of a text. Towards simplifying the task of gathering\nin-formation we evaluated LLMs with chat interfaces for their ability to\nprovide answers to standard questions that a human can be expected to answer\nbased on their reading of a body of text. The body of text under study is Title\n47 of the United States Code of Federal Regulations (CFR) which describes\nregula-tions for commercial telecommunications as governed by the Federal\nCom-munications Commission (FCC). This has been a body of text of interest\nbe-cause our larger research concerns the issue of making sense of information\nrelated to Wireless Spectrum Governance and usage in an automated manner to\nsupport Dynamic Spectrum Access. The information concerning this wireless\nspectrum domain is found in many disparate sources, with Title 47 of the CFR\nbeing just one of many. Using a range of LLMs and providing the required CFR\ntext as context we were able to quantify the performance of those LLMs on the\nspecific task of answering the questions below.\n","authors":["Andrew Kernycky","David Coleman","Christopher Spence","Udayan Das"],"pdf_url":"https://arxiv.org/pdf/2403.15503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15501v1","updated":"2024-03-21T21:28:07Z","published":"2024-03-21T21:28:07Z","title":"Enhancing Medical Support in the Arabic Language Through Personalized\n  ChatGPT Assistance","summary":"  This Paper discusses the growing popularity of online medical diagnosis as an\nalternative to traditional doctor visits. It highlights the limitations of\nexisting tools and emphasizes the advantages of using ChatGPT, which provides\nreal-time, personalized medical diagnosis at no cost. The paragraph summarizes\na research study that evaluated the performance of ChatGPT in Arabic medical\ndiagnosis. The study involved compiling a dataset of disease information and\ngenerating multiple messages for each disease using different prompting\ntechniques. ChatGPT's performance was assessed by measuring the similarity\nbetween its responses and the actual diseases. The results showed promising\nperformance, with average scores of around 76% for similarity measures. Various\nprompting techniques were used, and chain prompting demonstrated a relative\nadvantage. The study also recorded an average response time of 6.12 seconds for\nthe ChatGPT API, which is considered acceptable but has room for improvement.\nWhile ChatGPT cannot replace human doctors entirely, the findings suggest its\npotential in emergency cases and addressing general medical inquiries. Overall,\nthe study highlights ChatGPT's viability as a valuable tool in the medical\nfield.\n","authors":["Mohamed Issa","Ahmed Abdelwahed"],"pdf_url":"https://arxiv.org/pdf/2403.15501v1.pdf","comment":"This paper was presented at The International conference for Arabic\n  language and applied linguistics"},{"id":"http://arxiv.org/abs/2402.03049v3","updated":"2024-03-21T15:33:34Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v3.pdf","comment":"Project website: https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2403.08206v2","updated":"2024-03-21T15:17:46Z","published":"2024-03-13T03:03:15Z","title":"Discrete Semantic Tokenization for Deep CTR Prediction","summary":"  Incorporating item content information into click-through rate (CTR)\nprediction models remains a challenge, especially with the time and space\nconstraints of industrial scenarios. The content-encoding paradigm, which\nintegrates user and item encoders directly into CTR models, prioritizes space\nover time. In contrast, the embedding-based paradigm transforms item and user\nsemantics into latent embeddings, subsequently caching them to optimize\nprocessing time at the expense of space. In this paper, we introduce a new\nsemantic-token paradigm and propose a discrete semantic tokenization approach,\nnamely UIST, for user and item representation. UIST facilitates swift training\nand inference while maintaining a conservative memory footprint. Specifically,\nUIST quantizes dense embedding vectors into discrete tokens with shorter\nlengths and employs a hierarchical mixture inference module to weigh the\ncontribution of each user--item token pair. Our experimental results on news\nrecommendation showcase the effectiveness and efficiency (about 200-fold space\ncompression) of UIST for CTR prediction.\n","authors":["Qijiong Liu","Hengchang Hu","Jiahao Wu","Jieming Zhu","Min-Yen Kan","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08206v2.pdf","comment":"TheWebConf 2024 accepted paper"},{"id":"http://arxiv.org/abs/2403.14457v1","updated":"2024-03-21T15:04:32Z","published":"2024-03-21T15:04:32Z","title":"gTBLS: Generating Tables from Text by Conditional Question Answering","summary":"  Distilling large, unstructured text into a structured, condensed form such as\ntables is an open research problem. One of the primary challenges in\nautomatically generating tables is ensuring their syntactic validity. Prior\napproaches address this challenge by including additional parameters in the\nTransformer's attention mechanism to attend to specific rows and column\nheaders. In contrast to this single-stage method, this paper presents a\ntwo-stage approach called Generative Tables (gTBLS). The first stage infers\ntable structure (row and column headers) from the text. The second stage\nformulates questions using these headers and fine-tunes a causal language model\nto answer them. Furthermore, the gTBLS approach is amenable to the utilization\nof pre-trained Large Language Models in a zero-shot configuration, presenting a\nsolution for table generation in situations where fine-tuning is not feasible.\ngTBLS improves prior approaches by up to 10% in BERTScore on the table\nconstruction task and up to 20% on the table content generation task of the\nE2E, WikiTableText, WikiBio, and RotoWire datasets.\n","authors":["Anirudh Sundar","Christopher Richardson","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2403.14457v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2401.16097v2","updated":"2024-03-21T13:29:03Z","published":"2024-01-29T12:11:34Z","title":"Pushing the Limits: Concurrency Detection in Acyclic Sound Free-Choice\n  Workflow Nets in $O(P^2 + T^2)$","summary":"  Concurrency is an important aspect of Petri nets to describe and simulate the\nbehavior of complex systems. Knowing which places and transitions could be\nexecuted in parallel helps to understand nets and enables analysis techniques\nand the computation of other properties, such as causality, exclusivity, etc..\nAll techniques based on concurrency detection depend on the efficiency of this\ndetection methodology. Kovalyov and Esparza have developed algorithms that\ncompute all concurrent places in $O\\big((P+T)TP^2\\big)$ for live and bounded\nnets (where $P$ and $T$ are the numbers of places and transitions) and in\n$O\\big(P(P+T)^2\\big)$ for live and bounded free-choice nets. Although these\nalgorithms have a reasonably good computational complexity, large numbers of\nconcurrent pairs of nodes may still lead to long computation times. This paper\ncomplements the palette of concurrency detection algorithms with the Concurrent\nPaths (CP) algorithm for sound free-choice workflow nets. The algorithm allows\nparallelization and has a worst-case computational complexity of $O(P^2 + T^2)$\nfor acyclic nets and of $O(P^3 + PT^2)$ for cyclic nets. Although the\ncomputational complexity of cyclic nets has not improved, the evaluation shows\nthe benefits of CP, especially, if the net contains many nodes in concurrency\nrelation.\n","authors":["Thomas M. Prinz","Julien Klaus","Nick R. T. P. van Beest"],"pdf_url":"https://arxiv.org/pdf/2401.16097v2.pdf","comment":"19 pages, 14 figures, 5 algorithms"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.14773v1","updated":"2024-03-21T18:27:29Z","published":"2024-03-21T18:27:29Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"  Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V\n","authors":["Roberto Henschel","Levon Khachatryan","Daniil Hayrapetyan","Hayk Poghosyan","Vahram Tadevosyan","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.14773v1.pdf","comment":"https://github.com/Picsart-AI-Research/StreamingT2V"},{"id":"http://arxiv.org/abs/2403.14449v1","updated":"2024-03-21T14:56:46Z","published":"2024-03-21T14:56:46Z","title":"Bringing Robots Home: The Rise of AI Robots in Consumer Electronics","summary":"  On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose\nmultimodal generative AI model designed specifically for training humanoid\nrobots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid\nrobot on December 12, 2023, underscored the profound impact robotics is poised\nto have on reshaping various facets of our daily lives. While robots have long\ndominated industrial settings, their presence within our homes is a burgeoning\nphenomenon. This can be attributed, in part, to the complexities of domestic\nenvironments and the challenges of creating robots that can seamlessly\nintegrate into our daily routines.\n","authors":["Haiwei Dong","Yang Liu","Ted Chu","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2403.14449v1.pdf","comment":"Accepted by IEEE Consumer Electronics Magazine"},{"id":"http://arxiv.org/abs/2301.12831v3","updated":"2024-03-21T05:39:44Z","published":"2023-01-30T12:37:04Z","title":"M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing\n  System","summary":"  Face presentation attacks (FPA), also known as face spoofing, have brought\nincreasing concerns to the public through various malicious applications, such\nas financial fraud and privacy leakage. Therefore, safeguarding face\nrecognition systems against FPA is of utmost importance. Although existing\nlearning-based face anti-spoofing (FAS) models can achieve outstanding\ndetection performance, they lack generalization capability and suffer\nsignificant performance drops in unforeseen environments. Many methodologies\nseek to use auxiliary modality data (e.g., depth and infrared maps) during the\npresentation attack detection (PAD) to address this limitation. However, these\nmethods can be limited since (1) they require specific sensors such as depth\nand infrared cameras for data capture, which are rarely available on commodity\nmobile devices, and (2) they cannot work properly in practical scenarios when\neither modality is missing or of poor quality. In this paper, we devise an\naccurate and robust MultiModal Mobile Face Anti-Spoofing system named M3FAS to\novercome the issues above. The primary innovation of this work lies in the\nfollowing aspects: (1) To achieve robust PAD, our system combines visual and\nauditory modalities using three commonly available sensors: camera, speaker,\nand microphone; (2) We design a novel two-branch neural network with three\nhierarchical feature aggregation modules to perform cross-modal feature fusion;\n(3). We propose a multi-head training strategy, allowing the model to output\npredictions from the vision, acoustic, and fusion heads, resulting in a more\nflexible PAD. Extensive experiments have demonstrated the accuracy, robustness,\nand flexibility of M3FAS under various challenging experimental settings. The\nsource code and dataset are available at: https://github.com/ChenqiKONG/M3FAS/\n","authors":["Chenqi Kong","Kexin Zheng","Yibing Liu","Shiqi Wang","Anderson Rocha","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2301.12831v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06255v3","updated":"2024-03-21T03:21:24Z","published":"2023-09-12T14:16:34Z","title":"Enhancing Multimodal Cooperation via Fine-grained Modality Valuation","summary":"  One primary topic of multimodal learning is to jointly incorporate\nheterogeneous information from different modalities. However, most models often\nsuffer from unsatisfactory multimodal cooperation, which cannot jointly utilize\nall modalities well. Some methods are proposed to identify and enhance the\nworse learnt modality, but they are often hard to provide the fine-grained\nobservation of multimodal cooperation at sample-level with theoretical support.\nHence, it is essential to reasonably observe and improve the fine-grained\ncooperation between modalities, especially when facing realistic scenarios\nwhere the modality discrepancy could vary across different samples. To this\nend, we introduce a sample-level modality valuation metric to evaluate the\ncontribution of each modality for each sample. Via modality valuation, we\nobserve that modality discrepancy indeed could be different at sample-level,\nbeyond the global contribution discrepancy at dataset-level. We further analyze\nthis issue and improve cooperation between modalities at sample-level by\nenhancing the discriminative ability of low-contributing modalities in a\ntargeted manner. Overall, our methods reasonably observe the fine-grained\nuni-modal contribution and achieve considerable improvement. The source code\nand dataset are available at\n\\url{https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation}.\n","authors":["Yake Wei","Ruoxuan Feng","Zihe Wang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2309.06255v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.10406v2","updated":"2024-03-21T01:45:43Z","published":"2024-03-15T15:38:30Z","title":"Deep Bi-directional Attention Network for Image Super-Resolution Quality\n  Assessment","summary":"  There has emerged a growing interest in exploring efficient quality\nassessment algorithms for image super-resolution (SR). However, employing deep\nlearning techniques, especially dual-branch algorithms, to automatically\nevaluate the visual quality of SR images remains challenging. Existing SR image\nquality assessment (IQA) metrics based on two-stream networks lack interactions\nbetween branches. To address this, we propose a novel full-reference IQA\n(FR-IQA) method for SR images. Specifically, producing SR images and evaluating\nhow close the SR images are to the corresponding HR references are separate\nprocesses. Based on this consideration, we construct a deep Bi-directional\nAttention Network (BiAtten-Net) that dynamically deepens visual attention to\ndistortions in both processes, which aligns well with the human visual system\n(HVS). Experiments on public SR quality databases demonstrate the superiority\nof our proposed BiAtten-Net over state-of-the-art quality assessment methods.\nIn addition, the visualization results and ablation study show the\neffectiveness of bi-directional attention.\n","authors":["Yixiao Li","Xiaoyuan Yang","Jun Fu","Guanghui Yue","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.10406v2.pdf","comment":"7 pages, 3 figures, published to 2024 IEEE International Conference\n  on Multimedia and Expo (ICME)"}]},"2024-03-20T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2403.13667v1","updated":"2024-03-20T15:24:57Z","published":"2024-03-20T15:24:57Z","title":"DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance","summary":"  Choreographers determine what the dances look like, while cameramen determine\nthe final presentation of dances. Recently, various methods and datasets have\nshowcased the feasibility of dance synthesis. However, camera movement\nsynthesis with music and dance remains an unsolved challenging problem due to\nthe scarcity of paired data. Thus, we present DCM, a new multi-modal 3D\ndataset, which for the first time combines camera movement with dance motion\nand music audio. This dataset encompasses 108 dance sequences (3.2 hours) of\npaired dance-camera-music data from the anime community, covering 4 music\ngenres. With this dataset, we uncover that dance camera movement is\nmultifaceted and human-centric, and possesses multiple influencing factors,\nmaking dance camera synthesis a more challenging task compared to camera or\ndance synthesis alone. To overcome these difficulties, we propose\nDanceCamera3D, a transformer-based diffusion model that incorporates a novel\nbody attention loss and a condition separation strategy. For evaluation, we\ndevise new metrics measuring camera movement quality, diversity, and dancer\nfidelity. Utilizing these metrics, we conduct extensive experiments on our DCM\ndataset, providing both quantitative and qualitative evidence showcasing the\neffectiveness of our DanceCamera3D model. Code and video demos are available at\nhttps://github.com/Carmenw1203/DanceCamera3D-Official.\n","authors":["Zixuan Wang","Jia Jia","Shikun Sun","Haozhe Wu","Rong Han","Zhenyu Li","Di Tang","Jiaqing Zhou","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13667v1.pdf","comment":"Accept to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11959v2","updated":"2024-03-20T11:58:23Z","published":"2024-03-18T16:56:47Z","title":"IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video\n  Action Counting","summary":"  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and\neveryday activities by quantifying repetitive actions in videos. However,\ntraditional VAC methods have overlooked the complexity of action repetitions,\nsuch as interruptions and the variability in cycle duration. Our research\naddresses the shortfall by introducing a novel approach to VAC, called\nIrregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular\nrepetition patterns in videos, which we define through two primary aspects:\nInter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle\nConsistency ensures homogeneity in the spatial-temporal representations of\ncycle segments, signifying action uniformity within cycles. Cycle-interval\ninconsistency highlights the importance of distinguishing between cycle\nsegments and intervals based on their inherent content differences. To\nencapsulate these principles, we propose a new methodology that includes\nconsistency and inconsistency modules, supported by a unique pull-push loss\n(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence\namong cycle segment features and a push loss to clearly distinguish features of\ncycle segments from interval segments. Empirical evaluations conducted on the\nRepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in\nVAC task performance. Furthermore, the model demonstrates exceptional\nadaptability and generalization across various video contents, outperforming\nexisting models on two additional datasets, UCFRep and Countix, without the\nneed for dataset-specific optimization. These results confirm the efficacy of\nour approach in addressing irregular repetitions in videos and pave the way for\nfurther advancements in video analysis and understanding.\n","authors":["Hang Wang","Zhi-Qi Cheng","Youtian Du","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11959v2.pdf","comment":"Source code: https://github.com/hwang-cs-ime/IVAC-P2L"},{"id":"http://arxiv.org/abs/2403.13501v1","updated":"2024-03-20T10:58:58Z","published":"2024-03-20T10:58:58Z","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","summary":"  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n","authors":["Yumeng Li","William Beluch","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2403.13501v1.pdf","comment":"Project page: https://yumengli007.github.io/VSTAR"},{"id":"http://arxiv.org/abs/2403.13480v1","updated":"2024-03-20T10:34:40Z","published":"2024-03-20T10:34:40Z","title":"A Unified Optimal Transport Framework for Cross-Modal Retrieval with\n  Noisy Labels","summary":"  Cross-modal retrieval (CMR) aims to establish interaction between different\nmodalities, among which supervised CMR is emerging due to its flexibility in\nlearning semantic category discrimination. Despite the remarkable performance\nof previous supervised CMR methods, much of their success can be attributed to\nthe well-annotated data. However, even for unimodal data, precise annotation is\nexpensive and time-consuming, and it becomes more challenging with the\nmultimodal scenario. In practice, massive multimodal data are collected from\nthe Internet with coarse annotation, which inevitably introduces noisy labels.\nTraining with such misleading labels would bring two key challenges --\nenforcing the multimodal samples to \\emph{align incorrect semantics} and\n\\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To\ntackle these challenges, this work proposes UOT-RCL, a Unified framework based\non Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a\nsemantic alignment based on partial OT to progressively correct the noisy\nlabels, where a novel cross-modal consistent cost function is designed to blend\ndifferent modalities and provide precise transport cost. Second, to narrow the\ndiscrepancy in multi-modal data, an OT-based relation alignment is proposed to\ninfer the semantic-level cross-modal matching. Both of these two components\nleverage the inherent correlation among multi-modal data to facilitate\neffective cost function. The experiments on three widely-used cross-modal\nretrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art\napproaches and significantly improves the robustness against noisy labels.\n","authors":["Haochen Han","Minnan Luo","Huan Liu","Fang Nan"],"pdf_url":"https://arxiv.org/pdf/2403.13480v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.12053v2","updated":"2024-03-20T03:45:34Z","published":"2024-01-04T12:02:15Z","title":"PiGW: A Plug-in Generative Watermarking Framework","summary":"  Integrating watermarks into generative images is a critical strategy for\nprotecting intellectual property and enhancing artificial intelligence\nsecurity. This paper proposes Plug-in Generative Watermarking (PiGW) as a\ngeneral framework for integrating watermarks into generative images. More\nspecifically, PiGW embeds watermark information into the initial noise using a\nlearnable watermark embedding network and an adaptive frequency spectrum mask.\nFurthermore, it optimizes training costs by gradually increasing timesteps.\nExtensive experiments demonstrate that PiGW enables embedding watermarks into\nthe generated image with negligible quality loss while achieving true\ninvisibility and high resistance to noise attacks. Moreover, PiGW can serve as\na plugin for various commonly used generative structures and multimodal\ngenerative content types. Finally, we demonstrate how PiGW can also be utilized\nfor detecting generated images, contributing to the promotion of secure AI\ndevelopment. The project code will be made available on GitHub.\n","authors":["Rui Ma","Mengxi Guo","Li Yuming","Hengyuan Zhang","Cong Ma","Yuan Li","Xiaodong Xie","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12053v2.pdf","comment":"Improve experimental content"},{"id":"http://arxiv.org/abs/2403.12667v2","updated":"2024-03-20T03:15:31Z","published":"2024-03-19T12:05:09Z","title":"ICE: Interactive 3D Game Character Editing via Dialogue","summary":"  Text-driven in-game 3D character auto-customization systems eliminate the\ncomplicated process of manipulating intricate character control parameters.\nHowever, current methods are limited by their single-round generation,\nincapable of further editing and fine-grained modification. In this paper, we\npropose an Interactive Character Editing framework (ICE) to achieve a\nmulti-round dialogue-based refinement process. In a nutshell, our ICE offers a\nmore user-friendly way to enable players to convey creative ideas iteratively\nwhile ensuring that created characters align with the expectations of players.\nSpecifically, we propose an Instruction Parsing Module (IPM) that utilizes\nlarge language models (LLMs) to parse multi-round dialogues into clear editing\ninstruction prompts in each round. To reliably and swiftly modify character\ncontrol parameters at a fine-grained level, we propose a Semantic-guided\nLow-dimension Parameter Solver (SLPS) that edits character control parameters\naccording to prompts in a zero-shot manner. Our SLPS first localizes the\ncharacter control parameters related to the fine-grained modification, and then\noptimizes the corresponding parameters in a low-dimension space to avoid\nunrealistic results. Extensive experimental results demonstrate the\neffectiveness of our proposed ICE for in-game character creation and the\nsuperior editing performance of ICE. Project page: https://iceedit.github.io/.\n","authors":["Haoqian Wu","Yunjie Wu","Zhipeng Hu","Lincheng Li","Weijie Chen","Rui Zhao","Changjie Fan","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2403.12667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10943v2","updated":"2024-03-20T02:52:42Z","published":"2024-03-16T15:14:15Z","title":"MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent\n  Recognition and Out-of-scope Detection in Conversations","summary":"  Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.\n","authors":["Hanlei Zhang","Xin Wang","Hua Xu","Qianrui Zhou","Kai Gao","Jianhua Su","jinyue Zhao","Wenrui Li","Yanting Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10943v2.pdf","comment":"Published in ICLR 2024; The abstract is slightly modified due to the\n  length limitation"},{"id":"http://arxiv.org/abs/2403.08505v2","updated":"2024-03-20T02:35:57Z","published":"2024-03-13T13:12:57Z","title":"Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Jiawei Shao","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v2.pdf","comment":null}]}}